{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":24,"seo":110,"textContent":111},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"fr","advanced-topics","Advanced Topics","cyan","precision","Précision : FP32, FP16, FP8, FP4 et le Rôle du GPU","\u003Cp>La \u003Cstrong>précision\u003C/strong> d&#39;un modèle fait référence au format numérique utilisé pour stocker ses &quot;poids&quot;. Ces poids sont des nombres réels, et les ordinateurs...\u003C/p>\n","La précision d'un modèle fait référence au format numérique utilisé pour stocker ses \"poids\". Ces poids sont des nombres réels, et les ordinateurs...","\u003Cp>La \u003Cstrong>précision\u003C/strong> d&#39;un modèle fait référence au format numérique utilisé pour stocker ses &quot;poids&quot;. Ces poids sont des nombres réels, et les ordinateurs les représentent à l&#39;aide d&#39;un système appelé \u003Cstrong>Virgule Flottante\u003C/strong> (d&#39;où l&#39;acronyme \u003Cstrong>FP\u003C/strong>). [1]\u003C/p>\n\u003Cp>Le nombre qui suit l&#39;acronyme (par exemple, FP\u003Cstrong>32\u003C/strong>, FP\u003Cstrong>16\u003C/strong>) indique combien de \u003Cstrong>bits\u003C/strong> de mémoire sont utilisés pour représenter un seul nombre. Plus on utilise de bits, plus le nombre est précis, mais plus il prend de place et plus il est lent à traiter. Le choix de la précision est donc un compromis fondamental entre la qualité, la vitesse et la consommation de mémoire (VRAM).\u003C/p>\n\u003Ch3>Le Lien Indissoluble avec le Matériel (GPU)\u003C/h3>\n\u003Cp>Le choix de la précision n&#39;est pas seulement logiciel : les performances dépendent de manière critique du \u003Cstrong>support matériel natif\u003C/strong> de votre GPU. Si une GPU ne prend pas en charge nativement un format à faible précision, elle doit l&#39;émuler par logiciel, ce qui entraîne des performances inférieures. [4]\u003C/p>\n\u003Cp>Les GPU modernes, en particulier celles de NVIDIA, incluent du matériel spécialisé appelé \u003Cstrong>Tensor Cores\u003C/strong>, conçu pour accélérer considérablement les calculs à précision réduite. [4]\u003C/p>\n\u003Ch3>Formats Courants et Support Matériel (Exemples GeForce)\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>FP32 (Pleine Précision - 32 bits) :\u003C/strong>\nC&#39;est la &quot;qualité maximale&quot;. Chaque nombre occupe 32 bits de mémoire. C&#39;est la norme sur laquelle les modèles sont entraînés, mais il est très lourd à exécuter pour l&#39;inférence. [2] Un modèle est rarement utilisé entièrement en FP32 pour générer des images. Toutes les GPU le prennent en charge.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>FP16 (Demi-Précision - 16 bits) :\u003C/strong>\nLa référence pour l&#39;inférence. Il divise par deux la VRAM et double (ou plus) la vitesse par rapport au FP32, avec une perte de qualité presque imperceptible. [2] La compatibilité dans ce cas peut également être trouvée avec des GPU moins récentes. [4]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>BF16 (Bfloat16 - 16 bits) :\u003C/strong>\nUn format alternatif de 16 bits, plus robuste pendant l&#39;entraînement. Pris en charge nativement par les séries \u003Cstrong>Ampere (RTX 30)\u003C/strong> et ultérieures.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch3>Quantification et Support Matériel Avancé\u003C/h3>\n\u003Cp>La \u003Cstrong>quantification\u003C/strong> convertit les poids dans des formats encore plus bas (8 ou 4 bits). Ici, le support matériel devient encore plus critique.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>\u003Cstrong>FP8 / INT8 (8 bits) :\u003C/strong>\n  Il représente une énorme avancée en termes d&#39;efficacité. L&#39;accélération matérielle pour le FP8 est une fonctionnalité phare des architectures les plus récentes, comme \u003Cstrong>Ada Lovelace (RTX 40)\u003C/strong> qui introduit le support natif, garantissant une augmentation significative des performances avec ce format. [3] Les cartes plus anciennes peuvent l&#39;exécuter, mais avec une efficacité bien moindre.\u003C/p>\n\u003Ch3>Un Regard plus Approfondi sur le FP8\u003C/h3>\n\u003Cp>  Le terme \u003Ccode>FP8\u003C/code> décrit en fait une famille de formats. La principale différence réside dans la manière dont les 8 bits disponibles sont alloués entre l&#39;\u003Cstrong>Exposant\u003C/strong> (qui détermine la plage de valeurs possibles) et la \u003Cstrong>Mantisse\u003C/strong> (qui détermine la précision entre une valeur et une autre). Les deux normes principales sont :\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>E4M3\u003C/code>\u003C/strong> : Utilise 4 bits pour l&#39;exposant et 3 pour la mantisse. Il offre un bon équilibre entre la plage et la précision, et est souvent utilisé pour stocker les poids du modèle.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>E5M2\u003C/code>\u003C/strong> : Utilise 5 bits pour l&#39;exposant et 2 pour la mantisse. Il a une plage dynamique plus large mais moins de précision. Il est généralement utilisé pour les gradients pendant l&#39;entraînement.\u003C/li>\n\u003C/ul>\n\u003Cp>  \u003Cstrong>FP8 Scaled\u003C/strong> n&#39;est pas un format en soi, mais décrit la technique d&#39;utilisation d&#39;un facteur d&#39;échelle pour optimiser la conversion des poids au format FP8, maximisant la précision dans la plage de valeurs la plus importante. Les GPU modernes comme la série RTX 40 gèrent ces facteurs d&#39;échelle de manière très efficace.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>4 bits (par exemple, NF4) :\u003C/strong>\n  Une forme de quantification extrême, les performances dépendent fortement des implémentations logicielles optimisées qui tirent le meilleur parti des capacités générales de la GPU. Le support matériel est arrivé avec \u003Cstrong>Blackwell (RTX 50)\u003C/strong>.\u003C/p>\n\u003C/li>\n\u003C/ul>\n",[12,15,18,21],{"text":13,"url":14},"Explication des types de données à virgule flottante - Wikipedia","https://en.wikipedia.org/wiki/Floating-point_arithmetic",{"text":16,"url":17},"Guide de la Précision Mixte - Hugging Face","https://huggingface.co/docs/diffusers/main/en/optimization/fp16",{"text":19,"url":20},"Introduction aux formats 8 bits (FP8) - Blog NVIDIA","https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/",{"text":22,"url":23},"Architecture NVIDIA Ampere (Série 30) et les Tensor Cores de 3e génération","https://www.nvidia.com/it-it/geforce/graphics-cards/30-series/ampere-architecture/",{"it":25,"en":40,"fr":55,"es":69,"de":84,"pt":99},{"category":26,"connections":27,"backToHub":28,"noPostsFound":29,"pageTitleCategory":26,"initializing":30,"backToArticles":31,"sources":32,"searchPlaceholder":33,"showMap":34,"hideMap":35,"listenToArticle":36,"playing":37,"paused":38,"voice":39},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":41,"connections":42,"backToHub":43,"noPostsFound":44,"pageTitleCategory":41,"initializing":45,"backToArticles":46,"sources":47,"searchPlaceholder":48,"showMap":49,"hideMap":50,"listenToArticle":51,"playing":52,"paused":53,"voice":54},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":56,"connections":57,"backToHub":58,"noPostsFound":59,"pageTitleCategory":56,"initializing":60,"backToArticles":61,"sources":47,"searchPlaceholder":62,"showMap":63,"hideMap":64,"listenToArticle":65,"playing":66,"paused":67,"voice":68},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":70,"connections":71,"backToHub":72,"noPostsFound":73,"pageTitleCategory":70,"initializing":74,"backToArticles":75,"sources":76,"searchPlaceholder":77,"showMap":78,"hideMap":79,"listenToArticle":80,"playing":81,"paused":82,"voice":83},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":85,"connections":86,"backToHub":87,"noPostsFound":88,"pageTitleCategory":85,"initializing":89,"backToArticles":90,"sources":91,"searchPlaceholder":92,"showMap":93,"hideMap":94,"listenToArticle":95,"playing":96,"paused":97,"voice":98},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":26,"connections":100,"backToHub":101,"noPostsFound":102,"pageTitleCategory":26,"initializing":74,"backToArticles":103,"sources":104,"searchPlaceholder":105,"showMap":106,"hideMap":79,"listenToArticle":107,"playing":108,"paused":109,"voice":83},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Précision : FP32, FP16, FP8, FP4 et le Rôle du GPU. La précision d'un modèle fait référence au format numérique utilisé pour stocker ses \"poids\". Ces poids sont des nombres réels, et les ordinateurs les représentent à l'aide d'un système appelé Virgule Flottante (d'où l'acronyme FP). Le nombre qui suit l'acronyme (par exemple, FP32, FP16) indique combien de bits de mémoire sont utilisés pour représenter un seul nombre. Plus on utilise de bits, plus le nombre est précis, mais plus il prend de place et plus il est lent à traiter. Le choix de la précision est donc un compromis fondamental entre la qualité, la vitesse et la consommation de mémoire (VRAM). Le Lien Indissoluble avec le Matériel (GPU); Le choix de la précision n'est pas seulement logiciel : les performances dépendent de manière critique du support matériel natif de votre GPU. Si une GPU ne prend pas en charge nativement un format à faible précision, elle doit l'émuler par logiciel, ce qui entraîne des performances inférieures. Les GPU modernes, en particulier celles de NVIDIA, incluent du matériel spécialisé appelé Tensor Cores, conçu pour accélérer considérablement les calculs à précision réduite. Formats Courants et Support Matériel (Exemples GeForce); 1. FP32 (Pleine Précision - 32 bits) : C'est la \"qualité maximale\". Chaque nombre occupe 32 bits de mémoire. C'est la norme sur laquelle les modèles sont entraînés, mais il est très lourd à exécuter pour l'inférence. Un modèle est rarement utilisé entièrement en FP32 pour générer des images. Toutes les GPU le prennent en charge. 2. FP16 (Demi-Précision - 16 bits) : La référence pour l'inférence. Il divise par deux la VRAM et double (ou plus) la vitesse par rapport au FP32, avec une perte de qualité presque imperceptible. La compatibilité dans ce cas peut également être trouvée avec des GPU moins récentes. 3. BF16 (Bfloat16 - 16 bits) : Un format alternatif de 16 bits, plus robuste pendant l'entraînement. Pris en charge nativement par les séries Ampere (RTX 30) et ultérieures. Quantification et Support Matériel Avancé; La quantification convertit les poids dans des formats encore plus bas (8 ou 4 bits). Ici, le support matériel devient encore plus critique. - FP8 / INT8 (8 bits) : Il représente une énorme avancée en termes d'efficacité. L'accélération matérielle pour le FP8 est une fonctionnalité phare des architectures les plus récentes, comme Ada Lovelace (RTX 40) qui introduit le support natif, garantissant une augmentation significative des performances avec ce format. Les cartes plus anciennes peuvent l'exécuter, mais avec une efficacité bien moindre. ### Un Regard plus Approfondi sur le FP8 Le terme `FP8` décrit en fait une famille de formats. La principale différence réside dans la manière dont les 8 bits disponibles sont alloués entre l'Exposant (qui détermine la plage de valeurs possibles) et la Mantisse (qui détermine la précision entre une valeur et une autre). Les deux normes principales sont : - `E4M3` : Utilise 4 bits pour l'exposant et 3 pour la mantisse. Il offre un bon équilibre entre la plage et la précision, et est souvent utilisé pour stocker les poids du modèle. - `E5M2` : Utilise 5 bits pour l'exposant et 2 pour la mantisse. Il a une plage dynamique plus large mais moins de précision. Il est généralement utilisé pour les gradients pendant l'entraînement. FP8 Scaled n'est pas un format en soi, mais décrit la technique d'utilisation d'un facteur d'échelle pour optimiser la conversion des poids au format FP8, maximisant la précision dans la plage de valeurs la plus importante. Les GPU modernes comme la série RTX 40 gèrent ces facteurs d'échelle de manière très efficace. - 4 bits (par exemple, NF4) : Une forme de quantification extrême, les performances dépendent fortement des implémentations logicielles optimisées qui tirent le meilleur parti des capacités générales de la GPU. Le support matériel est arrivé avec Blackwell (RTX 50)."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

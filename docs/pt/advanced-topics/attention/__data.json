{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":18,"seo":104,"textContent":105},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"pt","advanced-topics","Advanced Topics","cyan","attention","Atenção: O Mecanismo de Foco","\u003Cp>A \u003Cstrong>Atenção\u003C/strong> (ou \u003Cem>Auto-Atenção\u003C/em>) é o mecanismo computacional no coração da arquitetura \u003Cstrong>Transformer\u003C/strong>, que revolucionou tanto os modelos de linguage...\u003C/p>\n","A Atenção (ou Auto-Atenção) é o mecanismo computacional no coração da arquitetura Transformer, que revolucionou tanto os modelos de linguage...","\u003Cp>A \u003Cstrong>Atenção\u003C/strong> (ou \u003Cem>Auto-Atenção\u003C/em>) é o mecanismo computacional no coração da arquitetura \u003Cstrong>Transformer\u003C/strong>, que revolucionou tanto os modelos de linguagem (LLMs) quanto, mais recentemente, os modelos de difusão (DiTs). [1]\u003C/p>\n\u003Cp>Em termos simples, a Atenção permite que um modelo \u003Cstrong>pondere dinamicamente a importância de diferentes partes de uma entrada\u003C/strong> (como palavras em uma frase ou patches em uma imagem) para entender o contexto e as relações entre elas. [2]\u003C/p>\n\u003Ch3>Como Funciona (Conceitualmente)?\u003C/h3>\n\u003Cp>Imagine ler a frase: \u003Ccode>Um gato vermelho persegue um rato cinza\u003C/code>.\nQuando o modelo processa a palavra &quot;vermelho&quot;, o mecanismo de Atenção permite que ele entenda que &quot;vermelho&quot; está fortemente conectado a &quot;gato&quot; e não a &quot;rato&quot;. Na prática, para cada palavra, a Atenção calcula uma &quot;pontuação de atenção&quot; em relação a todas as outras palavras da frase, &quot;focando&quot; nas relações mais importantes. [2]\u003C/p>\n\u003Cp>Isso é fundamental para resolver ambiguidades e entender as nuances da linguagem.\u003C/p>\n\u003Ch3>Atenção na Geração de Imagens\u003C/h3>\n\u003Cp>O mecanismo de Atenção é crucial em dois pontos do nosso fluxo de trabalho:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>No Codificador de Texto CLIP:\u003C/strong>\nQuando o CLIP processa nosso prompt, a Atenção é o que permite que ele entenda que em \u003Ccode>um astronauta a cavalo\u003C/code>, é o astronauta que deve estar no cavalo. É também o mecanismo que é influenciado quando aumentamos o peso de uma palavra com a sintaxe \u003Ccode>(palavra:1.2)\u003C/code>, dizendo-lhe para &quot;prestar mais atenção&quot; a esse conceito.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Nos Transformadores de Difusão (DiTs):\u003C/strong>\nEm modelos como o Stable Diffusion 3, a Atenção não é aplicada apenas ao texto, mas também aos &quot;tokens visuais&quot; (os patches da imagem). Isso permite que o modelo crie relações complexas entre as diferentes partes da imagem, melhorando drasticamente a coerência e a composição. Por exemplo, ele pode garantir que um reflexo em um espelho corresponda corretamente ao objeto refletido.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>Em resumo, a Atenção é a tecnologia que permitiu que os modelos passassem de uma simples &quot;associação&quot; de palavras para uma verdadeira &quot;compreensão&quot; do contexto e das relações, tanto no texto quanto nas imagens.\u003C/p>\n",[12,15],{"text":13,"url":14},"Artigo 'Attention Is All You Need' que introduziu o Transformer","https://arxiv.org/abs/1706.03762",{"text":16,"url":17},"Explicação ilustrada do mecanismo de Atenção","https://jalammar.github.io/visualizing-neural-machine-translation-self-attention-visualizations-for-transformer-models/",{"it":19,"en":34,"fr":49,"es":63,"de":78,"pt":93},{"category":20,"connections":21,"backToHub":22,"noPostsFound":23,"pageTitleCategory":20,"initializing":24,"backToArticles":25,"sources":26,"searchPlaceholder":27,"showMap":28,"hideMap":29,"listenToArticle":30,"playing":31,"paused":32,"voice":33},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":35,"connections":36,"backToHub":37,"noPostsFound":38,"pageTitleCategory":35,"initializing":39,"backToArticles":40,"sources":41,"searchPlaceholder":42,"showMap":43,"hideMap":44,"listenToArticle":45,"playing":46,"paused":47,"voice":48},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":50,"connections":51,"backToHub":52,"noPostsFound":53,"pageTitleCategory":50,"initializing":54,"backToArticles":55,"sources":41,"searchPlaceholder":56,"showMap":57,"hideMap":58,"listenToArticle":59,"playing":60,"paused":61,"voice":62},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":64,"connections":65,"backToHub":66,"noPostsFound":67,"pageTitleCategory":64,"initializing":68,"backToArticles":69,"sources":70,"searchPlaceholder":71,"showMap":72,"hideMap":73,"listenToArticle":74,"playing":75,"paused":76,"voice":77},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":79,"connections":80,"backToHub":81,"noPostsFound":82,"pageTitleCategory":79,"initializing":83,"backToArticles":84,"sources":85,"searchPlaceholder":86,"showMap":87,"hideMap":88,"listenToArticle":89,"playing":90,"paused":91,"voice":92},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":20,"connections":94,"backToHub":95,"noPostsFound":96,"pageTitleCategory":20,"initializing":68,"backToArticles":97,"sources":98,"searchPlaceholder":99,"showMap":100,"hideMap":73,"listenToArticle":101,"playing":102,"paused":103,"voice":77},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Atenção: O Mecanismo de Foco. A Atenção (ou Auto-Atenção) é o mecanismo computacional no coração da arquitetura Transformer, que revolucionou tanto os modelos de linguagem (LLMs) quanto, mais recentemente, os modelos de difusão (DiTs). Em termos simples, a Atenção permite que um modelo pondere dinamicamente a importância de diferentes partes de uma entrada (como palavras em uma frase ou patches em uma imagem) para entender o contexto e as relações entre elas. Como Funciona (Conceitualmente)?; Imagine ler a frase: `Um gato vermelho persegue um rato cinza`. Quando o modelo processa a palavra \"vermelho\", o mecanismo de Atenção permite que ele entenda que \"vermelho\" está fortemente conectado a \"gato\" e não a \"rato\". Na prática, para cada palavra, a Atenção calcula uma \"pontuação de atenção\" em relação a todas as outras palavras da frase, \"focando\" nas relações mais importantes. Isso é fundamental para resolver ambiguidades e entender as nuances da linguagem. Atenção na Geração de Imagens; O mecanismo de Atenção é crucial em dois pontos do nosso fluxo de trabalho: 1. No Codificador de Texto CLIP: Quando o CLIP processa nosso prompt, a Atenção é o que permite que ele entenda que em `um astronauta a cavalo`, é o astronauta que deve estar no cavalo. É também o mecanismo que é influenciado quando aumentamos o peso de uma palavra com a sintaxe `(palavra:1.2)`, dizendo-lhe para \"prestar mais atenção\" a esse conceito. 2. Nos Transformadores de Difusão (DiTs): Em modelos como o Stable Diffusion 3, a Atenção não é aplicada apenas ao texto, mas também aos \"tokens visuais\" (os patches da imagem). Isso permite que o modelo crie relações complexas entre as diferentes partes da imagem, melhorando drasticamente a coerência e a composição. Por exemplo, ele pode garantir que um reflexo em um espelho corresponda corretamente ao objeto refletido. Em resumo, a Atenção é a tecnologia que permitiu que os modelos passassem de uma simples \"associação\" de palavras para uma verdadeira \"compreensão\" do contexto e das relações, tanto no texto quanto nas imagens."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

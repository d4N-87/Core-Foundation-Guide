{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"pt","advanced-topics","Advanced Topics","cyan","gguf","GGUF: Quantização para CPU e GPU","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> é um formato de arquivo projetado para conter modelos neurais \u003Cstrong>quantizados\u003C/strong>, ou seja, convertidos para f...\u003C/p>\n","GGUF (Georgi Gerganov Universal Format) é um formato de arquivo projetado para conter modelos neurais quantizados, ou seja, convertidos para f...","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> é um formato de arquivo projetado para conter modelos neurais \u003Cstrong>quantizados\u003C/strong>, ou seja, convertidos para formatos de baixíssima precisão (como 4 ou 8 bits) para reduzir drasticamente seu tamanho e consumo de memória. [1]\u003C/p>\n\u003Cp>Nascido do projeto \u003Cstrong>\u003Ccode>llama.cpp\u003C/code>\u003C/strong> para executar Grandes Modelos de Linguagem (LLMs) em CPUs, seu uso se expandiu recentemente para o ecossistema de modelos de difusão de imagens. [2]\u003C/p>\n\u003Ch3>O objetivo principal: reduzir o consumo de memória\u003C/h3>\n\u003Cp>A principal vantagem do GGUF é a \u003Cstrong>quantização\u003C/strong>. Um modelo que em formato FP16 (\u003Ccode>.safetensors\u003C/code>) ocuparia 14 GB de VRAM, em formato GGUF quantizado para 4 bits (\u003Ccode>q4_K_M\u003C/code>) pode ocupar menos de 5 GB. Isso permite:\u003C/p>\n\u003Cul>\n\u003Cli>Executar modelos enormes em GPUs com menos VRAM.\u003C/li>\n\u003Cli>Carregar mais componentes na memória simultaneamente.\u003C/li>\n\u003Cli>Executar modelos em CPUs de forma eficiente.\u003C/li>\n\u003C/ul>\n\u003Ch3>GGUF no mundo dos LLMs (uso clássico)\u003C/h3>\n\u003Cp>O uso primário do GGUF é para modelos de linguagem. Interfaces como o LM Studio ou o Ollama usam arquivos GGUF para executar chatbots poderosos (como Llama, Mistral) em hardware de consumo, aproveitando principalmente a CPU. [3]\u003C/p>\n\u003Ch3>GGUF no mundo da difusão (uso moderno no ComfyUI)\u003C/h3>\n\u003Cp>Recentemente, a comunidade começou a aplicar as vantagens da quantização GGUF também aos componentes de processamento. No ComfyUI, através de nós especializados (\u003Ccode>Load GGUF Model\u003C/code>), é possível carregar versões GGUF de:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Codificador de texto (CLIP):\u003C/strong> Carregar um CLIP quantizado reduz significativamente seu impacto na VRAM, liberando recursos preciosos para o modelo UNet. Este é o uso mais comum e eficaz.\u003C/li>\n\u003Cli>\u003Cstrong>UNet:\u003C/strong> Também existem experimentos para quantizar toda a UNet em formato GGUF. Embora isso ofereça a máxima economia de memória, pode levar a uma perda de qualidade mais perceptível na imagem final em comparação com o uso de uma UNet em formato FP16.\u003C/li>\n\u003C/ul>\n\u003Cp>É uma ferramenta versátil para o \u003Cstrong>gerenciamento avançado de memória\u003C/strong>, permitindo que os usuários executem fluxos de trabalho cada vez mais complexos em hardware de consumo, equilibrando habilmente o compromisso entre o consumo de VRAM e a qualidade da saída.\u003C/p>\n\u003Ch3>Decifrando as nomenclaturas de quantização (por exemplo, \u003Ccode>Q4_K_M\u003C/code>)\u003C/h3>\n\u003Cp>Ao baixar um modelo GGUF, o nome do arquivo geralmente contém um acrônimo que descreve o método de quantização usado. Entendê-lo ajuda a escolher o equilíbrio certo entre tamanho e qualidade. Veja como lê-lo:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q\u003C/code> seguido por um número (por exemplo, \u003Ccode>Q4\u003C/code>, \u003Ccode>Q5\u003C/code>, \u003Ccode>Q8\u003C/code>):\u003C/strong> Indica o número de \u003Cstrong>bits\u003C/strong> usados para cada peso. \u003Ccode>Q8\u003C/code> usa 8 bits (maior qualidade, arquivo maior), \u003Ccode>Q4\u003C/code> usa 4 bits (menor qualidade, arquivo menor).\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_K\u003C/code>:\u003C/strong> Indica uma variante &quot;K-Quant&quot;. É uma técnica de quantização aprimorada que tenta preservar melhor a qualidade da informação, especialmente para os pesos mais importantes. Os modelos \u003Ccode>_K\u003C/code> são frequentemente a escolha recomendada.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_0\u003C/code> ou \u003Ccode>_1\u003C/code> (por exemplo, \u003Ccode>Q4_0\u003C/code>, \u003Ccode>Q5_1\u003C/code>):\u003C/strong> Indicam diferentes versões do mesmo método. \u003Ccode>_0\u003C/code> é a versão &quot;pura&quot; de 4 bits, enquanto \u003Ccode>_1\u003C/code> é uma versão mista que usa uma precisão ligeiramente maior (5 bits) para alguns pesos, oferecendo uma pequena melhoria de qualidade com um arquivo ligeiramente maior.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_S\u003C/code>, \u003Ccode>_M\u003C/code>, \u003Ccode>_L\u003C/code> (por exemplo, \u003Ccode>Q4_K_S\u003C/code>):\u003C/strong> Indicam os tamanhos do modelo (&quot;Pequeno&quot;, &quot;Médio&quot;, &quot;Grande&quot;). Eles não se referem à quantização em si, mas a diferentes &quot;tamanhos&quot; do modelo original.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Exemplos práticos:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q8_0\u003C/code>:\u003C/strong> Quantização de 8 bits. A mais alta qualidade entre as versões GGUF, mas também a mais pesada.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q5_K_M\u003C/code>:\u003C/strong> Quantização &quot;K-Quant&quot; de 5 bits, versão &quot;Média&quot;. Um excelente compromisso entre qualidade e tamanho.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q4_0\u003C/code>:\u003C/strong> Quantização &quot;pura&quot; de 4 bits. A versão menor e mais leve, mas com a maior perda de qualidade. Frequentemente usada para executar modelos enormes em hardware muito limitado.\u003C/li>\n\u003C/ul>\n",[12,15,18],{"text":13,"url":14},"Anúncio oficial do GGUF no blog da Hugging Face","https://huggingface.co/blog/gguf",{"text":16,"url":17},"Repositório do GitHub do llama.cpp","https://github.com/ggerganov/llama.cpp",{"text":19,"url":20},"Exemplo de um fluxo de trabalho no ComfyUI com o GGUF Loader","https://comfyanonymous.github.io/ComfyUI_examples/llm/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"GGUF: Quantização para CPU e GPU. GGUF (Georgi Gerganov Universal Format) é um formato de arquivo projetado para conter modelos neurais quantizados, ou seja, convertidos para formatos de baixíssima precisão (como 4 ou 8 bits) para reduzir drasticamente seu tamanho e consumo de memória. Nascido do projeto `llama.cpp` para executar Grandes Modelos de Linguagem (LLMs) em CPUs, seu uso se expandiu recentemente para o ecossistema de modelos de difusão de imagens. O objetivo principal: reduzir o consumo de memória; A principal vantagem do GGUF é a quantização. Um modelo que em formato FP16 (`.safetensors`) ocuparia 14 GB de VRAM, em formato GGUF quantizado para 4 bits (`q4KM`) pode ocupar menos de 5 GB. Isso permite: - Executar modelos enormes em GPUs com menos VRAM. - Carregar mais componentes na memória simultaneamente. - Executar modelos em CPUs de forma eficiente. GGUF no mundo dos LLMs (uso clássico); O uso primário do GGUF é para modelos de linguagem. Interfaces como o LM Studio ou o Ollama usam arquivos GGUF para executar chatbots poderosos (como Llama, Mistral) em hardware de consumo, aproveitando principalmente a CPU. GGUF no mundo da difusão (uso moderno no ComfyUI); Recentemente, a comunidade começou a aplicar as vantagens da quantização GGUF também aos componentes de processamento. No ComfyUI, através de nós especializados (`Load GGUF Model`), é possível carregar versões GGUF de: - Codificador de texto (CLIP): Carregar um CLIP quantizado reduz significativamente seu impacto na VRAM, liberando recursos preciosos para o modelo UNet. Este é o uso mais comum e eficaz. - UNet: Também existem experimentos para quantizar toda a UNet em formato GGUF. Embora isso ofereça a máxima economia de memória, pode levar a uma perda de qualidade mais perceptível na imagem final em comparação com o uso de uma UNet em formato FP16. É uma ferramenta versátil para o gerenciamento avançado de memória, permitindo que os usuários executem fluxos de trabalho cada vez mais complexos em hardware de consumo, equilibrando habilmente o compromisso entre o consumo de VRAM e a qualidade da saída. Decifrando as nomenclaturas de quantização (por exemplo, `Q4KM`); Ao baixar um modelo GGUF, o nome do arquivo geralmente contém um acrônimo que descreve o método de quantização usado. Entendê-lo ajuda a escolher o equilíbrio certo entre tamanho e qualidade. Veja como lê-lo: - `Q` seguido por um número (por exemplo, `Q4`, `Q5`, `Q8`): Indica o número de bits usados para cada peso. `Q8` usa 8 bits (maior qualidade, arquivo maior), `Q4` usa 4 bits (menor qualidade, arquivo menor). - `K`: Indica uma variante \"K-Quant\". É uma técnica de quantização aprimorada que tenta preservar melhor a qualidade da informação, especialmente para os pesos mais importantes. Os modelos `K` são frequentemente a escolha recomendada. - `0` ou `1` (por exemplo, `Q40`, `Q51`): Indicam diferentes versões do mesmo método. `0` é a versão \"pura\" de 4 bits, enquanto `1` é uma versão mista que usa uma precisão ligeiramente maior (5 bits) para alguns pesos, oferecendo uma pequena melhoria de qualidade com um arquivo ligeiramente maior. - `S`, `M`, `L` (por exemplo, `Q4KS`): Indicam os tamanhos do modelo (\"Pequeno\", \"Médio\", \"Grande\"). Eles não se referem à quantização em si, mas a diferentes \"tamanhos\" do modelo original. Exemplos práticos: - `Q80`: Quantização de 8 bits. A mais alta qualidade entre as versões GGUF, mas também a mais pesada. - `Q5KM`: Quantização \"K-Quant\" de 5 bits, versão \"Média\". Um excelente compromisso entre qualidade e tamanho. - `Q40`: Quantização \"pura\" de 4 bits. A versão menor e mais leve, mas com a maior perda de qualidade. Frequentemente usada para executar modelos enormes em hardware muito limitado."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"de","advanced-topics","Advanced Topics","cyan","dit","DiT: Die Diffusion Transformers","\u003Cp>Ein \u003Cstrong>DiT (Diffusion Transformer)\u003C/strong> ist eine neue Architektur für Diffusionsmodelle, die \u003Cstrong>das traditionelle UNet durch einen Transformer ersetzt\u003C/strong>. [...\u003C/p>\n","Ein DiT (Diffusion Transformer) ist eine neue Architektur für Diffusionsmodelle, die das traditionelle UNet durch einen Transformer ersetzt. [...","\u003Cp>Ein \u003Cstrong>DiT (Diffusion Transformer)\u003C/strong> ist eine neue Architektur für Diffusionsmodelle, die \u003Cstrong>das traditionelle UNet durch einen Transformer ersetzt\u003C/strong>. [1] Es ist eine Weiterentwicklung, die Innovationen aus der Welt der Großen Sprachmodelle (LLMs) entlehnt und sie auf die Bilderzeugung anwendet, was eine größere Skalierbarkeit und Effizienz verspricht.\u003C/p>\n\u003Ch3>Warum das UNet ersetzen?\u003C/h3>\n\u003Cp>Das \u003Cstrong>UNet\u003C/strong> war jahrelang die dominierende Architektur, hat aber inhärente Grenzen in seiner Fähigkeit zu &quot;skalieren&quot;, d.h. seine Leistung zu verbessern, wenn seine Größe und Rechenleistung zunehmen.\u003C/p>\n\u003Cp>Die \u003Cstrong>Transformer\u003C/strong>-Architektur hat dank ihres \u003Cstrong>Attention\u003C/strong>-Mechanismus in LLMs bewiesen, dass sie unglaublich effektiv bei der Verwaltung und Verknüpfung großer Datenmengen ist. Die Idee hinter den DiTs ist: &quot;Was wäre, wenn wir ein Bild nicht als ein Gitter von Pixeln behandeln, sondern als eine Sequenz von &#39;Patches&#39; (Stücken), ähnlich wie ein Transformer eine Sequenz von Wörtern behandelt?&quot;. [1]\u003C/p>\n\u003Ch3>Wie funktioniert ein DiT?\u003C/h3>\n\u003Col>\n\u003Cli>Das latente Bild wird in eine Reihe von &quot;Patches&quot; (visuelle Token) zerlegt.\u003C/li>\n\u003Cli>Diese Token werden von einem Transformer verarbeitet, der den Attention-Mechanismus verwendet, um die Beziehungen zwischen den verschiedenen Teilen des Bildes zu verstehen.\u003C/li>\n\u003Cli>Der Transformer, der durch den Prompt konditioniert ist, sagt das Rauschen voraus, das von jedem Patch entfernt werden soll.\u003C/li>\n\u003C/ol>\n\u003Cp>Dieser Ansatz hat sich als extrem skalierbar erwiesen: Je größer und leistungsfähiger der Transformer, desto besser die Ergebnisse, die die Leistung traditioneller UNets bei gleichen Ressourcen übertreffen. [1]\u003C/p>\n\u003Ch3>Konkrete Beispiele und die Zukunft\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Sora:\u003C/strong> OpenAIs revolutionäres Text-zu-Video-Modell basiert auf einer DiT-Architektur.\u003C/li>\n\u003Cli>\u003Cstrong>Stable Diffusion 3:\u003C/strong> Die neue Version des Modells von Stability AI verzichtet auf das UNet zugunsten einer DiT-Architektur, genauer gesagt \u003Cstrong>MMDiT (Multi-Modal DiT)\u003C/strong>. [2] Ein MMDiT verwendet zwei verschiedene Transformer, einen zur Verarbeitung von Textdaten und einen für Bilddaten, was ein viel tieferes und genaueres Verständnis des Prompts ermöglicht. [2]\u003C/li>\n\u003C/ul>\n\u003Cp>DiTs stellen einen grundlegenden Schritt in Richtung immer leistungsfähigerer, kohärenterer Erzeugungsmodelle dar, die in der Lage sind, die komplexen Nuancen der menschlichen Sprache zu verstehen.\u003C/p>\n",[12,15,18],{"text":13,"url":14},"Original-Paper: Skalierbare Diffusionsmodelle mit Transformern","https://arxiv.org/abs/2212.09748",{"text":16,"url":17},"Ankündigung von Stable Diffusion 3, basierend auf DiT","https://stability.ai/news/stable-diffusion-3",{"text":19,"url":20},"Erklärung der DiT-Architektur - Hugging Face","https://huggingface.co/papers/2212.09748",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"DiT: Die Diffusion Transformers. Ein DiT (Diffusion Transformer) ist eine neue Architektur für Diffusionsmodelle, die das traditionelle UNet durch einen Transformer ersetzt. Es ist eine Weiterentwicklung, die Innovationen aus der Welt der Großen Sprachmodelle (LLMs) entlehnt und sie auf die Bilderzeugung anwendet, was eine größere Skalierbarkeit und Effizienz verspricht. Warum das UNet ersetzen?; Das UNet war jahrelang die dominierende Architektur, hat aber inhärente Grenzen in seiner Fähigkeit zu \"skalieren\", d.h. seine Leistung zu verbessern, wenn seine Größe und Rechenleistung zunehmen. Die Transformer-Architektur hat dank ihres Attention-Mechanismus in LLMs bewiesen, dass sie unglaublich effektiv bei der Verwaltung und Verknüpfung großer Datenmengen ist. Die Idee hinter den DiTs ist: \"Was wäre, wenn wir ein Bild nicht als ein Gitter von Pixeln behandeln, sondern als eine Sequenz von 'Patches' (Stücken), ähnlich wie ein Transformer eine Sequenz von Wörtern behandelt?\". Wie funktioniert ein DiT?; 1. Das latente Bild wird in eine Reihe von \"Patches\" (visuelle Token) zerlegt. 2. Diese Token werden von einem Transformer verarbeitet, der den Attention-Mechanismus verwendet, um die Beziehungen zwischen den verschiedenen Teilen des Bildes zu verstehen. 3. Der Transformer, der durch den Prompt konditioniert ist, sagt das Rauschen voraus, das von jedem Patch entfernt werden soll. Dieser Ansatz hat sich als extrem skalierbar erwiesen: Je größer und leistungsfähiger der Transformer, desto besser die Ergebnisse, die die Leistung traditioneller UNets bei gleichen Ressourcen übertreffen. Konkrete Beispiele und die Zukunft; - Sora: OpenAIs revolutionäres Text-zu-Video-Modell basiert auf einer DiT-Architektur. - Stable Diffusion 3: Die neue Version des Modells von Stability AI verzichtet auf das UNet zugunsten einer DiT-Architektur, genauer gesagt MMDiT (Multi-Modal DiT). Ein MMDiT verwendet zwei verschiedene Transformer, einen zur Verarbeitung von Textdaten und einen für Bilddaten, was ein viel tieferes und genaueres Verständnis des Prompts ermöglicht. DiTs stellen einen grundlegenden Schritt in Richtung immer leistungsfähigerer, kohärenterer Erzeugungsmodelle dar, die in der Lage sind, die komplexen Nuancen der menschlichen Sprache zu verstehen."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

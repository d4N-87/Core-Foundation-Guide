<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		
		<!-- 
			English: A comprehensive set of favicon links for different platforms (Apple, standard browsers) and the web manifest for PWA capabilities.
			Italiano: Un set completo di link per le favicon per diverse piattaforme (Apple, browser standard) e il web manifest per le funzionalità PWA.
		-->
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
		
		<!-- 
			English: Sets the viewport to ensure the site is responsive and scales correctly on all devices.
			Italiano: Imposta il viewport per assicurare che il sito sia responsivo e si adatti correttamente a tutti i dispositivi.
		-->
		<meta name="viewport" content="width=device-width, initial-scale=1" />

		<!-- 
			English: SvelteKit placeholder. This is where SvelteKit injects all necessary head content, like CSS links and meta tags from `svelte:head`.
			Italiano: Segnaposto di SvelteKit. Qui è dove SvelteKit inietta tutto il contenuto necessario per l'head, come i link CSS e i meta tag da `svelte:head`.
		-->
		
		<link href="../../_app/immutable/assets/0.CHJcp1PB.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.CQW7PVBU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DdPt9bVq.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B-icdhcA.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dv9Va7Iy.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.BRxldS8A.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/PPVm8Dsz.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DsnmJJEf.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BGc_5KK-.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C7sTqTmI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DUX4vovL.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BU9Ixn5H.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.AQxqvBRY.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ctJiNxf1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D2fIjOlU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BfGA2QYN.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Cmlm8h2R.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D5MSuBSp.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C8S6TabI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D40XMBwt.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.CJIju4Kh.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/6.B0mU0U6k.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/AGRw3LU9.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BoPAQx1w.js"><!--[--><meta name="description" content="&lt;p>&lt;strong>GGUF (Georgi Gerganov Universal Format)&lt;/strong> ist ein Dateiformat, das entwickelt wurde, um &lt;strong>quantisierte&lt;/strong> neuronale Modelle zu enthalten, d.h. in For...&lt;/p>
"/>  <meta property="og:type" content="website"/> <meta property="og:url" content="http://sveltekit-prerender/de/advanced-topics/gguf"/> <meta property="og:title" content="GGUF: Quantisierung für CPU und GPU | Core Foundation Guide"/> <meta property="og:description" content="&lt;p>&lt;strong>GGUF (Georgi Gerganov Universal Format)&lt;/strong> ist ein Dateiformat, das entwickelt wurde, um &lt;strong>quantisierte&lt;/strong> neuronale Modelle zu enthalten, d.h. in For...&lt;/p>
"/>  <meta property="twitter:card" content="summary_large_image"/> <meta property="twitter:url" content="http://sveltekit-prerender/de/advanced-topics/gguf"/> <meta property="twitter:title" content="GGUF: Quantisierung für CPU und GPU | Core Foundation Guide"/> <meta property="twitter:description" content="&lt;p>&lt;strong>GGUF (Georgi Gerganov Universal Format)&lt;/strong> ist ein Dateiformat, das entwickelt wurde, um &lt;strong>quantisierte&lt;/strong> neuronale Modelle zu enthalten, d.h. in For...&lt;/p>
"/><!--]--><title>GGUF: Quantisierung für CPU und GPU | Core Foundation Guide</title>
	</head>
	<!-- 
		English: SvelteKit attribute to enable data preloading on mouse hover over links, making navigation feel faster.
		Italiano: Attributo di SvelteKit per abilitare il precaricamento dei dati al passaggio del mouse sui link, rendendo la navigazione più veloce.
	-->
	<body data-sveltekit-preload-data="hover">
		<!-- 
			English: SvelteKit placeholder for the main application body. The `display: contents` wrapper makes the div itself layout-neutral.
			Italiano: Segnaposto di SvelteKit per il corpo principale dell'applicazione. Il wrapper `display: contents` rende il div stesso neutro a livello di layout.
		-->
		<div style="display: contents"><!--[--><!--[--><!----><div class="fixed top-0 left-0 w-full h-full -z-10"></div><!----> <div class="relative z-10 isolate"><header class="fixed left-0 right-0 top-0 z-[60] w-full border-b border-cyan-900/50 bg-black/30 px-4 py-3 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl items-center justify-between"><div class="flex flex-shrink-0 items-center"><a href="#" target="_blank" rel="noopener noreferrer" aria-label="Core Foundation Guide GitHub Repository" class="transition-all duration-300 hover:scale-110 hover:drop-shadow-[0_0_8px_theme(colors.amber.400)] focus:scale-110 focus:outline-none"><img src="/logo.webp" alt="Logo" class="h-10 w-10 md:h-12 md:w-12"/></a> <div class="ml-3 min-w-0 text-xl font-bold tracking-wide text-slate-200 sm:ml-4 sm:text-2xl md:ml-6 md:text-3xl lg:text-4xl"><!--[--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">C</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">R</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">F</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">A</span><!--]--><!--[!--><span class="char-span inline-block ">T</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">G</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--]--></div></div> <svg class="hidden"><symbol id="icon-globe" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></symbol></svg> <div class="relative"><button aria-label="Change language" class="text-slate-400 transition-colors hover:text-white"><svg class="w-6 h-6"><use href="#icon-globe"></use></svg></button> <!--[!--><!--]--></div><!----></div></header><!----> <div class="flex min-h-screen flex-col pt-20"><main class="flex-grow"><!--[--><!--[--><!----><!--[--><!----><!----> <div class="flex w-full flex-grow items-center justify-center p-4 md:p-8"><div class="w-full max-w-4xl rounded-xl bg-gradient-to-br from-cyan-950/20 to-slate-950/10 backdrop-blur-lg border-2 border-cyan-500/30 shadow-2xl shadow-cyan-900/50 p-6 md:p-10" style="visibility: hidden;"><!--[--><button class="mb-8 block font-semibold text-cyan-400 transition-colors hover:text-amber-400">← Zurück zu den Artikeln</button> <article class="prose prose-invert prose-strong:text-amber-400 prose-hr:border-cyan-500/30 prose-ol:text-gray-400 lg:prose-xl"><h1>GGUF: Quantisierung für CPU und GPU</h1> <!--[--><div class="not-prose my-8"><svg class="hidden"><symbol id="icon-play" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></symbol><symbol id="icon-stop" viewBox="0 0 24 24" fill="currentColor"><path d="M6 6h12v12H6z"></path></symbol><symbol id="icon-settings" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 0 2l-.15.08a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l-.22-.38a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1 0-2l.15-.08a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"></path><circle cx="12" cy="12" r="3"></circle></symbol></svg> <!--[!--><!--]--><!----></div><!--]-->  <!----><p><strong>GGUF (Georgi Gerganov Universal Format)</strong> ist ein Dateiformat, das entwickelt wurde, um <strong>quantisierte</strong> neuronale Modelle zu enthalten, d.h. in Formate mit sehr geringer Präzision (wie 4 oder 8 Bit) konvertiert, um ihre Größe und ihren Speicherverbrauch drastisch zu reduzieren. [1]</p>
<p>Entstanden aus dem Projekt <strong><code>llama.cpp</code></strong> zur Ausführung von Großen Sprachmodellen (LLMs) auf CPUs, hat sich seine Verwendung kürzlich auch auf das Ökosystem der Bilddiffusionsmodelle ausgeweitet. [2]</p>
<h3>Der Hauptzweck: Reduzierung des Speicherverbrauchs</h3>
<p>Der Hauptvorteil von GGUF ist die <strong>Quantisierung</strong>. Ein Modell, das im FP16-Format (<code>.safetensors</code>) 14 GB VRAM belegen würde, kann im GGUF-Format, das auf 4 Bit quantisiert ist (<code>q4_K_M</code>), weniger als 5 GB belegen. Dies ermöglicht:</p>
<ul>
<li>Das Ausführen riesiger Modelle auf GPUs mit weniger VRAM.</li>
<li>Das gleichzeitige Laden mehrerer Komponenten in den Speicher.</li>
<li>Das effiziente Ausführen von Modellen auf CPUs.</li>
</ul>
<h3>GGUF in der Welt der LLMs (klassische Verwendung)</h3>
<p>Die primäre Verwendung von GGUF ist für Sprachmodelle. Schnittstellen wie LM Studio oder Ollama verwenden GGUF-Dateien, um leistungsstarke Chatbots (wie Llama, Mistral) auf Consumer-Hardware auszuführen, wobei hauptsächlich die CPU genutzt wird. [3]</p>
<h3>GGUF in der Welt der Diffusion (moderne Verwendung in ComfyUI)</h3>
<p>In jüngerer Zeit hat die Community begonnen, die Vorteile der GGUF-Quantisierung auch auf Verarbeitungskomponenten anzuwenden. In ComfyUI ist es über spezielle Knoten (<code>Load GGUF Model</code>) möglich, GGUF-Versionen von Folgendem zu laden:</p>
<ul>
<li><strong>Text Encoder (CLIP):</strong> Das Laden eines quantisierten CLIP reduziert seinen Einfluss auf den VRAM erheblich und gibt wertvolle Ressourcen für das UNet-Modell frei. Dies ist die häufigste und effektivste Verwendung.</li>
<li><strong>UNet:</strong> Es gibt auch Experimente, um das gesamte UNet im GGUF-Format zu quantisieren. Obwohl dies die maximale Speichereinsparung bietet, kann es zu einem deutlicheren Qualitätsverlust im endgültigen Bild führen, verglichen mit der Verwendung eines UNet im FP16-Format.</li>
</ul>
<p>Es ist ein vielseitiges Werkzeug für die <strong>fortgeschrittene Speicherverwaltung</strong>, das es Benutzern ermöglicht, immer komplexere Workflows auf Consumer-Hardware auszuführen und dabei den Kompromiss zwischen VRAM-Verbrauch und Ausgabequalität geschickt auszubalancieren.</p>
<h3>Entschlüsselung der Quantisierungsnomenklaturen (z. B. <code>Q4_K_M</code>)</h3>
<p>Beim Herunterladen eines GGUF-Modells enthält der Dateiname oft ein Akronym, das die verwendete Quantisierungsmethode beschreibt. Das Verständnis hilft, das richtige Gleichgewicht zwischen Größe und Qualität zu wählen. So wird es gelesen:</p>
<ul>
<li><strong><code>Q</code> gefolgt von einer Zahl (z. B. <code>Q4</code>, <code>Q5</code>, <code>Q8</code>):</strong> Gibt die Anzahl der <strong>Bits</strong> an, die für jedes Gewicht verwendet werden. <code>Q8</code> verwendet 8 Bits (höhere Qualität, größere Datei), <code>Q4</code> verwendet 4 Bits (niedrigere Qualität, kleinere Datei).</li>
<li><strong><code>_K</code>:</strong> Gibt eine &quot;K-Quant&quot;-Variante an. Es ist eine verbesserte Quantisierungstechnik, die versucht, die Qualität der Informationen besser zu erhalten, insbesondere bei den wichtigsten Gewichten. <code>_K</code>-Modelle sind oft die empfohlene Wahl.</li>
<li><strong><code>_0</code> oder <code>_1</code> (z. B. <code>Q4_0</code>, <code>Q5_1</code>):</strong> Geben verschiedene Versionen derselben Methode an. <code>_0</code> ist die &quot;reine&quot; 4-Bit-Version, während <code>_1</code> eine gemischte Version ist, die eine etwas höhere Präzision (5-Bit) für einige Gewichte verwendet und eine kleine Qualitätsverbesserung bei einer etwas größeren Datei bietet.</li>
<li><strong><code>_S</code>, <code>_M</code>, <code>_L</code> (z. B. <code>Q4_K_S</code>):</strong> Geben die Modellgrößen an (&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;). Sie beziehen sich nicht auf die Quantisierung selbst, sondern auf verschiedene &quot;Größen&quot; des ursprünglichen Modells.</li>
</ul>
<p><strong>Praktische Beispiele:</strong></p>
<ul>
<li><strong><code>Q8_0</code>:</strong> 8-Bit-Quantisierung. Höchste Qualität unter den GGUF-Versionen, aber auch die schwerste.</li>
<li><strong><code>Q5_K_M</code>:</strong> &quot;K-Quant&quot; 5-Bit-Quantisierung, &quot;Medium&quot;-Version. Ein ausgezeichneter Kompromiss zwischen Qualität und Größe.</li>
<li><strong><code>Q4_0</code>:</strong> &quot;Reine&quot; 4-Bit-Quantisierung. Die kleinste und leichteste Version, aber mit dem größten Qualitätsverlust. Wird oft verwendet, um riesige Modelle auf sehr begrenzter Hardware auszuführen.</li>
</ul>
<!---->  <!--[--><hr/> <h2>Quellen</h2> <ol><!--[--><li><a href="https://huggingface.co/blog/gguf" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Offizielle GGUF-Ankündigung im Hugging Face-Blog</a></li><li><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">GitHub-Repository von llama.cpp</a></li><li><a href="https://comfyanonymous.github.io/ComfyUI_examples/llm/" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Beispiel für einen Workflow in ComfyUI mit GGUF Loader</a></li><!--]--></ol><!--]--></article><!--]--></div></div><!----><!--]--><!----><!--]--><!--]--></main> <svg class="hidden"><symbol id="icon-github" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.91 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></symbol></svg> <footer class="w-full border-t border-cyan-900/50 bg-black/30 px-4 py-6 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl flex-col items-center justify-between gap-4 sm:flex-row"><p class="text-sm text-slate-400">© 2025 Core Foundation Guide. An interactive field manual for AI concepts.</p>  <a href="#" target="_blank" rel="noopener noreferrer" aria-label="GitHub Repository" class="text-slate-400 transition-colors hover:text-white"><svg class="h-6 w-6"><use href="#icon-github"></use></svg></a></div></footer><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1erws2 = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.CQW7PVBU.js"),
						import("../../_app/immutable/entry/app.BRxldS8A.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 6],
							data: [{type:"data",data:{translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}}},uses:{}},null,{type:"data",data:{post:{lang:"de",categorySlug:"advanced-topics",categoryName:"Advanced Topics",categoryColor:"cyan",slug:"gguf",title:"GGUF: Quantisierung für CPU und GPU",excerpt:"\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> ist ein Dateiformat, das entwickelt wurde, um \u003Cstrong>quantisierte\u003C/strong> neuronale Modelle zu enthalten, d.h. in For...\u003C/p>\n",plainExcerpt:"GGUF (Georgi Gerganov Universal Format) ist ein Dateiformat, das entwickelt wurde, um quantisierte neuronale Modelle zu enthalten, d.h. in For...",content:"\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> ist ein Dateiformat, das entwickelt wurde, um \u003Cstrong>quantisierte\u003C/strong> neuronale Modelle zu enthalten, d.h. in Formate mit sehr geringer Präzision (wie 4 oder 8 Bit) konvertiert, um ihre Größe und ihren Speicherverbrauch drastisch zu reduzieren. [1]\u003C/p>\n\u003Cp>Entstanden aus dem Projekt \u003Cstrong>\u003Ccode>llama.cpp\u003C/code>\u003C/strong> zur Ausführung von Großen Sprachmodellen (LLMs) auf CPUs, hat sich seine Verwendung kürzlich auch auf das Ökosystem der Bilddiffusionsmodelle ausgeweitet. [2]\u003C/p>\n\u003Ch3>Der Hauptzweck: Reduzierung des Speicherverbrauchs\u003C/h3>\n\u003Cp>Der Hauptvorteil von GGUF ist die \u003Cstrong>Quantisierung\u003C/strong>. Ein Modell, das im FP16-Format (\u003Ccode>.safetensors\u003C/code>) 14 GB VRAM belegen würde, kann im GGUF-Format, das auf 4 Bit quantisiert ist (\u003Ccode>q4_K_M\u003C/code>), weniger als 5 GB belegen. Dies ermöglicht:\u003C/p>\n\u003Cul>\n\u003Cli>Das Ausführen riesiger Modelle auf GPUs mit weniger VRAM.\u003C/li>\n\u003Cli>Das gleichzeitige Laden mehrerer Komponenten in den Speicher.\u003C/li>\n\u003Cli>Das effiziente Ausführen von Modellen auf CPUs.\u003C/li>\n\u003C/ul>\n\u003Ch3>GGUF in der Welt der LLMs (klassische Verwendung)\u003C/h3>\n\u003Cp>Die primäre Verwendung von GGUF ist für Sprachmodelle. Schnittstellen wie LM Studio oder Ollama verwenden GGUF-Dateien, um leistungsstarke Chatbots (wie Llama, Mistral) auf Consumer-Hardware auszuführen, wobei hauptsächlich die CPU genutzt wird. [3]\u003C/p>\n\u003Ch3>GGUF in der Welt der Diffusion (moderne Verwendung in ComfyUI)\u003C/h3>\n\u003Cp>In jüngerer Zeit hat die Community begonnen, die Vorteile der GGUF-Quantisierung auch auf Verarbeitungskomponenten anzuwenden. In ComfyUI ist es über spezielle Knoten (\u003Ccode>Load GGUF Model\u003C/code>) möglich, GGUF-Versionen von Folgendem zu laden:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Text Encoder (CLIP):\u003C/strong> Das Laden eines quantisierten CLIP reduziert seinen Einfluss auf den VRAM erheblich und gibt wertvolle Ressourcen für das UNet-Modell frei. Dies ist die häufigste und effektivste Verwendung.\u003C/li>\n\u003Cli>\u003Cstrong>UNet:\u003C/strong> Es gibt auch Experimente, um das gesamte UNet im GGUF-Format zu quantisieren. Obwohl dies die maximale Speichereinsparung bietet, kann es zu einem deutlicheren Qualitätsverlust im endgültigen Bild führen, verglichen mit der Verwendung eines UNet im FP16-Format.\u003C/li>\n\u003C/ul>\n\u003Cp>Es ist ein vielseitiges Werkzeug für die \u003Cstrong>fortgeschrittene Speicherverwaltung\u003C/strong>, das es Benutzern ermöglicht, immer komplexere Workflows auf Consumer-Hardware auszuführen und dabei den Kompromiss zwischen VRAM-Verbrauch und Ausgabequalität geschickt auszubalancieren.\u003C/p>\n\u003Ch3>Entschlüsselung der Quantisierungsnomenklaturen (z. B. \u003Ccode>Q4_K_M\u003C/code>)\u003C/h3>\n\u003Cp>Beim Herunterladen eines GGUF-Modells enthält der Dateiname oft ein Akronym, das die verwendete Quantisierungsmethode beschreibt. Das Verständnis hilft, das richtige Gleichgewicht zwischen Größe und Qualität zu wählen. So wird es gelesen:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q\u003C/code> gefolgt von einer Zahl (z. B. \u003Ccode>Q4\u003C/code>, \u003Ccode>Q5\u003C/code>, \u003Ccode>Q8\u003C/code>):\u003C/strong> Gibt die Anzahl der \u003Cstrong>Bits\u003C/strong> an, die für jedes Gewicht verwendet werden. \u003Ccode>Q8\u003C/code> verwendet 8 Bits (höhere Qualität, größere Datei), \u003Ccode>Q4\u003C/code> verwendet 4 Bits (niedrigere Qualität, kleinere Datei).\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_K\u003C/code>:\u003C/strong> Gibt eine &quot;K-Quant&quot;-Variante an. Es ist eine verbesserte Quantisierungstechnik, die versucht, die Qualität der Informationen besser zu erhalten, insbesondere bei den wichtigsten Gewichten. \u003Ccode>_K\u003C/code>-Modelle sind oft die empfohlene Wahl.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_0\u003C/code> oder \u003Ccode>_1\u003C/code> (z. B. \u003Ccode>Q4_0\u003C/code>, \u003Ccode>Q5_1\u003C/code>):\u003C/strong> Geben verschiedene Versionen derselben Methode an. \u003Ccode>_0\u003C/code> ist die &quot;reine&quot; 4-Bit-Version, während \u003Ccode>_1\u003C/code> eine gemischte Version ist, die eine etwas höhere Präzision (5-Bit) für einige Gewichte verwendet und eine kleine Qualitätsverbesserung bei einer etwas größeren Datei bietet.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_S\u003C/code>, \u003Ccode>_M\u003C/code>, \u003Ccode>_L\u003C/code> (z. B. \u003Ccode>Q4_K_S\u003C/code>):\u003C/strong> Geben die Modellgrößen an (&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;). Sie beziehen sich nicht auf die Quantisierung selbst, sondern auf verschiedene &quot;Größen&quot; des ursprünglichen Modells.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Praktische Beispiele:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q8_0\u003C/code>:\u003C/strong> 8-Bit-Quantisierung. Höchste Qualität unter den GGUF-Versionen, aber auch die schwerste.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q5_K_M\u003C/code>:\u003C/strong> &quot;K-Quant&quot; 5-Bit-Quantisierung, &quot;Medium&quot;-Version. Ein ausgezeichneter Kompromiss zwischen Qualität und Größe.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q4_0\u003C/code>:\u003C/strong> &quot;Reine&quot; 4-Bit-Quantisierung. Die kleinste und leichteste Version, aber mit dem größten Qualitätsverlust. Wird oft verwendet, um riesige Modelle auf sehr begrenzter Hardware auszuführen.\u003C/li>\n\u003C/ul>\n",sources:[{text:"Offizielle GGUF-Ankündigung im Hugging Face-Blog",url:"https://huggingface.co/blog/gguf"},{text:"GitHub-Repository von llama.cpp",url:"https://github.com/ggerganov/llama.cpp"},{text:"Beispiel für einen Workflow in ComfyUI mit GGUF Loader",url:"https://comfyanonymous.github.io/ComfyUI_examples/llm/"}]},translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}},seo:{title:"GGUF: Quantisierung für CPU und GPU",description:"\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> ist ein Dateiformat, das entwickelt wurde, um \u003Cstrong>quantisierte\u003C/strong> neuronale Modelle zu enthalten, d.h. in For...\u003C/p>\n"},textContent:"GGUF: Quantisierung für CPU und GPU. GGUF (Georgi Gerganov Universal Format) ist ein Dateiformat, das entwickelt wurde, um quantisierte neuronale Modelle zu enthalten, d.h. in Formate mit sehr geringer Präzision (wie 4 oder 8 Bit) konvertiert, um ihre Größe und ihren Speicherverbrauch drastisch zu reduzieren. Entstanden aus dem Projekt `llama.cpp` zur Ausführung von Großen Sprachmodellen (LLMs) auf CPUs, hat sich seine Verwendung kürzlich auch auf das Ökosystem der Bilddiffusionsmodelle ausgeweitet. Der Hauptzweck: Reduzierung des Speicherverbrauchs; Der Hauptvorteil von GGUF ist die Quantisierung. Ein Modell, das im FP16-Format (`.safetensors`) 14 GB VRAM belegen würde, kann im GGUF-Format, das auf 4 Bit quantisiert ist (`q4KM`), weniger als 5 GB belegen. Dies ermöglicht: - Das Ausführen riesiger Modelle auf GPUs mit weniger VRAM. - Das gleichzeitige Laden mehrerer Komponenten in den Speicher. - Das effiziente Ausführen von Modellen auf CPUs. GGUF in der Welt der LLMs (klassische Verwendung); Die primäre Verwendung von GGUF ist für Sprachmodelle. Schnittstellen wie LM Studio oder Ollama verwenden GGUF-Dateien, um leistungsstarke Chatbots (wie Llama, Mistral) auf Consumer-Hardware auszuführen, wobei hauptsächlich die CPU genutzt wird. GGUF in der Welt der Diffusion (moderne Verwendung in ComfyUI); In jüngerer Zeit hat die Community begonnen, die Vorteile der GGUF-Quantisierung auch auf Verarbeitungskomponenten anzuwenden. In ComfyUI ist es über spezielle Knoten (`Load GGUF Model`) möglich, GGUF-Versionen von Folgendem zu laden: - Text Encoder (CLIP): Das Laden eines quantisierten CLIP reduziert seinen Einfluss auf den VRAM erheblich und gibt wertvolle Ressourcen für das UNet-Modell frei. Dies ist die häufigste und effektivste Verwendung. - UNet: Es gibt auch Experimente, um das gesamte UNet im GGUF-Format zu quantisieren. Obwohl dies die maximale Speichereinsparung bietet, kann es zu einem deutlicheren Qualitätsverlust im endgültigen Bild führen, verglichen mit der Verwendung eines UNet im FP16-Format. Es ist ein vielseitiges Werkzeug für die fortgeschrittene Speicherverwaltung, das es Benutzern ermöglicht, immer komplexere Workflows auf Consumer-Hardware auszuführen und dabei den Kompromiss zwischen VRAM-Verbrauch und Ausgabequalität geschickt auszubalancieren. Entschlüsselung der Quantisierungsnomenklaturen (z. B. `Q4KM`); Beim Herunterladen eines GGUF-Modells enthält der Dateiname oft ein Akronym, das die verwendete Quantisierungsmethode beschreibt. Das Verständnis hilft, das richtige Gleichgewicht zwischen Größe und Qualität zu wählen. So wird es gelesen: - `Q` gefolgt von einer Zahl (z. B. `Q4`, `Q5`, `Q8`): Gibt die Anzahl der Bits an, die für jedes Gewicht verwendet werden. `Q8` verwendet 8 Bits (höhere Qualität, größere Datei), `Q4` verwendet 4 Bits (niedrigere Qualität, kleinere Datei). - `K`: Gibt eine \"K-Quant\"-Variante an. Es ist eine verbesserte Quantisierungstechnik, die versucht, die Qualität der Informationen besser zu erhalten, insbesondere bei den wichtigsten Gewichten. `K`-Modelle sind oft die empfohlene Wahl. - `0` oder `1` (z. B. `Q40`, `Q51`): Geben verschiedene Versionen derselben Methode an. `0` ist die \"reine\" 4-Bit-Version, während `1` eine gemischte Version ist, die eine etwas höhere Präzision (5-Bit) für einige Gewichte verwendet und eine kleine Qualitätsverbesserung bei einer etwas größeren Datei bietet. - `S`, `M`, `L` (z. B. `Q4KS`): Geben die Modellgrößen an (\"Small\", \"Medium\", \"Large\"). Sie beziehen sich nicht auf die Quantisierung selbst, sondern auf verschiedene \"Größen\" des ursprünglichen Modells. Praktische Beispiele: - `Q80`: 8-Bit-Quantisierung. Höchste Qualität unter den GGUF-Versionen, aber auch die schwerste. - `Q5KM`: \"K-Quant\" 5-Bit-Quantisierung, \"Medium\"-Version. Ein ausgezeichneter Kompromiss zwischen Qualität und Größe. - `Q40`: \"Reine\" 4-Bit-Quantisierung. Die kleinste und leichteste Version, aber mit dem größten Qualitätsverlust. Wird oft verwendet, um riesige Modelle auf sehr begrenzter Hardware auszuführen."},uses:{params:["lang","category","slug"],parent:1}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
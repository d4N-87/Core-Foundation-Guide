<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		
		<!-- 
			English: A comprehensive set of favicon links for different platforms (Apple, standard browsers) and the web manifest for PWA capabilities.
			Italiano: Un set completo di link per le favicon per diverse piattaforme (Apple, browser standard) e il web manifest per le funzionalità PWA.
		-->
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
		
		<!-- 
			English: Sets the viewport to ensure the site is responsive and scales correctly on all devices.
			Italiano: Imposta il viewport per assicurare che il sito sia responsivo e si adatti correttamente a tutti i dispositivi.
		-->
		<meta name="viewport" content="width=device-width, initial-scale=1" />

		<!-- 
			English: SvelteKit placeholder. This is where SvelteKit injects all necessary head content, like CSS links and meta tags from `svelte:head`.
			Italiano: Segnaposto di SvelteKit. Qui è dove SvelteKit inietta tutto il contenuto necessario per l'head, come i link CSS e i meta tag da `svelte:head`.
		-->
		
		<link href="../../_app/immutable/assets/0.CHJcp1PB.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.CQW7PVBU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DdPt9bVq.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B-icdhcA.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dv9Va7Iy.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.BRxldS8A.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/PPVm8Dsz.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DsnmJJEf.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BGc_5KK-.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C7sTqTmI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DUX4vovL.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BU9Ixn5H.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.AQxqvBRY.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ctJiNxf1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D2fIjOlU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BfGA2QYN.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Cmlm8h2R.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D5MSuBSp.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C8S6TabI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D40XMBwt.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.CJIju4Kh.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/6.B0mU0U6k.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/AGRw3LU9.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BoPAQx1w.js"><!--[--><meta name="description" content="&lt;p>&lt;strong>Token&lt;/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...&lt;/p>
"/>  <meta property="og:type" content="website"/> <meta property="og:url" content="http://sveltekit-prerender/de/advanced-topics/tokens"/> <meta property="og:title" content="Token: Die Bausteine der Sprache | Core Foundation Guide"/> <meta property="og:description" content="&lt;p>&lt;strong>Token&lt;/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...&lt;/p>
"/>  <meta property="twitter:card" content="summary_large_image"/> <meta property="twitter:url" content="http://sveltekit-prerender/de/advanced-topics/tokens"/> <meta property="twitter:title" content="Token: Die Bausteine der Sprache | Core Foundation Guide"/> <meta property="twitter:description" content="&lt;p>&lt;strong>Token&lt;/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...&lt;/p>
"/><!--]--><title>Token: Die Bausteine der Sprache | Core Foundation Guide</title>
	</head>
	<!-- 
		English: SvelteKit attribute to enable data preloading on mouse hover over links, making navigation feel faster.
		Italiano: Attributo di SvelteKit per abilitare il precaricamento dei dati al passaggio del mouse sui link, rendendo la navigazione più veloce.
	-->
	<body data-sveltekit-preload-data="hover">
		<!-- 
			English: SvelteKit placeholder for the main application body. The `display: contents` wrapper makes the div itself layout-neutral.
			Italiano: Segnaposto di SvelteKit per il corpo principale dell'applicazione. Il wrapper `display: contents` rende il div stesso neutro a livello di layout.
		-->
		<div style="display: contents"><!--[--><!--[--><!----><div class="fixed top-0 left-0 w-full h-full -z-10"></div><!----> <div class="relative z-10 isolate"><header class="fixed left-0 right-0 top-0 z-[60] w-full border-b border-cyan-900/50 bg-black/30 px-4 py-3 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl items-center justify-between"><div class="flex flex-shrink-0 items-center"><a href="#" target="_blank" rel="noopener noreferrer" aria-label="Core Foundation Guide GitHub Repository" class="transition-all duration-300 hover:scale-110 hover:drop-shadow-[0_0_8px_theme(colors.amber.400)] focus:scale-110 focus:outline-none"><img src="/logo.webp" alt="Logo" class="h-10 w-10 md:h-12 md:w-12"/></a> <div class="ml-3 min-w-0 text-xl font-bold tracking-wide text-slate-200 sm:ml-4 sm:text-2xl md:ml-6 md:text-3xl lg:text-4xl"><!--[--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">C</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">R</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">F</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">A</span><!--]--><!--[!--><span class="char-span inline-block ">T</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">G</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--]--></div></div> <svg class="hidden"><symbol id="icon-globe" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></symbol></svg> <div class="relative"><button aria-label="Change language" class="text-slate-400 transition-colors hover:text-white"><svg class="w-6 h-6"><use href="#icon-globe"></use></svg></button> <!--[!--><!--]--></div><!----></div></header><!----> <div class="flex min-h-screen flex-col pt-20"><main class="flex-grow"><!--[--><!--[--><!----><!--[--><!----><!----> <div class="flex w-full flex-grow items-center justify-center p-4 md:p-8"><div class="w-full max-w-4xl rounded-xl bg-gradient-to-br from-cyan-950/20 to-slate-950/10 backdrop-blur-lg border-2 border-cyan-500/30 shadow-2xl shadow-cyan-900/50 p-6 md:p-10" style="visibility: hidden;"><!--[--><button class="mb-8 block font-semibold text-cyan-400 transition-colors hover:text-amber-400">← Zurück zu den Artikeln</button> <article class="prose prose-invert prose-strong:text-amber-400 prose-hr:border-cyan-500/30 prose-ol:text-gray-400 lg:prose-xl"><h1>Token: Die Bausteine der Sprache</h1> <!--[--><div class="not-prose my-8"><svg class="hidden"><symbol id="icon-play" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></symbol><symbol id="icon-stop" viewBox="0 0 24 24" fill="currentColor"><path d="M6 6h12v12H6z"></path></symbol><symbol id="icon-settings" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 0 2l-.15.08a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l-.22-.38a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1 0-2l.15-.08a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"></path><circle cx="12" cy="12" r="3"></circle></symbol></svg> <!--[!--><!--]--><!----></div><!--]-->  <!----><p><strong>Token</strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die &quot;Bausteine&quot;, mit denen das Modell unseren Prompt liest und versteht.</p>
<p>Ein Token ist <strong>nicht unbedingt ein ganzes Wort</strong>. Der Prozess der <strong>Tokenisierung</strong> verwendet ein vordefiniertes Vokabular, um den Text in Teile zu zerlegen, die das Modell kennt. [3]</p>
<h3>Beispiele für die Tokenisierung</h3>
<p>Betrachten wir das Wort <code>unbeschreiblich</code>. Ein Tokenizer könnte es in mehrere ihm bekannte Token zerlegen:
<code>un</code> + <code>be</code> + <code>schreib</code> + <code>lich</code></p>
<ul>
<li><strong>Häufige Wörter</strong> (z. B. <code>Katze</code>, <code>der</code>, <code>ein</code>) sind oft ein einzelnes Token.</li>
<li><strong>Komplexe oder seltene Wörter</strong> werden in Unterwörter zerlegt.</li>
<li><strong>Leerzeichen und Satzzeichen</strong> werden als separate Token behandelt.</li>
</ul>
<p>Dieser Ansatz ermöglicht es dem Modell, ein praktisch unendliches Vokabular aus einer endlichen Anzahl von Token (normalerweise zwischen 30.000 und 50.000) zu verwalten. [1]</p>
<h3>Die Token-Grenze und die Entwicklung der Modelle</h3>
<p>Jeder Text-Encoder hat eine <strong>maximale Token-Grenze</strong>, die er in einem einzigen &quot;Chunk&quot; verarbeiten kann. Diese Grenze war lange Zeit eine der Haupteinschränkungen im Prompt-Engineering.</p>
<ul>
<li><p><strong>Alte Architekturen (z. B. Stable Diffusion 1.5, SDXL):</strong>
Diese Modelle verwenden Text-Encoder (CLIP) mit einer Grenze von <strong>75 Token</strong> pro Chunk. [3] Wenn ein Prompt länger ist, wird er in mehrere Chunks aufgeteilt, aber das Verständnis des Kontexts zwischen einem Block und dem nächsten ist viel schwächer. Dies hat die Benutzer gezwungen, die wichtigsten Konzepte am Anfang des Prompts zu konzentrieren.</p>
</li>
<li><p><strong>Neue Architekturen (z. B. FLUX.1):</strong>
Modelle der neuen Generation, wie <strong>FLUX.1</strong>, sind darauf ausgelegt, diese Einschränkung zu überwinden. FLUX.1 verwendet einen viel leistungsfähigeren Text-Encoder (basierend auf T5-XXL), der speziell darauf trainiert wurde, lange und komplexe Prompts nativ zu verstehen. [2] Dies ermöglicht einen viel natürlicheren und detaillierteren Ausdruck, ohne sich um die künstliche Grenze von 75 Token kümmern zu müssen.</p>
</li>
</ul>
<p>Das Verständnis des Konzepts der Token und der Einschränkungen der verschiedenen Modelle ist grundlegend, um effektive Prompts zu schreiben und die Fähigkeiten jeder Architektur optimal zu nutzen.</p>
<!---->  <!--[--><hr/> <h2>Quellen</h2> <ol><!--[--><li><a href="https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Einführung in die Tokenisierung - Hugging Face</a></li><li><a href="https://blackforestlabs.ai/announcing-flux/" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Ankündigung des FLUX.1-Modells von Black Forest Labs</a></li><li><a href="https://stable-diffusion-art.com/token/" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Erklärung von Token im Kontext von Stable Diffusion</a></li><!--]--></ol><!--]--></article><!--]--></div></div><!----><!--]--><!----><!--]--><!--]--></main> <svg class="hidden"><symbol id="icon-github" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.91 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></symbol></svg> <footer class="w-full border-t border-cyan-900/50 bg-black/30 px-4 py-6 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl flex-col items-center justify-between gap-4 sm:flex-row"><p class="text-sm text-slate-400">© 2025 Core Foundation Guide. An interactive field manual for AI concepts.</p>  <a href="#" target="_blank" rel="noopener noreferrer" aria-label="GitHub Repository" class="text-slate-400 transition-colors hover:text-white"><svg class="h-6 w-6"><use href="#icon-github"></use></svg></a></div></footer><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1erws2 = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.CQW7PVBU.js"),
						import("../../_app/immutable/entry/app.BRxldS8A.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 6],
							data: [{type:"data",data:{translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}}},uses:{}},null,{type:"data",data:{post:{lang:"de",categorySlug:"advanced-topics",categoryName:"Advanced Topics",categoryColor:"cyan",slug:"tokens",title:"Token: Die Bausteine der Sprache",excerpt:"\u003Cp>\u003Cstrong>Token\u003C/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...\u003C/p>\n",plainExcerpt:"Token sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...",content:"\u003Cp>\u003Cstrong>Token\u003C/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die &quot;Bausteine&quot;, mit denen das Modell unseren Prompt liest und versteht.\u003C/p>\n\u003Cp>Ein Token ist \u003Cstrong>nicht unbedingt ein ganzes Wort\u003C/strong>. Der Prozess der \u003Cstrong>Tokenisierung\u003C/strong> verwendet ein vordefiniertes Vokabular, um den Text in Teile zu zerlegen, die das Modell kennt. [3]\u003C/p>\n\u003Ch3>Beispiele für die Tokenisierung\u003C/h3>\n\u003Cp>Betrachten wir das Wort \u003Ccode>unbeschreiblich\u003C/code>. Ein Tokenizer könnte es in mehrere ihm bekannte Token zerlegen:\n\u003Ccode>un\u003C/code> + \u003Ccode>be\u003C/code> + \u003Ccode>schreib\u003C/code> + \u003Ccode>lich\u003C/code>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Häufige Wörter\u003C/strong> (z. B. \u003Ccode>Katze\u003C/code>, \u003Ccode>der\u003C/code>, \u003Ccode>ein\u003C/code>) sind oft ein einzelnes Token.\u003C/li>\n\u003Cli>\u003Cstrong>Komplexe oder seltene Wörter\u003C/strong> werden in Unterwörter zerlegt.\u003C/li>\n\u003Cli>\u003Cstrong>Leerzeichen und Satzzeichen\u003C/strong> werden als separate Token behandelt.\u003C/li>\n\u003C/ul>\n\u003Cp>Dieser Ansatz ermöglicht es dem Modell, ein praktisch unendliches Vokabular aus einer endlichen Anzahl von Token (normalerweise zwischen 30.000 und 50.000) zu verwalten. [1]\u003C/p>\n\u003Ch3>Die Token-Grenze und die Entwicklung der Modelle\u003C/h3>\n\u003Cp>Jeder Text-Encoder hat eine \u003Cstrong>maximale Token-Grenze\u003C/strong>, die er in einem einzigen &quot;Chunk&quot; verarbeiten kann. Diese Grenze war lange Zeit eine der Haupteinschränkungen im Prompt-Engineering.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>\u003Cstrong>Alte Architekturen (z. B. Stable Diffusion 1.5, SDXL):\u003C/strong>\nDiese Modelle verwenden Text-Encoder (CLIP) mit einer Grenze von \u003Cstrong>75 Token\u003C/strong> pro Chunk. [3] Wenn ein Prompt länger ist, wird er in mehrere Chunks aufgeteilt, aber das Verständnis des Kontexts zwischen einem Block und dem nächsten ist viel schwächer. Dies hat die Benutzer gezwungen, die wichtigsten Konzepte am Anfang des Prompts zu konzentrieren.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Neue Architekturen (z. B. FLUX.1):\u003C/strong>\nModelle der neuen Generation, wie \u003Cstrong>FLUX.1\u003C/strong>, sind darauf ausgelegt, diese Einschränkung zu überwinden. FLUX.1 verwendet einen viel leistungsfähigeren Text-Encoder (basierend auf T5-XXL), der speziell darauf trainiert wurde, lange und komplexe Prompts nativ zu verstehen. [2] Dies ermöglicht einen viel natürlicheren und detaillierteren Ausdruck, ohne sich um die künstliche Grenze von 75 Token kümmern zu müssen.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>Das Verständnis des Konzepts der Token und der Einschränkungen der verschiedenen Modelle ist grundlegend, um effektive Prompts zu schreiben und die Fähigkeiten jeder Architektur optimal zu nutzen.\u003C/p>\n",sources:[{text:"Einführung in die Tokenisierung - Hugging Face",url:"https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt"},{text:"Ankündigung des FLUX.1-Modells von Black Forest Labs",url:"https://blackforestlabs.ai/announcing-flux/"},{text:"Erklärung von Token im Kontext von Stable Diffusion",url:"https://stable-diffusion-art.com/token/"}]},translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}},seo:{title:"Token: Die Bausteine der Sprache",description:"\u003Cp>\u003Cstrong>Token\u003C/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...\u003C/p>\n"},textContent:"Token: Die Bausteine der Sprache. Token sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. Sie sind die \"Bausteine\", mit denen das Modell unseren Prompt liest und versteht. Ein Token ist nicht unbedingt ein ganzes Wort. Der Prozess der Tokenisierung verwendet ein vordefiniertes Vokabular, um den Text in Teile zu zerlegen, die das Modell kennt. Beispiele für die Tokenisierung; Betrachten wir das Wort `unbeschreiblich`. Ein Tokenizer könnte es in mehrere ihm bekannte Token zerlegen: `un` + `be` + `schreib` + `lich` - Häufige Wörter (z. B. `Katze`, `der`, `ein`) sind oft ein einzelnes Token. - Komplexe oder seltene Wörter werden in Unterwörter zerlegt. - Leerzeichen und Satzzeichen werden als separate Token behandelt. Dieser Ansatz ermöglicht es dem Modell, ein praktisch unendliches Vokabular aus einer endlichen Anzahl von Token (normalerweise zwischen 30.000 und 50.000) zu verwalten. Die Token-Grenze und die Entwicklung der Modelle; Jeder Text-Encoder hat eine maximale Token-Grenze, die er in einem einzigen \"Chunk\" verarbeiten kann. Diese Grenze war lange Zeit eine der Haupteinschränkungen im Prompt-Engineering. - Alte Architekturen (z. B. Stable Diffusion 1.5, SDXL): Diese Modelle verwenden Text-Encoder (CLIP) mit einer Grenze von 75 Token pro Chunk. Wenn ein Prompt länger ist, wird er in mehrere Chunks aufgeteilt, aber das Verständnis des Kontexts zwischen einem Block und dem nächsten ist viel schwächer. Dies hat die Benutzer gezwungen, die wichtigsten Konzepte am Anfang des Prompts zu konzentrieren. - Neue Architekturen (z. B. FLUX.1): Modelle der neuen Generation, wie FLUX.1, sind darauf ausgelegt, diese Einschränkung zu überwinden. FLUX.1 verwendet einen viel leistungsfähigeren Text-Encoder (basierend auf T5-XXL), der speziell darauf trainiert wurde, lange und komplexe Prompts nativ zu verstehen. Dies ermöglicht einen viel natürlicheren und detaillierteren Ausdruck, ohne sich um die künstliche Grenze von 75 Token kümmern zu müssen. Das Verständnis des Konzepts der Token und der Einschränkungen der verschiedenen Modelle ist grundlegend, um effektive Prompts zu schreiben und die Fähigkeiten jeder Architektur optimal zu nutzen."},uses:{params:["lang","category","slug"],parent:1}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
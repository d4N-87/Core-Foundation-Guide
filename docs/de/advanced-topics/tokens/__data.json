{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"de","advanced-topics","Advanced Topics","cyan","tokens","Token: Die Bausteine der Sprache","\u003Cp>\u003Cstrong>Token\u003C/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...\u003C/p>\n","Token sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die ...","\u003Cp>\u003Cstrong>Token\u003C/strong> sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. [1] Sie sind die &quot;Bausteine&quot;, mit denen das Modell unseren Prompt liest und versteht.\u003C/p>\n\u003Cp>Ein Token ist \u003Cstrong>nicht unbedingt ein ganzes Wort\u003C/strong>. Der Prozess der \u003Cstrong>Tokenisierung\u003C/strong> verwendet ein vordefiniertes Vokabular, um den Text in Teile zu zerlegen, die das Modell kennt. [3]\u003C/p>\n\u003Ch3>Beispiele für die Tokenisierung\u003C/h3>\n\u003Cp>Betrachten wir das Wort \u003Ccode>unbeschreiblich\u003C/code>. Ein Tokenizer könnte es in mehrere ihm bekannte Token zerlegen:\n\u003Ccode>un\u003C/code> + \u003Ccode>be\u003C/code> + \u003Ccode>schreib\u003C/code> + \u003Ccode>lich\u003C/code>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Häufige Wörter\u003C/strong> (z. B. \u003Ccode>Katze\u003C/code>, \u003Ccode>der\u003C/code>, \u003Ccode>ein\u003C/code>) sind oft ein einzelnes Token.\u003C/li>\n\u003Cli>\u003Cstrong>Komplexe oder seltene Wörter\u003C/strong> werden in Unterwörter zerlegt.\u003C/li>\n\u003Cli>\u003Cstrong>Leerzeichen und Satzzeichen\u003C/strong> werden als separate Token behandelt.\u003C/li>\n\u003C/ul>\n\u003Cp>Dieser Ansatz ermöglicht es dem Modell, ein praktisch unendliches Vokabular aus einer endlichen Anzahl von Token (normalerweise zwischen 30.000 und 50.000) zu verwalten. [1]\u003C/p>\n\u003Ch3>Die Token-Grenze und die Entwicklung der Modelle\u003C/h3>\n\u003Cp>Jeder Text-Encoder hat eine \u003Cstrong>maximale Token-Grenze\u003C/strong>, die er in einem einzigen &quot;Chunk&quot; verarbeiten kann. Diese Grenze war lange Zeit eine der Haupteinschränkungen im Prompt-Engineering.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>\u003Cstrong>Alte Architekturen (z. B. Stable Diffusion 1.5, SDXL):\u003C/strong>\nDiese Modelle verwenden Text-Encoder (CLIP) mit einer Grenze von \u003Cstrong>75 Token\u003C/strong> pro Chunk. [3] Wenn ein Prompt länger ist, wird er in mehrere Chunks aufgeteilt, aber das Verständnis des Kontexts zwischen einem Block und dem nächsten ist viel schwächer. Dies hat die Benutzer gezwungen, die wichtigsten Konzepte am Anfang des Prompts zu konzentrieren.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Neue Architekturen (z. B. FLUX.1):\u003C/strong>\nModelle der neuen Generation, wie \u003Cstrong>FLUX.1\u003C/strong>, sind darauf ausgelegt, diese Einschränkung zu überwinden. FLUX.1 verwendet einen viel leistungsfähigeren Text-Encoder (basierend auf T5-XXL), der speziell darauf trainiert wurde, lange und komplexe Prompts nativ zu verstehen. [2] Dies ermöglicht einen viel natürlicheren und detaillierteren Ausdruck, ohne sich um die künstliche Grenze von 75 Token kümmern zu müssen.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>Das Verständnis des Konzepts der Token und der Einschränkungen der verschiedenen Modelle ist grundlegend, um effektive Prompts zu schreiben und die Fähigkeiten jeder Architektur optimal zu nutzen.\u003C/p>\n",[12,15,18],{"text":13,"url":14},"Einführung in die Tokenisierung - Hugging Face","https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt",{"text":16,"url":17},"Ankündigung des FLUX.1-Modells von Black Forest Labs","https://blackforestlabs.ai/announcing-flux/",{"text":19,"url":20},"Erklärung von Token im Kontext von Stable Diffusion","https://stable-diffusion-art.com/token/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Token: Die Bausteine der Sprache. Token sind die grundlegenden Einheiten, in die ein Text zerlegt wird, bevor er von einem Sprachmodell wie CLIP verarbeitet wird. Sie sind die \"Bausteine\", mit denen das Modell unseren Prompt liest und versteht. Ein Token ist nicht unbedingt ein ganzes Wort. Der Prozess der Tokenisierung verwendet ein vordefiniertes Vokabular, um den Text in Teile zu zerlegen, die das Modell kennt. Beispiele für die Tokenisierung; Betrachten wir das Wort `unbeschreiblich`. Ein Tokenizer könnte es in mehrere ihm bekannte Token zerlegen: `un` + `be` + `schreib` + `lich` - Häufige Wörter (z. B. `Katze`, `der`, `ein`) sind oft ein einzelnes Token. - Komplexe oder seltene Wörter werden in Unterwörter zerlegt. - Leerzeichen und Satzzeichen werden als separate Token behandelt. Dieser Ansatz ermöglicht es dem Modell, ein praktisch unendliches Vokabular aus einer endlichen Anzahl von Token (normalerweise zwischen 30.000 und 50.000) zu verwalten. Die Token-Grenze und die Entwicklung der Modelle; Jeder Text-Encoder hat eine maximale Token-Grenze, die er in einem einzigen \"Chunk\" verarbeiten kann. Diese Grenze war lange Zeit eine der Haupteinschränkungen im Prompt-Engineering. - Alte Architekturen (z. B. Stable Diffusion 1.5, SDXL): Diese Modelle verwenden Text-Encoder (CLIP) mit einer Grenze von 75 Token pro Chunk. Wenn ein Prompt länger ist, wird er in mehrere Chunks aufgeteilt, aber das Verständnis des Kontexts zwischen einem Block und dem nächsten ist viel schwächer. Dies hat die Benutzer gezwungen, die wichtigsten Konzepte am Anfang des Prompts zu konzentrieren. - Neue Architekturen (z. B. FLUX.1): Modelle der neuen Generation, wie FLUX.1, sind darauf ausgelegt, diese Einschränkung zu überwinden. FLUX.1 verwendet einen viel leistungsfähigeren Text-Encoder (basierend auf T5-XXL), der speziell darauf trainiert wurde, lange und komplexe Prompts nativ zu verstehen. Dies ermöglicht einen viel natürlicheren und detaillierteren Ausdruck, ohne sich um die künstliche Grenze von 75 Token kümmern zu müssen. Das Verständnis des Konzepts der Token und der Einschränkungen der verschiedenen Modelle ist grundlegend, um effektive Prompts zu schreiben und die Fähigkeiten jeder Architektur optimal zu nutzen."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

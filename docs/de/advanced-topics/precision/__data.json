{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":24,"seo":110,"textContent":111},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"de","advanced-topics","Advanced Topics","cyan","precision","Präzision: FP32, FP16, FP8, FP4 und die Rolle der GPU","\u003Cp>Die \u003Cstrong>Präzision\u003C/strong> eines Modells bezieht sich auf das numerische Format, das zum Speichern seiner &quot;Gewichte&quot; verwendet wird. Diese Gewichte sind reelle...\u003C/p>\n","Die Präzision eines Modells bezieht sich auf das numerische Format, das zum Speichern seiner \"Gewichte\" verwendet wird. Diese Gewichte sind reelle...","\u003Cp>Die \u003Cstrong>Präzision\u003C/strong> eines Modells bezieht sich auf das numerische Format, das zum Speichern seiner &quot;Gewichte&quot; verwendet wird. Diese Gewichte sind reelle Zahlen, und Computer stellen sie mit einem System namens \u003Cstrong>Gleitkomma\u003C/strong> (daher das Akronym \u003Cstrong>FP\u003C/strong>) dar. [1]\u003C/p>\n\u003Cp>Die Zahl, die auf das Akronym folgt (z. B. FP\u003Cstrong>32\u003C/strong>, FP\u003Cstrong>16\u003C/strong>), gibt an, wie viele \u003Cstrong>Bits\u003C/strong> an Speicher verwendet werden, um eine einzelne Zahl darzustellen. Je mehr Bits verwendet werden, desto präziser ist die Zahl, aber desto mehr Platz nimmt sie ein und desto langsamer ist sie zu verarbeiten. Die Wahl der Präzision ist daher ein grundlegender Kompromiss zwischen Qualität, Geschwindigkeit und Speicherverbrauch (VRAM).\u003C/p>\n\u003Ch3>Die unzertrennliche Verbindung mit der Hardware (GPU)\u003C/h3>\n\u003Cp>Die Wahl der Präzision ist nicht nur eine Frage der Software: Die Leistung hängt entscheidend von der \u003Cstrong>nativen Hardware-Unterstützung\u003C/strong> Ihrer GPU ab. Wenn eine GPU ein Format mit geringer Präzision nicht nativ unterstützt, muss sie es per Software emulieren, was zu einer geringeren Leistung führt. [4]\u003C/p>\n\u003Cp>Moderne GPUs, insbesondere die von NVIDIA, enthalten spezielle Hardware namens \u003Cstrong>Tensor Cores\u003C/strong>, die entwickelt wurden, um Berechnungen mit reduzierter Präzision drastisch zu beschleunigen. [4]\u003C/p>\n\u003Ch3>Gängige Formate und Hardware-Unterstützung (GeForce-Beispiele)\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>FP32 (Volle Präzision - 32-Bit):\u003C/strong>\nEs ist die &quot;höchste Qualität&quot;. Jede Zahl belegt 32 Bit Speicher. Es ist der Standard, auf dem Modelle trainiert werden, aber es ist sehr aufwendig, es für die Inferenz auszuführen. [2] Ein Modell wird selten vollständig in FP32 verwendet, um Bilder zu erzeugen. Alle GPUs unterstützen es.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>FP16 (Halbe Präzision - 16-Bit):\u003C/strong>\nDer Goldstandard für die Inferenz. Es halbiert den VRAM und verdoppelt (oder mehr) die Geschwindigkeit im Vergleich zu FP32, mit einem fast unmerklichen Qualitätsverlust. [2] Die Kompatibilität ist in diesem Fall auch mit nicht ganz neuen GPUs gegeben. [4]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>BF16 (Bfloat16 - 16-Bit):\u003C/strong>\nEin alternatives 16-Bit-Format, das während des Trainings robuster ist. Wird nativ von der \u003Cstrong>Ampere (RTX 30)\u003C/strong>-Serie und neueren unterstützt.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch3>Quantisierung und erweiterte Hardware-Unterstützung\u003C/h3>\n\u003Cp>Die \u003Cstrong>Quantisierung\u003C/strong> wandelt Gewichte in noch niedrigere Formate (8 oder 4 Bit) um. Hier wird die Hardware-Unterstützung noch kritischer.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>\u003Cstrong>FP8 / INT8 (8-Bit):\u003C/strong>\n  Es stellt einen großen Fortschritt in Bezug auf die Effizienz dar. Die Hardware-Beschleunigung für FP8 ist ein Hauptmerkmal der neuesten Architekturen, wie \u003Cstrong>Ada Lovelace (RTX 40)\u003C/strong>, die native Unterstützung einführt und eine signifikante Leistungssteigerung mit diesem Format gewährleistet. [3] Ältere Karten können es ausführen, aber mit viel geringerer Effizienz.\u003C/p>\n\u003Ch3>Ein genauerer Blick auf FP8\u003C/h3>\n\u003Cp>  Der Begriff \u003Ccode>FP8\u003C/code> beschreibt eigentlich eine Familie von Formaten. Der Hauptunterschied liegt darin, wie die 8 verfügbaren Bits zwischen dem \u003Cstrong>Exponenten\u003C/strong> (der den möglichen Wertebereich bestimmt) und der \u003Cstrong>Mantisse\u003C/strong> (die die Präzision zwischen einem Wert und dem nächsten bestimmt) aufgeteilt werden. Die beiden Hauptstandards sind:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>E4M3\u003C/code>\u003C/strong>: Verwendet 4 Bits für den Exponenten und 3 für die Mantisse. Es bietet ein gutes Gleichgewicht zwischen Bereich und Präzision und wird oft zum Speichern der Modellgewichte verwendet.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>E5M2\u003C/code>\u003C/strong>: Verwendet 5 Bits für den Exponenten und 2 für die Mantisse. Es hat einen größeren Dynamikbereich, aber weniger Präzision. Es wird typischerweise für Gradienten während des Trainings verwendet.\u003C/li>\n\u003C/ul>\n\u003Cp>  \u003Cstrong>FP8 Scaled\u003C/strong> ist kein eigenständiges Format, sondern beschreibt die Technik, einen Skalierungsfaktor zu verwenden, um die Konvertierung von Gewichten in das FP8-Format zu optimieren und die Präzision im wichtigsten Wertebereich zu maximieren. Moderne GPUs wie die RTX 40-Serie verwalten diese Skalierungsfaktoren sehr effizient.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>4-Bit (z. B. NF4):\u003C/strong>\n  Eine extreme Form der Quantisierung, die Leistung hängt stark von optimierten Software-Implementierungen ab, die die allgemeinen Fähigkeiten der GPU optimal nutzen. Die Hardware-Unterstützung kam mit \u003Cstrong>Blackwell (RTX 50)\u003C/strong>.\u003C/p>\n\u003C/li>\n\u003C/ul>\n",[12,15,18,21],{"text":13,"url":14},"Erklärung von Gleitkomma-Datentypen - Wikipedia","https://en.wikipedia.org/wiki/Floating-point_arithmetic",{"text":16,"url":17},"Leitfaden zur gemischten Präzision - Hugging Face","https://huggingface.co/docs/diffusers/main/en/optimization/fp16",{"text":19,"url":20},"Einführung in 8-Bit-Formate (FP8) - NVIDIA-Blog","https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/",{"text":22,"url":23},"NVIDIA Ampere-Architektur (30er-Serie) und Tensor Cores der 3. Generation","https://www.nvidia.com/it-it/geforce/graphics-cards/30-series/ampere-architecture/",{"it":25,"en":40,"fr":55,"es":69,"de":84,"pt":99},{"category":26,"connections":27,"backToHub":28,"noPostsFound":29,"pageTitleCategory":26,"initializing":30,"backToArticles":31,"sources":32,"searchPlaceholder":33,"showMap":34,"hideMap":35,"listenToArticle":36,"playing":37,"paused":38,"voice":39},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":41,"connections":42,"backToHub":43,"noPostsFound":44,"pageTitleCategory":41,"initializing":45,"backToArticles":46,"sources":47,"searchPlaceholder":48,"showMap":49,"hideMap":50,"listenToArticle":51,"playing":52,"paused":53,"voice":54},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":56,"connections":57,"backToHub":58,"noPostsFound":59,"pageTitleCategory":56,"initializing":60,"backToArticles":61,"sources":47,"searchPlaceholder":62,"showMap":63,"hideMap":64,"listenToArticle":65,"playing":66,"paused":67,"voice":68},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":70,"connections":71,"backToHub":72,"noPostsFound":73,"pageTitleCategory":70,"initializing":74,"backToArticles":75,"sources":76,"searchPlaceholder":77,"showMap":78,"hideMap":79,"listenToArticle":80,"playing":81,"paused":82,"voice":83},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":85,"connections":86,"backToHub":87,"noPostsFound":88,"pageTitleCategory":85,"initializing":89,"backToArticles":90,"sources":91,"searchPlaceholder":92,"showMap":93,"hideMap":94,"listenToArticle":95,"playing":96,"paused":97,"voice":98},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":26,"connections":100,"backToHub":101,"noPostsFound":102,"pageTitleCategory":26,"initializing":74,"backToArticles":103,"sources":104,"searchPlaceholder":105,"showMap":106,"hideMap":79,"listenToArticle":107,"playing":108,"paused":109,"voice":83},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Präzision: FP32, FP16, FP8, FP4 und die Rolle der GPU. Die Präzision eines Modells bezieht sich auf das numerische Format, das zum Speichern seiner \"Gewichte\" verwendet wird. Diese Gewichte sind reelle Zahlen, und Computer stellen sie mit einem System namens Gleitkomma (daher das Akronym FP) dar. Die Zahl, die auf das Akronym folgt (z. B. FP32, FP16), gibt an, wie viele Bits an Speicher verwendet werden, um eine einzelne Zahl darzustellen. Je mehr Bits verwendet werden, desto präziser ist die Zahl, aber desto mehr Platz nimmt sie ein und desto langsamer ist sie zu verarbeiten. Die Wahl der Präzision ist daher ein grundlegender Kompromiss zwischen Qualität, Geschwindigkeit und Speicherverbrauch (VRAM). Die unzertrennliche Verbindung mit der Hardware (GPU); Die Wahl der Präzision ist nicht nur eine Frage der Software: Die Leistung hängt entscheidend von der nativen Hardware-Unterstützung Ihrer GPU ab. Wenn eine GPU ein Format mit geringer Präzision nicht nativ unterstützt, muss sie es per Software emulieren, was zu einer geringeren Leistung führt. Moderne GPUs, insbesondere die von NVIDIA, enthalten spezielle Hardware namens Tensor Cores, die entwickelt wurden, um Berechnungen mit reduzierter Präzision drastisch zu beschleunigen. Gängige Formate und Hardware-Unterstützung (GeForce-Beispiele); 1. FP32 (Volle Präzision - 32-Bit): Es ist die \"höchste Qualität\". Jede Zahl belegt 32 Bit Speicher. Es ist der Standard, auf dem Modelle trainiert werden, aber es ist sehr aufwendig, es für die Inferenz auszuführen. Ein Modell wird selten vollständig in FP32 verwendet, um Bilder zu erzeugen. Alle GPUs unterstützen es. 2. FP16 (Halbe Präzision - 16-Bit): Der Goldstandard für die Inferenz. Es halbiert den VRAM und verdoppelt (oder mehr) die Geschwindigkeit im Vergleich zu FP32, mit einem fast unmerklichen Qualitätsverlust. Die Kompatibilität ist in diesem Fall auch mit nicht ganz neuen GPUs gegeben. 3. BF16 (Bfloat16 - 16-Bit): Ein alternatives 16-Bit-Format, das während des Trainings robuster ist. Wird nativ von der Ampere (RTX 30)-Serie und neueren unterstützt. Quantisierung und erweiterte Hardware-Unterstützung; Die Quantisierung wandelt Gewichte in noch niedrigere Formate (8 oder 4 Bit) um. Hier wird die Hardware-Unterstützung noch kritischer. - FP8 / INT8 (8-Bit): Es stellt einen großen Fortschritt in Bezug auf die Effizienz dar. Die Hardware-Beschleunigung für FP8 ist ein Hauptmerkmal der neuesten Architekturen, wie Ada Lovelace (RTX 40), die native Unterstützung einführt und eine signifikante Leistungssteigerung mit diesem Format gewährleistet. Ältere Karten können es ausführen, aber mit viel geringerer Effizienz. ### Ein genauerer Blick auf FP8 Der Begriff `FP8` beschreibt eigentlich eine Familie von Formaten. Der Hauptunterschied liegt darin, wie die 8 verfügbaren Bits zwischen dem Exponenten (der den möglichen Wertebereich bestimmt) und der Mantisse (die die Präzision zwischen einem Wert und dem nächsten bestimmt) aufgeteilt werden. Die beiden Hauptstandards sind: - `E4M3`: Verwendet 4 Bits für den Exponenten und 3 für die Mantisse. Es bietet ein gutes Gleichgewicht zwischen Bereich und Präzision und wird oft zum Speichern der Modellgewichte verwendet. - `E5M2`: Verwendet 5 Bits für den Exponenten und 2 für die Mantisse. Es hat einen größeren Dynamikbereich, aber weniger Präzision. Es wird typischerweise für Gradienten während des Trainings verwendet. FP8 Scaled ist kein eigenständiges Format, sondern beschreibt die Technik, einen Skalierungsfaktor zu verwenden, um die Konvertierung von Gewichten in das FP8-Format zu optimieren und die Präzision im wichtigsten Wertebereich zu maximieren. Moderne GPUs wie die RTX 40-Serie verwalten diese Skalierungsfaktoren sehr effizient. - 4-Bit (z. B. NF4): Eine extreme Form der Quantisierung, die Leistung hängt stark von optimierten Software-Implementierungen ab, die die allgemeinen Fähigkeiten der GPU optimal nutzen. Die Hardware-Unterstützung kam mit Blackwell (RTX 50)."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

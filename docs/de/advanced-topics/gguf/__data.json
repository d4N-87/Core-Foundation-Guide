{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"de","advanced-topics","Advanced Topics","cyan","gguf","GGUF: Quantisierung für CPU und GPU","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> ist ein Dateiformat, das entwickelt wurde, um \u003Cstrong>quantisierte\u003C/strong> neuronale Modelle zu enthalten, d.h. in For...\u003C/p>\n","GGUF (Georgi Gerganov Universal Format) ist ein Dateiformat, das entwickelt wurde, um quantisierte neuronale Modelle zu enthalten, d.h. in For...","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> ist ein Dateiformat, das entwickelt wurde, um \u003Cstrong>quantisierte\u003C/strong> neuronale Modelle zu enthalten, d.h. in Formate mit sehr geringer Präzision (wie 4 oder 8 Bit) konvertiert, um ihre Größe und ihren Speicherverbrauch drastisch zu reduzieren. [1]\u003C/p>\n\u003Cp>Entstanden aus dem Projekt \u003Cstrong>\u003Ccode>llama.cpp\u003C/code>\u003C/strong> zur Ausführung von Großen Sprachmodellen (LLMs) auf CPUs, hat sich seine Verwendung kürzlich auch auf das Ökosystem der Bilddiffusionsmodelle ausgeweitet. [2]\u003C/p>\n\u003Ch3>Der Hauptzweck: Reduzierung des Speicherverbrauchs\u003C/h3>\n\u003Cp>Der Hauptvorteil von GGUF ist die \u003Cstrong>Quantisierung\u003C/strong>. Ein Modell, das im FP16-Format (\u003Ccode>.safetensors\u003C/code>) 14 GB VRAM belegen würde, kann im GGUF-Format, das auf 4 Bit quantisiert ist (\u003Ccode>q4_K_M\u003C/code>), weniger als 5 GB belegen. Dies ermöglicht:\u003C/p>\n\u003Cul>\n\u003Cli>Das Ausführen riesiger Modelle auf GPUs mit weniger VRAM.\u003C/li>\n\u003Cli>Das gleichzeitige Laden mehrerer Komponenten in den Speicher.\u003C/li>\n\u003Cli>Das effiziente Ausführen von Modellen auf CPUs.\u003C/li>\n\u003C/ul>\n\u003Ch3>GGUF in der Welt der LLMs (klassische Verwendung)\u003C/h3>\n\u003Cp>Die primäre Verwendung von GGUF ist für Sprachmodelle. Schnittstellen wie LM Studio oder Ollama verwenden GGUF-Dateien, um leistungsstarke Chatbots (wie Llama, Mistral) auf Consumer-Hardware auszuführen, wobei hauptsächlich die CPU genutzt wird. [3]\u003C/p>\n\u003Ch3>GGUF in der Welt der Diffusion (moderne Verwendung in ComfyUI)\u003C/h3>\n\u003Cp>In jüngerer Zeit hat die Community begonnen, die Vorteile der GGUF-Quantisierung auch auf Verarbeitungskomponenten anzuwenden. In ComfyUI ist es über spezielle Knoten (\u003Ccode>Load GGUF Model\u003C/code>) möglich, GGUF-Versionen von Folgendem zu laden:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Text Encoder (CLIP):\u003C/strong> Das Laden eines quantisierten CLIP reduziert seinen Einfluss auf den VRAM erheblich und gibt wertvolle Ressourcen für das UNet-Modell frei. Dies ist die häufigste und effektivste Verwendung.\u003C/li>\n\u003Cli>\u003Cstrong>UNet:\u003C/strong> Es gibt auch Experimente, um das gesamte UNet im GGUF-Format zu quantisieren. Obwohl dies die maximale Speichereinsparung bietet, kann es zu einem deutlicheren Qualitätsverlust im endgültigen Bild führen, verglichen mit der Verwendung eines UNet im FP16-Format.\u003C/li>\n\u003C/ul>\n\u003Cp>Es ist ein vielseitiges Werkzeug für die \u003Cstrong>fortgeschrittene Speicherverwaltung\u003C/strong>, das es Benutzern ermöglicht, immer komplexere Workflows auf Consumer-Hardware auszuführen und dabei den Kompromiss zwischen VRAM-Verbrauch und Ausgabequalität geschickt auszubalancieren.\u003C/p>\n\u003Ch3>Entschlüsselung der Quantisierungsnomenklaturen (z. B. \u003Ccode>Q4_K_M\u003C/code>)\u003C/h3>\n\u003Cp>Beim Herunterladen eines GGUF-Modells enthält der Dateiname oft ein Akronym, das die verwendete Quantisierungsmethode beschreibt. Das Verständnis hilft, das richtige Gleichgewicht zwischen Größe und Qualität zu wählen. So wird es gelesen:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q\u003C/code> gefolgt von einer Zahl (z. B. \u003Ccode>Q4\u003C/code>, \u003Ccode>Q5\u003C/code>, \u003Ccode>Q8\u003C/code>):\u003C/strong> Gibt die Anzahl der \u003Cstrong>Bits\u003C/strong> an, die für jedes Gewicht verwendet werden. \u003Ccode>Q8\u003C/code> verwendet 8 Bits (höhere Qualität, größere Datei), \u003Ccode>Q4\u003C/code> verwendet 4 Bits (niedrigere Qualität, kleinere Datei).\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_K\u003C/code>:\u003C/strong> Gibt eine &quot;K-Quant&quot;-Variante an. Es ist eine verbesserte Quantisierungstechnik, die versucht, die Qualität der Informationen besser zu erhalten, insbesondere bei den wichtigsten Gewichten. \u003Ccode>_K\u003C/code>-Modelle sind oft die empfohlene Wahl.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_0\u003C/code> oder \u003Ccode>_1\u003C/code> (z. B. \u003Ccode>Q4_0\u003C/code>, \u003Ccode>Q5_1\u003C/code>):\u003C/strong> Geben verschiedene Versionen derselben Methode an. \u003Ccode>_0\u003C/code> ist die &quot;reine&quot; 4-Bit-Version, während \u003Ccode>_1\u003C/code> eine gemischte Version ist, die eine etwas höhere Präzision (5-Bit) für einige Gewichte verwendet und eine kleine Qualitätsverbesserung bei einer etwas größeren Datei bietet.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_S\u003C/code>, \u003Ccode>_M\u003C/code>, \u003Ccode>_L\u003C/code> (z. B. \u003Ccode>Q4_K_S\u003C/code>):\u003C/strong> Geben die Modellgrößen an (&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;). Sie beziehen sich nicht auf die Quantisierung selbst, sondern auf verschiedene &quot;Größen&quot; des ursprünglichen Modells.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Praktische Beispiele:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q8_0\u003C/code>:\u003C/strong> 8-Bit-Quantisierung. Höchste Qualität unter den GGUF-Versionen, aber auch die schwerste.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q5_K_M\u003C/code>:\u003C/strong> &quot;K-Quant&quot; 5-Bit-Quantisierung, &quot;Medium&quot;-Version. Ein ausgezeichneter Kompromiss zwischen Qualität und Größe.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q4_0\u003C/code>:\u003C/strong> &quot;Reine&quot; 4-Bit-Quantisierung. Die kleinste und leichteste Version, aber mit dem größten Qualitätsverlust. Wird oft verwendet, um riesige Modelle auf sehr begrenzter Hardware auszuführen.\u003C/li>\n\u003C/ul>\n",[12,15,18],{"text":13,"url":14},"Offizielle GGUF-Ankündigung im Hugging Face-Blog","https://huggingface.co/blog/gguf",{"text":16,"url":17},"GitHub-Repository von llama.cpp","https://github.com/ggerganov/llama.cpp",{"text":19,"url":20},"Beispiel für einen Workflow in ComfyUI mit GGUF Loader","https://comfyanonymous.github.io/ComfyUI_examples/llm/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"GGUF: Quantisierung für CPU und GPU. GGUF (Georgi Gerganov Universal Format) ist ein Dateiformat, das entwickelt wurde, um quantisierte neuronale Modelle zu enthalten, d.h. in Formate mit sehr geringer Präzision (wie 4 oder 8 Bit) konvertiert, um ihre Größe und ihren Speicherverbrauch drastisch zu reduzieren. Entstanden aus dem Projekt `llama.cpp` zur Ausführung von Großen Sprachmodellen (LLMs) auf CPUs, hat sich seine Verwendung kürzlich auch auf das Ökosystem der Bilddiffusionsmodelle ausgeweitet. Der Hauptzweck: Reduzierung des Speicherverbrauchs; Der Hauptvorteil von GGUF ist die Quantisierung. Ein Modell, das im FP16-Format (`.safetensors`) 14 GB VRAM belegen würde, kann im GGUF-Format, das auf 4 Bit quantisiert ist (`q4KM`), weniger als 5 GB belegen. Dies ermöglicht: - Das Ausführen riesiger Modelle auf GPUs mit weniger VRAM. - Das gleichzeitige Laden mehrerer Komponenten in den Speicher. - Das effiziente Ausführen von Modellen auf CPUs. GGUF in der Welt der LLMs (klassische Verwendung); Die primäre Verwendung von GGUF ist für Sprachmodelle. Schnittstellen wie LM Studio oder Ollama verwenden GGUF-Dateien, um leistungsstarke Chatbots (wie Llama, Mistral) auf Consumer-Hardware auszuführen, wobei hauptsächlich die CPU genutzt wird. GGUF in der Welt der Diffusion (moderne Verwendung in ComfyUI); In jüngerer Zeit hat die Community begonnen, die Vorteile der GGUF-Quantisierung auch auf Verarbeitungskomponenten anzuwenden. In ComfyUI ist es über spezielle Knoten (`Load GGUF Model`) möglich, GGUF-Versionen von Folgendem zu laden: - Text Encoder (CLIP): Das Laden eines quantisierten CLIP reduziert seinen Einfluss auf den VRAM erheblich und gibt wertvolle Ressourcen für das UNet-Modell frei. Dies ist die häufigste und effektivste Verwendung. - UNet: Es gibt auch Experimente, um das gesamte UNet im GGUF-Format zu quantisieren. Obwohl dies die maximale Speichereinsparung bietet, kann es zu einem deutlicheren Qualitätsverlust im endgültigen Bild führen, verglichen mit der Verwendung eines UNet im FP16-Format. Es ist ein vielseitiges Werkzeug für die fortgeschrittene Speicherverwaltung, das es Benutzern ermöglicht, immer komplexere Workflows auf Consumer-Hardware auszuführen und dabei den Kompromiss zwischen VRAM-Verbrauch und Ausgabequalität geschickt auszubalancieren. Entschlüsselung der Quantisierungsnomenklaturen (z. B. `Q4KM`); Beim Herunterladen eines GGUF-Modells enthält der Dateiname oft ein Akronym, das die verwendete Quantisierungsmethode beschreibt. Das Verständnis hilft, das richtige Gleichgewicht zwischen Größe und Qualität zu wählen. So wird es gelesen: - `Q` gefolgt von einer Zahl (z. B. `Q4`, `Q5`, `Q8`): Gibt die Anzahl der Bits an, die für jedes Gewicht verwendet werden. `Q8` verwendet 8 Bits (höhere Qualität, größere Datei), `Q4` verwendet 4 Bits (niedrigere Qualität, kleinere Datei). - `K`: Gibt eine \"K-Quant\"-Variante an. Es ist eine verbesserte Quantisierungstechnik, die versucht, die Qualität der Informationen besser zu erhalten, insbesondere bei den wichtigsten Gewichten. `K`-Modelle sind oft die empfohlene Wahl. - `0` oder `1` (z. B. `Q40`, `Q51`): Geben verschiedene Versionen derselben Methode an. `0` ist die \"reine\" 4-Bit-Version, während `1` eine gemischte Version ist, die eine etwas höhere Präzision (5-Bit) für einige Gewichte verwendet und eine kleine Qualitätsverbesserung bei einer etwas größeren Datei bietet. - `S`, `M`, `L` (z. B. `Q4KS`): Geben die Modellgrößen an (\"Small\", \"Medium\", \"Large\"). Sie beziehen sich nicht auf die Quantisierung selbst, sondern auf verschiedene \"Größen\" des ursprünglichen Modells. Praktische Beispiele: - `Q80`: 8-Bit-Quantisierung. Höchste Qualität unter den GGUF-Versionen, aber auch die schwerste. - `Q5KM`: \"K-Quant\" 5-Bit-Quantisierung, \"Medium\"-Version. Ein ausgezeichneter Kompromiss zwischen Qualität und Größe. - `Q40`: \"Reine\" 4-Bit-Quantisierung. Die kleinste und leichteste Version, aber mit dem größten Qualitätsverlust. Wird oft verwendet, um riesige Modelle auf sehr begrenzter Hardware auszuführen."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

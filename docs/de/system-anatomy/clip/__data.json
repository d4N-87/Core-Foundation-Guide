{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"de","system-anatomy","System Anatomy","teal","clip","CLIP Text Encoder: Der Prompt-Übersetzer","\u003Cp>\u003Cstrong>CLIP (Contrastive Language-Image Pre-training)\u003C/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...\u003C/p>\n","CLIP (Contrastive Language-Image Pre-training) ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...","\u003Cp>\u003Cstrong>CLIP (Contrastive Language-Image Pre-training)\u003C/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwischen Text und Bildern &quot;verstehen&quot;, revolutioniert hat. [1] Innerhalb eines Diffusionsmodells ist seine Rolle die eines \u003Cstrong>Universalübersetzers\u003C/strong>: Es nimmt Ihren Prompt in menschlicher Sprache und wandelt ihn in eine mathematische Darstellung (genannt \u003Cem>Embedding\u003C/em>) um, die das UNet als Leitfaden verwenden kann. [3]\u003C/p>\n\u003Cp>Die Begriffe &quot;Text Encoder&quot; und &quot;CLIP&quot; werden oft synonym verwendet. Ersterer beschreibt die \u003Cem>Funktion\u003C/em>, letzterer den \u003Cem>Namen\u003C/em> der Komponente, die diese Funktion ausführt.\u003C/p>\n\u003Cp>Ohne CLIP wäre der Prompt &quot;ein Astronaut auf einem Pferd&quot; für das UNet nur eine bedeutungslose Zeichenkette. Dank CLIP wird dieser Satz in eine Reihe von Zahlen umgewandelt, die die Konzepte &quot;Astronaut&quot;, &quot;Pferd&quot; und ihre räumliche Beziehung mathematisch &quot;beschreiben&quot;.\u003C/p>\n\u003Ch3>Wie funktioniert der &quot;Übersetzungs&quot;-Prozess?\u003C/h3>\n\u003Cp>Der \u003Ccode>CLIP Text Encode\u003C/code>-Knoten in ComfyUI führt einen mehrstufigen Prozess durch:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Tokenisierung:\u003C/strong>\nZuerst wird der Prompt in kleinere Teile namens \u003Cstrong>Token\u003C/strong> zerlegt. [2] Ein Token entspricht nicht unbedingt einem Wort. Komplexe Wörter können in mehrere Token aufgeteilt werden, während gebräuchliche Wörter ein einzelnes Token sein können. Jedes CLIP-Modell hat eine maximale Anzahl von Token, die es auf einmal verarbeiten kann (normalerweise 75). Wenn Ihr Prompt länger ist, wird er in mehrere &quot;Chunks&quot; (Teile) aufgeteilt.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Embedding:\u003C/strong>\nJedes Token wird in einen numerischen Vektor umgewandelt. An diesem Punkt haben wir eine Sequenz von Zahlen, die unseren Prompt darstellt.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Verarbeitung (Attention):\u003C/strong>\nDiese Sequenz von Zahlen wird von einer Transformer-Architektur verarbeitet. [3] Hier geschieht die eigentliche Magie: Der \u003Cstrong>Attention\u003C/strong>-Mechanismus ermöglicht es dem Modell, die Beziehungen zwischen den Wörtern zu verstehen. Es sieht nicht nur &quot;rot&quot; und &quot;Würfel&quot;, sondern es versteht, dass es der &quot;Würfel&quot; ist, der &quot;rot&quot; sein soll. Hier wirkt sich das Gewicht aus, das wir den Wörtern im Prompt geben (z. B. \u003Ccode>(Wort:1.2)\u003C/code>), indem es dem Aufmerksamkeitsmechanismus sagt, dass er bestimmten Konzepten &quot;mehr Aufmerksamkeit&quot; schenken soll.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>Das Endergebnis dieses Prozesses ist das \u003Cstrong>Conditioning\u003C/strong>, eine Ausgabe, die die Embeddings des Prompts enthält, die bereit sind, in das UNet &quot;injiziert&quot; zu werden und die Bilderzeugung zu leiten.\u003C/p>\n\u003Ch3>Verschiedene Modelle, verschiedene CLIPs\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Stable Diffusion 1.5\u003C/strong> verwendet ein einzelnes CLIP-Modell (OpenCLIP).\u003C/li>\n\u003Cli>\u003Cstrong>Stable Diffusion XL (SDXL)\u003C/strong> verwendet eine Kombination aus zwei verschiedenen CLIP-Modellen (OpenCLIP und CLIP ViT-L), was ihm ermöglicht, Prompts auf eine viel reichhaltigere und nuanciertere Weise zu verstehen. Dies ist einer der Hauptgründe für seine überlegene Qualität.\u003C/li>\n\u003C/ul>\n\u003Cp>Dies bedeutet, dass je mehr CLIPs zusammenarbeiten, mit vielen Milliarden von Parametern, desto mehr wird das Ergebnis dem Prompt entsprechen.\u003C/p>\n",[12,15,18],{"text":13,"url":14},"OpenAIs Original-Paper zu CLIP","https://arxiv.org/abs/2103.00020",{"text":16,"url":17},"Erklärung von CLIP im Hugging Face-Blog","https://huggingface.co/docs/transformers/main/en/model_doc/clip",{"text":19,"url":20},"Illustrierter Artikel über die Funktionsweise von Stable Diffusion","https://jalammar.github.io/illustrated-stable-diffusion/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"CLIP Text Encoder: Der Prompt-Übersetzer. CLIP (Contrastive Language-Image Pre-training) ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwischen Text und Bildern \"verstehen\", revolutioniert hat. Innerhalb eines Diffusionsmodells ist seine Rolle die eines Universalübersetzers: Es nimmt Ihren Prompt in menschlicher Sprache und wandelt ihn in eine mathematische Darstellung (genannt Embedding) um, die das UNet als Leitfaden verwenden kann. Die Begriffe \"Text Encoder\" und \"CLIP\" werden oft synonym verwendet. Ersterer beschreibt die Funktion, letzterer den Namen der Komponente, die diese Funktion ausführt. Ohne CLIP wäre der Prompt \"ein Astronaut auf einem Pferd\" für das UNet nur eine bedeutungslose Zeichenkette. Dank CLIP wird dieser Satz in eine Reihe von Zahlen umgewandelt, die die Konzepte \"Astronaut\", \"Pferd\" und ihre räumliche Beziehung mathematisch \"beschreiben\". Wie funktioniert der \"Übersetzungs\"-Prozess?; Der `CLIP Text Encode`-Knoten in ComfyUI führt einen mehrstufigen Prozess durch: 1. Tokenisierung: Zuerst wird der Prompt in kleinere Teile namens Token zerlegt. Ein Token entspricht nicht unbedingt einem Wort. Komplexe Wörter können in mehrere Token aufgeteilt werden, während gebräuchliche Wörter ein einzelnes Token sein können. Jedes CLIP-Modell hat eine maximale Anzahl von Token, die es auf einmal verarbeiten kann (normalerweise 75). Wenn Ihr Prompt länger ist, wird er in mehrere \"Chunks\" (Teile) aufgeteilt. 2. Embedding: Jedes Token wird in einen numerischen Vektor umgewandelt. An diesem Punkt haben wir eine Sequenz von Zahlen, die unseren Prompt darstellt. 3. Verarbeitung (Attention): Diese Sequenz von Zahlen wird von einer Transformer-Architektur verarbeitet. Hier geschieht die eigentliche Magie: Der Attention-Mechanismus ermöglicht es dem Modell, die Beziehungen zwischen den Wörtern zu verstehen. Es sieht nicht nur \"rot\" und \"Würfel\", sondern es versteht, dass es der \"Würfel\" ist, der \"rot\" sein soll. Hier wirkt sich das Gewicht aus, das wir den Wörtern im Prompt geben (z. B. `(Wort:1.2)`), indem es dem Aufmerksamkeitsmechanismus sagt, dass er bestimmten Konzepten \"mehr Aufmerksamkeit\" schenken soll. Das Endergebnis dieses Prozesses ist das Conditioning, eine Ausgabe, die die Embeddings des Prompts enthält, die bereit sind, in das UNet \"injiziert\" zu werden und die Bilderzeugung zu leiten. Verschiedene Modelle, verschiedene CLIPs; - Stable Diffusion 1.5 verwendet ein einzelnes CLIP-Modell (OpenCLIP). - Stable Diffusion XL (SDXL) verwendet eine Kombination aus zwei verschiedenen CLIP-Modellen (OpenCLIP und CLIP ViT-L), was ihm ermöglicht, Prompts auf eine viel reichhaltigere und nuanciertere Weise zu verstehen. Dies ist einer der Hauptgründe für seine überlegene Qualität. Dies bedeutet, dass je mehr CLIPs zusammenarbeiten, mit vielen Milliarden von Parametern, desto mehr wird das Ergebnis dem Prompt entsprechen."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

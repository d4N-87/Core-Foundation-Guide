<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		
		<!-- 
			English: A comprehensive set of favicon links for different platforms (Apple, standard browsers) and the web manifest for PWA capabilities.
			Italiano: Un set completo di link per le favicon per diverse piattaforme (Apple, browser standard) e il web manifest per le funzionalità PWA.
		-->
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
		
		<!-- 
			English: Sets the viewport to ensure the site is responsive and scales correctly on all devices.
			Italiano: Imposta il viewport per assicurare che il sito sia responsivo e si adatti correttamente a tutti i dispositivi.
		-->
		<meta name="viewport" content="width=device-width, initial-scale=1" />

		<!-- 
			English: SvelteKit placeholder. This is where SvelteKit injects all necessary head content, like CSS links and meta tags from `svelte:head`.
			Italiano: Segnaposto di SvelteKit. Qui è dove SvelteKit inietta tutto il contenuto necessario per l'head, come i link CSS e i meta tag da `svelte:head`.
		-->
		
		<link href="../../_app/immutable/assets/0.CHJcp1PB.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.CQW7PVBU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DdPt9bVq.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B-icdhcA.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dv9Va7Iy.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.BRxldS8A.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/PPVm8Dsz.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DsnmJJEf.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BGc_5KK-.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C7sTqTmI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DUX4vovL.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BU9Ixn5H.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.AQxqvBRY.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ctJiNxf1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D2fIjOlU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BfGA2QYN.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Cmlm8h2R.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D5MSuBSp.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C8S6TabI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D40XMBwt.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.CJIju4Kh.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/6.B0mU0U6k.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/AGRw3LU9.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BoPAQx1w.js"><!--[--><meta name="description" content="&lt;p>&lt;strong>CLIP (Contrastive Language-Image Pre-training)&lt;/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...&lt;/p>
"/>  <meta property="og:type" content="website"/> <meta property="og:url" content="http://sveltekit-prerender/de/system-anatomy/clip"/> <meta property="og:title" content="CLIP Text Encoder: Der Prompt-Übersetzer | Core Foundation Guide"/> <meta property="og:description" content="&lt;p>&lt;strong>CLIP (Contrastive Language-Image Pre-training)&lt;/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...&lt;/p>
"/>  <meta property="twitter:card" content="summary_large_image"/> <meta property="twitter:url" content="http://sveltekit-prerender/de/system-anatomy/clip"/> <meta property="twitter:title" content="CLIP Text Encoder: Der Prompt-Übersetzer | Core Foundation Guide"/> <meta property="twitter:description" content="&lt;p>&lt;strong>CLIP (Contrastive Language-Image Pre-training)&lt;/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...&lt;/p>
"/><!--]--><title>CLIP Text Encoder: Der Prompt-Übersetzer | Core Foundation Guide</title>
	</head>
	<!-- 
		English: SvelteKit attribute to enable data preloading on mouse hover over links, making navigation feel faster.
		Italiano: Attributo di SvelteKit per abilitare il precaricamento dei dati al passaggio del mouse sui link, rendendo la navigazione più veloce.
	-->
	<body data-sveltekit-preload-data="hover">
		<!-- 
			English: SvelteKit placeholder for the main application body. The `display: contents` wrapper makes the div itself layout-neutral.
			Italiano: Segnaposto di SvelteKit per il corpo principale dell'applicazione. Il wrapper `display: contents` rende il div stesso neutro a livello di layout.
		-->
		<div style="display: contents"><!--[--><!--[--><!----><div class="fixed top-0 left-0 w-full h-full -z-10"></div><!----> <div class="relative z-10 isolate"><header class="fixed left-0 right-0 top-0 z-[60] w-full border-b border-cyan-900/50 bg-black/30 px-4 py-3 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl items-center justify-between"><div class="flex flex-shrink-0 items-center"><a href="#" target="_blank" rel="noopener noreferrer" aria-label="Core Foundation Guide GitHub Repository" class="transition-all duration-300 hover:scale-110 hover:drop-shadow-[0_0_8px_theme(colors.amber.400)] focus:scale-110 focus:outline-none"><img src="/logo.webp" alt="Logo" class="h-10 w-10 md:h-12 md:w-12"/></a> <div class="ml-3 min-w-0 text-xl font-bold tracking-wide text-slate-200 sm:ml-4 sm:text-2xl md:ml-6 md:text-3xl lg:text-4xl"><!--[--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">C</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">R</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">F</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">A</span><!--]--><!--[!--><span class="char-span inline-block ">T</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">G</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--]--></div></div> <svg class="hidden"><symbol id="icon-globe" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></symbol></svg> <div class="relative"><button aria-label="Change language" class="text-slate-400 transition-colors hover:text-white"><svg class="w-6 h-6"><use href="#icon-globe"></use></svg></button> <!--[!--><!--]--></div><!----></div></header><!----> <div class="flex min-h-screen flex-col pt-20"><main class="flex-grow"><!--[--><!--[--><!----><!--[--><!----><!----> <div class="flex w-full flex-grow items-center justify-center p-4 md:p-8"><div class="w-full max-w-4xl rounded-xl bg-gradient-to-br from-cyan-950/20 to-slate-950/10 backdrop-blur-lg border-2 border-cyan-500/30 shadow-2xl shadow-cyan-900/50 p-6 md:p-10" style="visibility: hidden;"><!--[--><button class="mb-8 block font-semibold text-cyan-400 transition-colors hover:text-amber-400">← Zurück zu den Artikeln</button> <article class="prose prose-invert prose-strong:text-amber-400 prose-hr:border-cyan-500/30 prose-ol:text-gray-400 lg:prose-xl"><h1>CLIP Text Encoder: Der Prompt-Übersetzer</h1> <!--[--><div class="not-prose my-8"><svg class="hidden"><symbol id="icon-play" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></symbol><symbol id="icon-stop" viewBox="0 0 24 24" fill="currentColor"><path d="M6 6h12v12H6z"></path></symbol><symbol id="icon-settings" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 0 2l-.15.08a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l-.22-.38a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1 0-2l.15-.08a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"></path><circle cx="12" cy="12" r="3"></circle></symbol></svg> <!--[!--><!--]--><!----></div><!--]-->  <!----><p><strong>CLIP (Contrastive Language-Image Pre-training)</strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwischen Text und Bildern &quot;verstehen&quot;, revolutioniert hat. [1] Innerhalb eines Diffusionsmodells ist seine Rolle die eines <strong>Universalübersetzers</strong>: Es nimmt Ihren Prompt in menschlicher Sprache und wandelt ihn in eine mathematische Darstellung (genannt <em>Embedding</em>) um, die das UNet als Leitfaden verwenden kann. [3]</p>
<p>Die Begriffe &quot;Text Encoder&quot; und &quot;CLIP&quot; werden oft synonym verwendet. Ersterer beschreibt die <em>Funktion</em>, letzterer den <em>Namen</em> der Komponente, die diese Funktion ausführt.</p>
<p>Ohne CLIP wäre der Prompt &quot;ein Astronaut auf einem Pferd&quot; für das UNet nur eine bedeutungslose Zeichenkette. Dank CLIP wird dieser Satz in eine Reihe von Zahlen umgewandelt, die die Konzepte &quot;Astronaut&quot;, &quot;Pferd&quot; und ihre räumliche Beziehung mathematisch &quot;beschreiben&quot;.</p>
<h3>Wie funktioniert der &quot;Übersetzungs&quot;-Prozess?</h3>
<p>Der <code>CLIP Text Encode</code>-Knoten in ComfyUI führt einen mehrstufigen Prozess durch:</p>
<ol>
<li><p><strong>Tokenisierung:</strong>
Zuerst wird der Prompt in kleinere Teile namens <strong>Token</strong> zerlegt. [2] Ein Token entspricht nicht unbedingt einem Wort. Komplexe Wörter können in mehrere Token aufgeteilt werden, während gebräuchliche Wörter ein einzelnes Token sein können. Jedes CLIP-Modell hat eine maximale Anzahl von Token, die es auf einmal verarbeiten kann (normalerweise 75). Wenn Ihr Prompt länger ist, wird er in mehrere &quot;Chunks&quot; (Teile) aufgeteilt.</p>
</li>
<li><p><strong>Embedding:</strong>
Jedes Token wird in einen numerischen Vektor umgewandelt. An diesem Punkt haben wir eine Sequenz von Zahlen, die unseren Prompt darstellt.</p>
</li>
<li><p><strong>Verarbeitung (Attention):</strong>
Diese Sequenz von Zahlen wird von einer Transformer-Architektur verarbeitet. [3] Hier geschieht die eigentliche Magie: Der <strong>Attention</strong>-Mechanismus ermöglicht es dem Modell, die Beziehungen zwischen den Wörtern zu verstehen. Es sieht nicht nur &quot;rot&quot; und &quot;Würfel&quot;, sondern es versteht, dass es der &quot;Würfel&quot; ist, der &quot;rot&quot; sein soll. Hier wirkt sich das Gewicht aus, das wir den Wörtern im Prompt geben (z. B. <code>(Wort:1.2)</code>), indem es dem Aufmerksamkeitsmechanismus sagt, dass er bestimmten Konzepten &quot;mehr Aufmerksamkeit&quot; schenken soll.</p>
</li>
</ol>
<p>Das Endergebnis dieses Prozesses ist das <strong>Conditioning</strong>, eine Ausgabe, die die Embeddings des Prompts enthält, die bereit sind, in das UNet &quot;injiziert&quot; zu werden und die Bilderzeugung zu leiten.</p>
<h3>Verschiedene Modelle, verschiedene CLIPs</h3>
<ul>
<li><strong>Stable Diffusion 1.5</strong> verwendet ein einzelnes CLIP-Modell (OpenCLIP).</li>
<li><strong>Stable Diffusion XL (SDXL)</strong> verwendet eine Kombination aus zwei verschiedenen CLIP-Modellen (OpenCLIP und CLIP ViT-L), was ihm ermöglicht, Prompts auf eine viel reichhaltigere und nuanciertere Weise zu verstehen. Dies ist einer der Hauptgründe für seine überlegene Qualität.</li>
</ul>
<p>Dies bedeutet, dass je mehr CLIPs zusammenarbeiten, mit vielen Milliarden von Parametern, desto mehr wird das Ergebnis dem Prompt entsprechen.</p>
<!---->  <!--[--><hr/> <h2>Quellen</h2> <ol><!--[--><li><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">OpenAIs Original-Paper zu CLIP</a></li><li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/clip" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Erklärung von CLIP im Hugging Face-Blog</a></li><li><a href="https://jalammar.github.io/illustrated-stable-diffusion/" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Illustrierter Artikel über die Funktionsweise von Stable Diffusion</a></li><!--]--></ol><!--]--></article><!--]--></div></div><!----><!--]--><!----><!--]--><!--]--></main> <svg class="hidden"><symbol id="icon-github" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.91 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></symbol></svg> <footer class="w-full border-t border-cyan-900/50 bg-black/30 px-4 py-6 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl flex-col items-center justify-between gap-4 sm:flex-row"><p class="text-sm text-slate-400">© 2025 Core Foundation Guide. An interactive field manual for AI concepts.</p>  <a href="#" target="_blank" rel="noopener noreferrer" aria-label="GitHub Repository" class="text-slate-400 transition-colors hover:text-white"><svg class="h-6 w-6"><use href="#icon-github"></use></svg></a></div></footer><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1erws2 = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.CQW7PVBU.js"),
						import("../../_app/immutable/entry/app.BRxldS8A.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 6],
							data: [{type:"data",data:{translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}}},uses:{}},null,{type:"data",data:{post:{lang:"de",categorySlug:"system-anatomy",categoryName:"System Anatomy",categoryColor:"teal",slug:"clip",title:"CLIP Text Encoder: Der Prompt-Übersetzer",excerpt:"\u003Cp>\u003Cstrong>CLIP (Contrastive Language-Image Pre-training)\u003C/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...\u003C/p>\n",plainExcerpt:"CLIP (Contrastive Language-Image Pre-training) ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...",content:"\u003Cp>\u003Cstrong>CLIP (Contrastive Language-Image Pre-training)\u003C/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwischen Text und Bildern &quot;verstehen&quot;, revolutioniert hat. [1] Innerhalb eines Diffusionsmodells ist seine Rolle die eines \u003Cstrong>Universalübersetzers\u003C/strong>: Es nimmt Ihren Prompt in menschlicher Sprache und wandelt ihn in eine mathematische Darstellung (genannt \u003Cem>Embedding\u003C/em>) um, die das UNet als Leitfaden verwenden kann. [3]\u003C/p>\n\u003Cp>Die Begriffe &quot;Text Encoder&quot; und &quot;CLIP&quot; werden oft synonym verwendet. Ersterer beschreibt die \u003Cem>Funktion\u003C/em>, letzterer den \u003Cem>Namen\u003C/em> der Komponente, die diese Funktion ausführt.\u003C/p>\n\u003Cp>Ohne CLIP wäre der Prompt &quot;ein Astronaut auf einem Pferd&quot; für das UNet nur eine bedeutungslose Zeichenkette. Dank CLIP wird dieser Satz in eine Reihe von Zahlen umgewandelt, die die Konzepte &quot;Astronaut&quot;, &quot;Pferd&quot; und ihre räumliche Beziehung mathematisch &quot;beschreiben&quot;.\u003C/p>\n\u003Ch3>Wie funktioniert der &quot;Übersetzungs&quot;-Prozess?\u003C/h3>\n\u003Cp>Der \u003Ccode>CLIP Text Encode\u003C/code>-Knoten in ComfyUI führt einen mehrstufigen Prozess durch:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Tokenisierung:\u003C/strong>\nZuerst wird der Prompt in kleinere Teile namens \u003Cstrong>Token\u003C/strong> zerlegt. [2] Ein Token entspricht nicht unbedingt einem Wort. Komplexe Wörter können in mehrere Token aufgeteilt werden, während gebräuchliche Wörter ein einzelnes Token sein können. Jedes CLIP-Modell hat eine maximale Anzahl von Token, die es auf einmal verarbeiten kann (normalerweise 75). Wenn Ihr Prompt länger ist, wird er in mehrere &quot;Chunks&quot; (Teile) aufgeteilt.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Embedding:\u003C/strong>\nJedes Token wird in einen numerischen Vektor umgewandelt. An diesem Punkt haben wir eine Sequenz von Zahlen, die unseren Prompt darstellt.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Verarbeitung (Attention):\u003C/strong>\nDiese Sequenz von Zahlen wird von einer Transformer-Architektur verarbeitet. [3] Hier geschieht die eigentliche Magie: Der \u003Cstrong>Attention\u003C/strong>-Mechanismus ermöglicht es dem Modell, die Beziehungen zwischen den Wörtern zu verstehen. Es sieht nicht nur &quot;rot&quot; und &quot;Würfel&quot;, sondern es versteht, dass es der &quot;Würfel&quot; ist, der &quot;rot&quot; sein soll. Hier wirkt sich das Gewicht aus, das wir den Wörtern im Prompt geben (z. B. \u003Ccode>(Wort:1.2)\u003C/code>), indem es dem Aufmerksamkeitsmechanismus sagt, dass er bestimmten Konzepten &quot;mehr Aufmerksamkeit&quot; schenken soll.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>Das Endergebnis dieses Prozesses ist das \u003Cstrong>Conditioning\u003C/strong>, eine Ausgabe, die die Embeddings des Prompts enthält, die bereit sind, in das UNet &quot;injiziert&quot; zu werden und die Bilderzeugung zu leiten.\u003C/p>\n\u003Ch3>Verschiedene Modelle, verschiedene CLIPs\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Stable Diffusion 1.5\u003C/strong> verwendet ein einzelnes CLIP-Modell (OpenCLIP).\u003C/li>\n\u003Cli>\u003Cstrong>Stable Diffusion XL (SDXL)\u003C/strong> verwendet eine Kombination aus zwei verschiedenen CLIP-Modellen (OpenCLIP und CLIP ViT-L), was ihm ermöglicht, Prompts auf eine viel reichhaltigere und nuanciertere Weise zu verstehen. Dies ist einer der Hauptgründe für seine überlegene Qualität.\u003C/li>\n\u003C/ul>\n\u003Cp>Dies bedeutet, dass je mehr CLIPs zusammenarbeiten, mit vielen Milliarden von Parametern, desto mehr wird das Ergebnis dem Prompt entsprechen.\u003C/p>\n",sources:[{text:"OpenAIs Original-Paper zu CLIP",url:"https://arxiv.org/abs/2103.00020"},{text:"Erklärung von CLIP im Hugging Face-Blog",url:"https://huggingface.co/docs/transformers/main/en/model_doc/clip"},{text:"Illustrierter Artikel über die Funktionsweise von Stable Diffusion",url:"https://jalammar.github.io/illustrated-stable-diffusion/"}]},translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}},seo:{title:"CLIP Text Encoder: Der Prompt-Übersetzer",description:"\u003Cp>\u003Cstrong>CLIP (Contrastive Language-Image Pre-training)\u003C/strong> ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwi...\u003C/p>\n"},textContent:"CLIP Text Encoder: Der Prompt-Übersetzer. CLIP (Contrastive Language-Image Pre-training) ist ein von OpenAI entwickeltes neuronales Modell, das die Art und Weise, wie KIs die Beziehung zwischen Text und Bildern \"verstehen\", revolutioniert hat. Innerhalb eines Diffusionsmodells ist seine Rolle die eines Universalübersetzers: Es nimmt Ihren Prompt in menschlicher Sprache und wandelt ihn in eine mathematische Darstellung (genannt Embedding) um, die das UNet als Leitfaden verwenden kann. Die Begriffe \"Text Encoder\" und \"CLIP\" werden oft synonym verwendet. Ersterer beschreibt die Funktion, letzterer den Namen der Komponente, die diese Funktion ausführt. Ohne CLIP wäre der Prompt \"ein Astronaut auf einem Pferd\" für das UNet nur eine bedeutungslose Zeichenkette. Dank CLIP wird dieser Satz in eine Reihe von Zahlen umgewandelt, die die Konzepte \"Astronaut\", \"Pferd\" und ihre räumliche Beziehung mathematisch \"beschreiben\". Wie funktioniert der \"Übersetzungs\"-Prozess?; Der `CLIP Text Encode`-Knoten in ComfyUI führt einen mehrstufigen Prozess durch: 1. Tokenisierung: Zuerst wird der Prompt in kleinere Teile namens Token zerlegt. Ein Token entspricht nicht unbedingt einem Wort. Komplexe Wörter können in mehrere Token aufgeteilt werden, während gebräuchliche Wörter ein einzelnes Token sein können. Jedes CLIP-Modell hat eine maximale Anzahl von Token, die es auf einmal verarbeiten kann (normalerweise 75). Wenn Ihr Prompt länger ist, wird er in mehrere \"Chunks\" (Teile) aufgeteilt. 2. Embedding: Jedes Token wird in einen numerischen Vektor umgewandelt. An diesem Punkt haben wir eine Sequenz von Zahlen, die unseren Prompt darstellt. 3. Verarbeitung (Attention): Diese Sequenz von Zahlen wird von einer Transformer-Architektur verarbeitet. Hier geschieht die eigentliche Magie: Der Attention-Mechanismus ermöglicht es dem Modell, die Beziehungen zwischen den Wörtern zu verstehen. Es sieht nicht nur \"rot\" und \"Würfel\", sondern es versteht, dass es der \"Würfel\" ist, der \"rot\" sein soll. Hier wirkt sich das Gewicht aus, das wir den Wörtern im Prompt geben (z. B. `(Wort:1.2)`), indem es dem Aufmerksamkeitsmechanismus sagt, dass er bestimmten Konzepten \"mehr Aufmerksamkeit\" schenken soll. Das Endergebnis dieses Prozesses ist das Conditioning, eine Ausgabe, die die Embeddings des Prompts enthält, die bereit sind, in das UNet \"injiziert\" zu werden und die Bilderzeugung zu leiten. Verschiedene Modelle, verschiedene CLIPs; - Stable Diffusion 1.5 verwendet ein einzelnes CLIP-Modell (OpenCLIP). - Stable Diffusion XL (SDXL) verwendet eine Kombination aus zwei verschiedenen CLIP-Modellen (OpenCLIP und CLIP ViT-L), was ihm ermöglicht, Prompts auf eine viel reichhaltigere und nuanciertere Weise zu verstehen. Dies ist einer der Hauptgründe für seine überlegene Qualität. Dies bedeutet, dass je mehr CLIPs zusammenarbeiten, mit vielen Milliarden von Parametern, desto mehr wird das Ergebnis dem Prompt entsprechen."},uses:{params:["lang","category","slug"],parent:1}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
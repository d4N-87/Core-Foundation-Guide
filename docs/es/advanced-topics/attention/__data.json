{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":18,"seo":104,"textContent":105},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"es","advanced-topics","Advanced Topics","cyan","attention","Atención: El Mecanismo de Enfoque","\u003Cp>La \u003Cstrong>Atención\u003C/strong> (o \u003Cem>Autoatención\u003C/em>) es el mecanismo computacional en el corazón de la arquitectura \u003Cstrong>Transformer\u003C/strong>, que ha revolucionado tanto los mode...\u003C/p>\n","La Atención (o Autoatención) es el mecanismo computacional en el corazón de la arquitectura Transformer, que ha revolucionado tanto los mode...","\u003Cp>La \u003Cstrong>Atención\u003C/strong> (o \u003Cem>Autoatención\u003C/em>) es el mecanismo computacional en el corazón de la arquitectura \u003Cstrong>Transformer\u003C/strong>, que ha revolucionado tanto los modelos de lenguaje (LLM) como, más recientemente, los modelos de difusión (DiT). [1]\u003C/p>\n\u003Cp>En términos simples, la Atención permite a un modelo \u003Cstrong>ponderar dinámicamente la importancia de las diferentes partes de una entrada\u003C/strong> (como las palabras en una oración o los parches en una imagen) para comprender el contexto y las relaciones entre ellas. [2]\u003C/p>\n\u003Ch3>¿Cómo funciona (conceptualmente)?\u003C/h3>\n\u003Cp>Imagina que lees la frase: \u003Ccode>Un gato rojo persigue a un ratón gris\u003C/code>.\nCuando el modelo procesa la palabra &quot;rojo&quot;, el mecanismo de Atención le permite entender que &quot;rojo&quot; está fuertemente conectado a &quot;gato&quot; y no a &quot;ratón&quot;. En la práctica, para cada palabra, la Atención calcula una &quot;puntuación de atención&quot; con respecto a todas las demás palabras de la frase, &quot;enfocándose&quot; en las relaciones más importantes. [2]\u003C/p>\n\u003Cp>Esto es fundamental para resolver ambigüedades y comprender los matices del lenguaje.\u003C/p>\n\u003Ch3>La Atención en la Generación de Imágenes\u003C/h3>\n\u003Cp>El mecanismo de Atención es crucial en dos puntos de nuestro flujo de trabajo:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>En el Codificador de Texto CLIP:\u003C/strong>\nCuando CLIP procesa nuestro prompt, la Atención es lo que le permite entender que en \u003Ccode>un astronauta a caballo\u003C/code>, es el astronauta quien debe estar sobre el caballo. También es el mecanismo que se ve influenciado cuando aumentamos el peso de una palabra con la sintaxis \u003Ccode>(palabra:1.2)\u003C/code>, diciéndole que &quot;preste más atención&quot; a ese concepto.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>En los Transformadores de Difusión (DiT):\u003C/strong>\nEn modelos como Stable Diffusion 3, la Atención no solo se aplica al texto, sino también a los &quot;tokens visuales&quot; (los parches de la imagen). Esto permite al modelo crear relaciones complejas entre las diferentes partes de la imagen, mejorando drásticamente la coherencia y la composición. Por ejemplo, puede asegurarse de que un reflejo en un espejo corresponda correctamente al objeto reflejado.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>En resumen, la Atención es la tecnología que ha permitido a los modelos pasar de una simple &quot;asociación&quot; de palabras a una verdadera &quot;comprensión&quot; del contexto y las relaciones, tanto en el texto como en las imágenes.\u003C/p>\n",[12,15],{"text":13,"url":14},"Artículo 'Attention Is All You Need' que introdujo el Transformer","https://arxiv.org/abs/1706.03762",{"text":16,"url":17},"Explicación ilustrada del mecanismo de Atención","https://jalammar.github.io/visualizing-neural-machine-translation-self-attention-visualizations-for-transformer-models/",{"it":19,"en":34,"fr":49,"es":63,"de":78,"pt":93},{"category":20,"connections":21,"backToHub":22,"noPostsFound":23,"pageTitleCategory":20,"initializing":24,"backToArticles":25,"sources":26,"searchPlaceholder":27,"showMap":28,"hideMap":29,"listenToArticle":30,"playing":31,"paused":32,"voice":33},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":35,"connections":36,"backToHub":37,"noPostsFound":38,"pageTitleCategory":35,"initializing":39,"backToArticles":40,"sources":41,"searchPlaceholder":42,"showMap":43,"hideMap":44,"listenToArticle":45,"playing":46,"paused":47,"voice":48},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":50,"connections":51,"backToHub":52,"noPostsFound":53,"pageTitleCategory":50,"initializing":54,"backToArticles":55,"sources":41,"searchPlaceholder":56,"showMap":57,"hideMap":58,"listenToArticle":59,"playing":60,"paused":61,"voice":62},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":64,"connections":65,"backToHub":66,"noPostsFound":67,"pageTitleCategory":64,"initializing":68,"backToArticles":69,"sources":70,"searchPlaceholder":71,"showMap":72,"hideMap":73,"listenToArticle":74,"playing":75,"paused":76,"voice":77},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":79,"connections":80,"backToHub":81,"noPostsFound":82,"pageTitleCategory":79,"initializing":83,"backToArticles":84,"sources":85,"searchPlaceholder":86,"showMap":87,"hideMap":88,"listenToArticle":89,"playing":90,"paused":91,"voice":92},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":20,"connections":94,"backToHub":95,"noPostsFound":96,"pageTitleCategory":20,"initializing":68,"backToArticles":97,"sources":98,"searchPlaceholder":99,"showMap":100,"hideMap":73,"listenToArticle":101,"playing":102,"paused":103,"voice":77},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Atención: El Mecanismo de Enfoque. La Atención (o Autoatención) es el mecanismo computacional en el corazón de la arquitectura Transformer, que ha revolucionado tanto los modelos de lenguaje (LLM) como, más recientemente, los modelos de difusión (DiT). En términos simples, la Atención permite a un modelo ponderar dinámicamente la importancia de las diferentes partes de una entrada (como las palabras en una oración o los parches en una imagen) para comprender el contexto y las relaciones entre ellas. ¿Cómo funciona (conceptualmente)?; Imagina que lees la frase: `Un gato rojo persigue a un ratón gris`. Cuando el modelo procesa la palabra \"rojo\", el mecanismo de Atención le permite entender que \"rojo\" está fuertemente conectado a \"gato\" y no a \"ratón\". En la práctica, para cada palabra, la Atención calcula una \"puntuación de atención\" con respecto a todas las demás palabras de la frase, \"enfocándose\" en las relaciones más importantes. Esto es fundamental para resolver ambigüedades y comprender los matices del lenguaje. La Atención en la Generación de Imágenes; El mecanismo de Atención es crucial en dos puntos de nuestro flujo de trabajo: 1. En el Codificador de Texto CLIP: Cuando CLIP procesa nuestro prompt, la Atención es lo que le permite entender que en `un astronauta a caballo`, es el astronauta quien debe estar sobre el caballo. También es el mecanismo que se ve influenciado cuando aumentamos el peso de una palabra con la sintaxis `(palabra:1.2)`, diciéndole que \"preste más atención\" a ese concepto. 2. En los Transformadores de Difusión (DiT): En modelos como Stable Diffusion 3, la Atención no solo se aplica al texto, sino también a los \"tokens visuales\" (los parches de la imagen). Esto permite al modelo crear relaciones complejas entre las diferentes partes de la imagen, mejorando drásticamente la coherencia y la composición. Por ejemplo, puede asegurarse de que un reflejo en un espejo corresponda correctamente al objeto reflejado. En resumen, la Atención es la tecnología que ha permitido a los modelos pasar de una simple \"asociación\" de palabras a una verdadera \"comprensión\" del contexto y las relaciones, tanto en el texto como en las imágenes."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

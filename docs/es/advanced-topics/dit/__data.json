{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"es","advanced-topics","Advanced Topics","cyan","dit","DiT: Los Transformadores de Difusión","\u003Cp>Un \u003Cstrong>DiT (Diffusion Transformer)\u003C/strong> es una nueva arquitectura para los modelos de difusión que \u003Cstrong>reemplaza la UNet tradicional por un Transformer\u003C/strong>. [1...\u003C/p>\n","Un DiT (Diffusion Transformer) es una nueva arquitectura para los modelos de difusión que reemplaza la UNet tradicional por un Transformer. [1...","\u003Cp>Un \u003Cstrong>DiT (Diffusion Transformer)\u003C/strong> es una nueva arquitectura para los modelos de difusión que \u003Cstrong>reemplaza la UNet tradicional por un Transformer\u003C/strong>. [1] Es una evolución que toma prestadas las innovaciones del mundo de los Grandes Modelos de Lenguaje (LLM) y las aplica a la generación de imágenes, prometiendo una mayor escalabilidad y eficiencia.\u003C/p>\n\u003Ch3>¿Por qué reemplazar la UNet?\u003C/h3>\n\u003Cp>La \u003Cstrong>UNet\u003C/strong> ha sido la arquitectura dominante durante años, pero tiene limitaciones intrínsecas en su capacidad de &quot;escalar&quot;, es decir, de mejorar su rendimiento a medida que aumentan su tamaño y su potencia de cálculo.\u003C/p>\n\u003Cp>La arquitectura \u003Cstrong>Transformer\u003C/strong>, gracias a su mecanismo de \u003Cstrong>Atención\u003C/strong>, ha demostrado en los LLM ser increíblemente eficaz en la gestión y relación de grandes cantidades de datos. La idea detrás de los DiT es: &quot;¿Y si tratamos una imagen no como una cuadrícula de píxeles, sino como una secuencia de &#39;parches&#39; (trozos), de forma similar a como un Transformer trata una secuencia de palabras?&quot;. [1]\u003C/p>\n\u003Ch3>¿Cómo funciona un DiT?\u003C/h3>\n\u003Col>\n\u003Cli>La imagen latente se descompone en una serie de &quot;parches&quot; (tokens visuales).\u003C/li>\n\u003Cli>Estos tokens son procesados por un Transformer, que utiliza el mecanismo de Atención para comprender las relaciones entre las diferentes partes de la imagen.\u003C/li>\n\u003Cli>El Transformer, condicionado por el prompt, predice el ruido que se debe eliminar de cada parche.\u003C/li>\n\u003C/ol>\n\u003Cp>Este enfoque ha demostrado ser extremadamente escalable: cuanto más grande y potente es el Transformer, mejores son los resultados, superando el rendimiento de las UNet tradicionales con los mismos recursos. [1]\u003C/p>\n\u003Ch3>Ejemplos concretos y el futuro\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Sora:\u003C/strong> El revolucionario modelo de texto a vídeo de OpenAI se basa en una arquitectura DiT.\u003C/li>\n\u003Cli>\u003Cstrong>Stable Diffusion 3:\u003C/strong> La nueva versión del modelo de Stability AI abandona la UNet en favor de una arquitectura DiT, o más precisamente \u003Cstrong>MMDiT (Multi-Modal DiT)\u003C/strong>. [2] Un MMDiT utiliza dos Transformadores diferentes, uno para procesar los datos de texto y otro para los datos de imagen, lo que permite una comprensión mucho más profunda y precisa del prompt. [2]\u003C/li>\n\u003C/ul>\n\u003Cp>Los DiT representan un paso fundamental hacia modelos de generación cada vez más potentes, coherentes y capaces de comprender los complejos matices del lenguaje humano.\u003C/p>\n",[12,15,18],{"text":13,"url":14},"Artículo Original: Modelos de Difusión Escalables con Transformadores","https://arxiv.org/abs/2212.09748",{"text":16,"url":17},"Anuncio de Stable Diffusion 3, basado en DiT","https://stability.ai/news/stable-diffusion-3",{"text":19,"url":20},"Explicación de la arquitectura DiT - Hugging Face","https://huggingface.co/papers/2212.09748",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"DiT: Los Transformadores de Difusión. Un DiT (Diffusion Transformer) es una nueva arquitectura para los modelos de difusión que reemplaza la UNet tradicional por un Transformer. Es una evolución que toma prestadas las innovaciones del mundo de los Grandes Modelos de Lenguaje (LLM) y las aplica a la generación de imágenes, prometiendo una mayor escalabilidad y eficiencia. ¿Por qué reemplazar la UNet?; La UNet ha sido la arquitectura dominante durante años, pero tiene limitaciones intrínsecas en su capacidad de \"escalar\", es decir, de mejorar su rendimiento a medida que aumentan su tamaño y su potencia de cálculo. La arquitectura Transformer, gracias a su mecanismo de Atención, ha demostrado en los LLM ser increíblemente eficaz en la gestión y relación de grandes cantidades de datos. La idea detrás de los DiT es: \"¿Y si tratamos una imagen no como una cuadrícula de píxeles, sino como una secuencia de 'parches' (trozos), de forma similar a como un Transformer trata una secuencia de palabras?\". ¿Cómo funciona un DiT?; 1. La imagen latente se descompone en una serie de \"parches\" (tokens visuales). 2. Estos tokens son procesados por un Transformer, que utiliza el mecanismo de Atención para comprender las relaciones entre las diferentes partes de la imagen. 3. El Transformer, condicionado por el prompt, predice el ruido que se debe eliminar de cada parche. Este enfoque ha demostrado ser extremadamente escalable: cuanto más grande y potente es el Transformer, mejores son los resultados, superando el rendimiento de las UNet tradicionales con los mismos recursos. Ejemplos concretos y el futuro; - Sora: El revolucionario modelo de texto a vídeo de OpenAI se basa en una arquitectura DiT. - Stable Diffusion 3: La nueva versión del modelo de Stability AI abandona la UNet en favor de una arquitectura DiT, o más precisamente MMDiT (Multi-Modal DiT). Un MMDiT utiliza dos Transformadores diferentes, uno para procesar los datos de texto y otro para los datos de imagen, lo que permite una comprensión mucho más profunda y precisa del prompt. Los DiT representan un paso fundamental hacia modelos de generación cada vez más potentes, coherentes y capaces de comprender los complejos matices del lenguaje humano."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		
		<!-- 
			English: A comprehensive set of favicon links for different platforms (Apple, standard browsers) and the web manifest for PWA capabilities.
			Italiano: Un set completo di link per le favicon per diverse piattaforme (Apple, browser standard) e il web manifest per le funzionalità PWA.
		-->
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
		
		<!-- 
			English: Sets the viewport to ensure the site is responsive and scales correctly on all devices.
			Italiano: Imposta il viewport per assicurare che il sito sia responsivo e si adatti correttamente a tutti i dispositivi.
		-->
		<meta name="viewport" content="width=device-width, initial-scale=1" />

		<!-- 
			English: SvelteKit placeholder. This is where SvelteKit injects all necessary head content, like CSS links and meta tags from `svelte:head`.
			Italiano: Segnaposto di SvelteKit. Qui è dove SvelteKit inietta tutto il contenuto necessario per l'head, come i link CSS e i meta tag da `svelte:head`.
		-->
		
		<link href="../../_app/immutable/assets/0.CHJcp1PB.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.CQW7PVBU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DdPt9bVq.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B-icdhcA.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dv9Va7Iy.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.BRxldS8A.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/PPVm8Dsz.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DsnmJJEf.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BGc_5KK-.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C7sTqTmI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DUX4vovL.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BU9Ixn5H.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.AQxqvBRY.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ctJiNxf1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D2fIjOlU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BfGA2QYN.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Cmlm8h2R.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D5MSuBSp.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C8S6TabI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D40XMBwt.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.CJIju4Kh.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/6.B0mU0U6k.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/AGRw3LU9.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BoPAQx1w.js"><!--[--><meta name="description" content="&lt;p>The term &lt;strong>Checkpoint&lt;/strong> (or &lt;em>Model&lt;/em>) refers to the files that contain the &amp;quot;weights&amp;quot; of the neural network, that is, the &lt;strong>trained brain&lt;/strong> of the artif...&lt;/p>
"/>  <meta property="og:type" content="website"/> <meta property="og:url" content="http://sveltekit-prerender/en/system-anatomy/checkpoint"/> <meta property="og:title" content="Checkpoint: The Brain of the Model | Core Foundation Guide"/> <meta property="og:description" content="&lt;p>The term &lt;strong>Checkpoint&lt;/strong> (or &lt;em>Model&lt;/em>) refers to the files that contain the &amp;quot;weights&amp;quot; of the neural network, that is, the &lt;strong>trained brain&lt;/strong> of the artif...&lt;/p>
"/>  <meta property="twitter:card" content="summary_large_image"/> <meta property="twitter:url" content="http://sveltekit-prerender/en/system-anatomy/checkpoint"/> <meta property="twitter:title" content="Checkpoint: The Brain of the Model | Core Foundation Guide"/> <meta property="twitter:description" content="&lt;p>The term &lt;strong>Checkpoint&lt;/strong> (or &lt;em>Model&lt;/em>) refers to the files that contain the &amp;quot;weights&amp;quot; of the neural network, that is, the &lt;strong>trained brain&lt;/strong> of the artif...&lt;/p>
"/><!--]--><title>Checkpoint: The Brain of the Model | Core Foundation Guide</title>
	</head>
	<!-- 
		English: SvelteKit attribute to enable data preloading on mouse hover over links, making navigation feel faster.
		Italiano: Attributo di SvelteKit per abilitare il precaricamento dei dati al passaggio del mouse sui link, rendendo la navigazione più veloce.
	-->
	<body data-sveltekit-preload-data="hover">
		<!-- 
			English: SvelteKit placeholder for the main application body. The `display: contents` wrapper makes the div itself layout-neutral.
			Italiano: Segnaposto di SvelteKit per il corpo principale dell'applicazione. Il wrapper `display: contents` rende il div stesso neutro a livello di layout.
		-->
		<div style="display: contents"><!--[--><!--[--><!----><div class="fixed top-0 left-0 w-full h-full -z-10"></div><!----> <div class="relative z-10 isolate"><header class="fixed left-0 right-0 top-0 z-[60] w-full border-b border-cyan-900/50 bg-black/30 px-4 py-3 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl items-center justify-between"><div class="flex flex-shrink-0 items-center"><a href="#" target="_blank" rel="noopener noreferrer" aria-label="Core Foundation Guide GitHub Repository" class="transition-all duration-300 hover:scale-110 hover:drop-shadow-[0_0_8px_theme(colors.amber.400)] focus:scale-110 focus:outline-none"><img src="/logo.webp" alt="Logo" class="h-10 w-10 md:h-12 md:w-12"/></a> <div class="ml-3 min-w-0 text-xl font-bold tracking-wide text-slate-200 sm:ml-4 sm:text-2xl md:ml-6 md:text-3xl lg:text-4xl"><!--[--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">C</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">R</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">F</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">A</span><!--]--><!--[!--><span class="char-span inline-block ">T</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">O</span><!--]--><!--[!--><span class="char-span inline-block ">N</span><!--]--><!--[--><span class="char-span"> </span><!--]--><!--[!--><span class="char-span inline-block text-amber-400 char-highlighted">G</span><!--]--><!--[!--><span class="char-span inline-block ">U</span><!--]--><!--[!--><span class="char-span inline-block ">I</span><!--]--><!--[!--><span class="char-span inline-block ">D</span><!--]--><!--[!--><span class="char-span inline-block ">E</span><!--]--><!--]--></div></div> <svg class="hidden"><symbol id="icon-globe" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></symbol></svg> <div class="relative"><button aria-label="Change language" class="text-slate-400 transition-colors hover:text-white"><svg class="w-6 h-6"><use href="#icon-globe"></use></svg></button> <!--[!--><!--]--></div><!----></div></header><!----> <div class="flex min-h-screen flex-col pt-20"><main class="flex-grow"><!--[--><!--[--><!----><!--[--><!----><!----> <div class="flex w-full flex-grow items-center justify-center p-4 md:p-8"><div class="w-full max-w-4xl rounded-xl bg-gradient-to-br from-cyan-950/20 to-slate-950/10 backdrop-blur-lg border-2 border-cyan-500/30 shadow-2xl shadow-cyan-900/50 p-6 md:p-10" style="visibility: hidden;"><!--[--><button class="mb-8 block font-semibold text-cyan-400 transition-colors hover:text-amber-400">← Back to articles</button> <article class="prose prose-invert prose-strong:text-amber-400 prose-hr:border-cyan-500/30 prose-ol:text-gray-400 lg:prose-xl"><h1>Checkpoint: The Brain of the Model</h1> <!--[--><div class="not-prose my-8"><svg class="hidden"><symbol id="icon-play" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></symbol><symbol id="icon-stop" viewBox="0 0 24 24" fill="currentColor"><path d="M6 6h12v12H6z"></path></symbol><symbol id="icon-settings" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 0 2l-.15.08a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l-.22-.38a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1 0-2l.15-.08a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"></path><circle cx="12" cy="12" r="3"></circle></symbol></svg> <!--[!--><!--]--><!----></div><!--]-->  <!----><p>The term <strong>Checkpoint</strong> (or <em>Model</em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the <strong>trained brain</strong> of the artificial intelligence. [1] Loading a checkpoint is the first step of any workflow, but the way it is done reflects two main approaches: monolithic and modular.</p>
<h3>1. The Monolithic Approach (Traditional)</h3>
<p>In this approach, a single checkpoint file (with extension <code>.ckpt</code> or <code>.safetensors</code>) contains all three key components necessary for generation: [3]</p>
<ul>
<li>The <strong>UNet</strong>, the heart of the diffusion model.</li>
<li>The <strong>Text Encoder</strong> (CLIP), to interpret the prompt.</li>
<li>The <strong>VAE</strong>, to create the final image.</li>
</ul>
<p>This method is simple and direct: you load one file and you have everything you need. It is very common for models based on Stable Diffusion 1.5.</p>
<h3>2. The Modular Approach (Modern)</h3>
<p>With the advent of more complex models like FLUX.1 and the flexibility of interfaces like ComfyUI, it has become common to load the components separately. In this scenario, you don&#39;t load a single &quot;checkpoint&quot;, but its constituent parts:</p>
<ul>
<li>A file for the <strong>UNet</strong> (often called &quot;base model&quot; or &quot;diffusion model&quot;).</li>
<li>One or more files for the <strong>CLIP Text Encoder</strong> (FLUX.1 even uses two).</li>
<li>A file for the <strong>VAE</strong>.</li>
</ul>
<p>This approach offers enormous flexibility: you can, for example, use the UNet of one model with the VAE of another to correct color problems, or experiment with different Text Encoders. [1]</p>
<p><strong>So, does it still make sense to talk about Checkpoints?</strong>
Yes. The term &quot;checkpoint&quot; is still commonly used in the community to refer to the main model file, especially the <strong>UNet</strong>. When you download a &quot;finetuned&quot; model from Civitai, you are primarily downloading a modified UNet, which you can use both monolithically (if it contains everything) and modularly, pairing it with CLIP and VAE of your choice.</p>
<h3>The Hierarchy of Models</h3>
<p>We can classify models into a sort of hierarchy:</p>
<ol>
<li><p><strong>Base Models:</strong>
They are the foundations. Released by research labs (e.g., Stability AI, Black Forest Labs), they are trained on huge and generic datasets. They are very powerful but often do not have a defined artistic style. Examples: <code>Stable Diffusion 1.5</code>, <code>SDXL Base</code>. [3]</p>
</li>
<li><p><strong>Finetuned Models:</strong>
These are base models that the community has further trained on smaller and more specific datasets to achieve a particular style (e.g., photorealism, anime, fantasy). The vast majority of models on sites like Civitai fall into this category. [1, 3]</p>
</li>
<li><p><strong>Custom Models (Merge):</strong>
These are not trained, but created by <strong>mixing the weights</strong> of two or more finetuned models. It is a very popular technique for combining the styles of different models and creating a new and unique one. It&#39;s more of an art than a science, and the results can vary. [3]</p>
</li>
<li><p><strong>Distilled Models:</strong>
They are a special category. A &quot;distilled&quot; model is a smaller and faster version of a base model, created with a training process that &quot;distills&quot; the knowledge of the larger model. The most famous example is <strong>SDXL Turbo</strong>, which can generate high-quality images in very few steps (1-4), at the cost of less flexibility. [4] Or even versions like FLUX.1 Dev distilled from the Pro.</p>
</li>
</ol>
<h3>Formats: <code>.ckpt</code> vs. <code>.safetensors</code></h3>
<p>Regardless of the approach, the files are distributed in two formats:</p>
<ul>
<li><strong><code>.ckpt</code> (Checkpoint):</strong> The original format based on Python&#39;s &quot;pickle&quot;. Potentially insecure, as it can contain executable code. [2]</li>
<li><strong><code>.safetensors</code> (Safe Tensors):</strong> The new standard, safer and faster to load, which contains only the model&#39;s data. [2] <strong>It is always recommended to prefer the <code>.safetensors</code> format when available.</strong></li>
</ul>
<!---->  <!--[--><hr/> <h2>Sources</h2> <ol><!--[--><li><a href="https://stablediffusionart.com/models/" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">What are Stable Diffusion Models? - Stable Diffusion Art</a></li><li><a href="https://huggingface.co/docs/safetensors/index" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Explanation of .ckpt and .safetensors formats - Hugging Face</a></li><li><a href="https://civitai.com/articles/8/a-guide-to-the-different-ai-model-types" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Guide to different types of AI models</a></li><li><a href="https://stability.ai/news/sdxl-turbo" target="_blank" rel="noopener noreferrer" class="text-cyan-400 transition-colors hover:text-amber-400 hover:underline">Introduction to Distilled Models (SDXL Turbo)</a></li><!--]--></ol><!--]--></article><!--]--></div></div><!----><!--]--><!----><!--]--><!--]--></main> <svg class="hidden"><symbol id="icon-github" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.91 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></symbol></svg> <footer class="w-full border-t border-cyan-900/50 bg-black/30 px-4 py-6 backdrop-blur-md md:px-8"><div class="mx-auto flex max-w-7xl flex-col items-center justify-between gap-4 sm:flex-row"><p class="text-sm text-slate-400">© 2025 Core Foundation Guide. An interactive field manual for AI concepts.</p>  <a href="#" target="_blank" rel="noopener noreferrer" aria-label="GitHub Repository" class="text-slate-400 transition-colors hover:text-white"><svg class="h-6 w-6"><use href="#icon-github"></use></svg></a></div></footer><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1erws2 = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.CQW7PVBU.js"),
						import("../../_app/immutable/entry/app.BRxldS8A.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 6],
							data: [{type:"data",data:{translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}}},uses:{}},null,{type:"data",data:{post:{lang:"en",categorySlug:"system-anatomy",categoryName:"System Anatomy",categoryColor:"teal",slug:"checkpoint",title:"Checkpoint: The Brain of the Model",excerpt:"\u003Cp>The term \u003Cstrong>Checkpoint\u003C/strong> (or \u003Cem>Model\u003C/em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the \u003Cstrong>trained brain\u003C/strong> of the artif...\u003C/p>\n",plainExcerpt:"The term Checkpoint (or Model) refers to the files that contain the \"weights\" of the neural network, that is, the trained brain of the artif...",content:"\u003Cp>The term \u003Cstrong>Checkpoint\u003C/strong> (or \u003Cem>Model\u003C/em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the \u003Cstrong>trained brain\u003C/strong> of the artificial intelligence. [1] Loading a checkpoint is the first step of any workflow, but the way it is done reflects two main approaches: monolithic and modular.\u003C/p>\n\u003Ch3>1. The Monolithic Approach (Traditional)\u003C/h3>\n\u003Cp>In this approach, a single checkpoint file (with extension \u003Ccode>.ckpt\u003C/code> or \u003Ccode>.safetensors\u003C/code>) contains all three key components necessary for generation: [3]\u003C/p>\n\u003Cul>\n\u003Cli>The \u003Cstrong>UNet\u003C/strong>, the heart of the diffusion model.\u003C/li>\n\u003Cli>The \u003Cstrong>Text Encoder\u003C/strong> (CLIP), to interpret the prompt.\u003C/li>\n\u003Cli>The \u003Cstrong>VAE\u003C/strong>, to create the final image.\u003C/li>\n\u003C/ul>\n\u003Cp>This method is simple and direct: you load one file and you have everything you need. It is very common for models based on Stable Diffusion 1.5.\u003C/p>\n\u003Ch3>2. The Modular Approach (Modern)\u003C/h3>\n\u003Cp>With the advent of more complex models like FLUX.1 and the flexibility of interfaces like ComfyUI, it has become common to load the components separately. In this scenario, you don&#39;t load a single &quot;checkpoint&quot;, but its constituent parts:\u003C/p>\n\u003Cul>\n\u003Cli>A file for the \u003Cstrong>UNet\u003C/strong> (often called &quot;base model&quot; or &quot;diffusion model&quot;).\u003C/li>\n\u003Cli>One or more files for the \u003Cstrong>CLIP Text Encoder\u003C/strong> (FLUX.1 even uses two).\u003C/li>\n\u003Cli>A file for the \u003Cstrong>VAE\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003Cp>This approach offers enormous flexibility: you can, for example, use the UNet of one model with the VAE of another to correct color problems, or experiment with different Text Encoders. [1]\u003C/p>\n\u003Cp>\u003Cstrong>So, does it still make sense to talk about Checkpoints?\u003C/strong>\nYes. The term &quot;checkpoint&quot; is still commonly used in the community to refer to the main model file, especially the \u003Cstrong>UNet\u003C/strong>. When you download a &quot;finetuned&quot; model from Civitai, you are primarily downloading a modified UNet, which you can use both monolithically (if it contains everything) and modularly, pairing it with CLIP and VAE of your choice.\u003C/p>\n\u003Ch3>The Hierarchy of Models\u003C/h3>\n\u003Cp>We can classify models into a sort of hierarchy:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Base Models:\u003C/strong>\nThey are the foundations. Released by research labs (e.g., Stability AI, Black Forest Labs), they are trained on huge and generic datasets. They are very powerful but often do not have a defined artistic style. Examples: \u003Ccode>Stable Diffusion 1.5\u003C/code>, \u003Ccode>SDXL Base\u003C/code>. [3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Finetuned Models:\u003C/strong>\nThese are base models that the community has further trained on smaller and more specific datasets to achieve a particular style (e.g., photorealism, anime, fantasy). The vast majority of models on sites like Civitai fall into this category. [1, 3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Custom Models (Merge):\u003C/strong>\nThese are not trained, but created by \u003Cstrong>mixing the weights\u003C/strong> of two or more finetuned models. It is a very popular technique for combining the styles of different models and creating a new and unique one. It&#39;s more of an art than a science, and the results can vary. [3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Distilled Models:\u003C/strong>\nThey are a special category. A &quot;distilled&quot; model is a smaller and faster version of a base model, created with a training process that &quot;distills&quot; the knowledge of the larger model. The most famous example is \u003Cstrong>SDXL Turbo\u003C/strong>, which can generate high-quality images in very few steps (1-4), at the cost of less flexibility. [4] Or even versions like FLUX.1 Dev distilled from the Pro.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch3>Formats: \u003Ccode>.ckpt\u003C/code> vs. \u003Ccode>.safetensors\u003C/code>\u003C/h3>\n\u003Cp>Regardless of the approach, the files are distributed in two formats:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>.ckpt\u003C/code> (Checkpoint):\u003C/strong> The original format based on Python&#39;s &quot;pickle&quot;. Potentially insecure, as it can contain executable code. [2]\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>.safetensors\u003C/code> (Safe Tensors):\u003C/strong> The new standard, safer and faster to load, which contains only the model&#39;s data. [2] \u003Cstrong>It is always recommended to prefer the \u003Ccode>.safetensors\u003C/code> format when available.\u003C/strong>\u003C/li>\n\u003C/ul>\n",sources:[{text:"What are Stable Diffusion Models? - Stable Diffusion Art",url:"https://stablediffusionart.com/models/"},{text:"Explanation of .ckpt and .safetensors formats - Hugging Face",url:"https://huggingface.co/docs/safetensors/index"},{text:"Guide to different types of AI models",url:"https://civitai.com/articles/8/a-guide-to-the-different-ai-model-types"},{text:"Introduction to Distilled Models (SDXL Turbo)",url:"https://stability.ai/news/sdxl-turbo"}]},translations:{it:{category:"Categoria",connections:"Collegamenti",backToHub:"Torna all'Hub",noPostsFound:"Nessun articolo trovato.",pageTitleCategory:"Categoria",initializing:"Inizializzazione...",backToArticles:"Torna agli articoli",sources:"Fonti",searchPlaceholder:"Cerca articoli...",showMap:"Mostra Mappa Contenuti",hideMap:"Nascondi Mappa",listenToArticle:"Ascolta questo articolo",playing:"In riproduzione...",paused:"In pausa",voice:"Voce"},en:{category:"Category",connections:"Connections",backToHub:"Back to Hub",noPostsFound:"No articles found.",pageTitleCategory:"Category",initializing:"Initializing...",backToArticles:"Back to articles",sources:"Sources",searchPlaceholder:"Search articles...",showMap:"Show Content Map",hideMap:"Hide Map",listenToArticle:"Listen to this article",playing:"Playing...",paused:"Paused",voice:"Voice"},fr:{category:"Catégorie",connections:"Connexions",backToHub:"Retour à l'accueil",noPostsFound:"Aucun article trouvé.",pageTitleCategory:"Catégorie",initializing:"Initialisation...",backToArticles:"Retour aux articles",sources:"Sources",searchPlaceholder:"Rechercher des articles...",showMap:"Afficher la carte du contenu",hideMap:"Masquer la carte",listenToArticle:"Écouter cet article",playing:"Lecture en cours...",paused:"En pause",voice:"Voix"},es:{category:"Categoría",connections:"Conexiones",backToHub:"Volver al inicio",noPostsFound:"No se encontraron artículos.",pageTitleCategory:"Categoría",initializing:"Inicializando...",backToArticles:"Volver a los artículos",sources:"Fuentes",searchPlaceholder:"Buscar artículos...",showMap:"Mostrar mapa de contenido",hideMap:"Ocultar mapa",listenToArticle:"Escuchar este artículo",playing:"Reproduciendo...",paused:"En pausa",voice:"Voz"},de:{category:"Kategorie",connections:"Verbindungen",backToHub:"Zurück zum Hub",noPostsFound:"Keine Artikel gefunden.",pageTitleCategory:"Kategorie",initializing:"Initialisiere...",backToArticles:"Zurück zu den Artikeln",sources:"Quellen",searchPlaceholder:"Artikel suchen...",showMap:"Inhaltsverzeichnis anzeigen",hideMap:"Verzeichnis ausblenden",listenToArticle:"Diesen Artikel anhören",playing:"Wiedergabe...",paused:"Pausiert",voice:"Stimme"},pt:{category:"Categoria",connections:"Conexões",backToHub:"Voltar ao início",noPostsFound:"Nenhum artigo encontrado.",pageTitleCategory:"Categoria",initializing:"Inicializando...",backToArticles:"Voltar aos artigos",sources:"Fontes",searchPlaceholder:"Pesquisar artigos...",showMap:"Mostrar mapa de conteúdo",hideMap:"Ocultar mapa",listenToArticle:"Ouvir este artigo",playing:"Reproduzindo...",paused:"Em pausa",voice:"Voz"}},seo:{title:"Checkpoint: The Brain of the Model",description:"\u003Cp>The term \u003Cstrong>Checkpoint\u003C/strong> (or \u003Cem>Model\u003C/em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the \u003Cstrong>trained brain\u003C/strong> of the artif...\u003C/p>\n"},textContent:"Checkpoint: The Brain of the Model. The term Checkpoint (or Model) refers to the files that contain the \"weights\" of the neural network, that is, the trained brain of the artificial intelligence. Loading a checkpoint is the first step of any workflow, but the way it is done reflects two main approaches: monolithic and modular. 1. The Monolithic Approach (Traditional); In this approach, a single checkpoint file (with extension `.ckpt` or `.safetensors`) contains all three key components necessary for generation: - The UNet, the heart of the diffusion model. - The Text Encoder (CLIP), to interpret the prompt. - The VAE, to create the final image. This method is simple and direct: you load one file and you have everything you need. It is very common for models based on Stable Diffusion 1.5. 2. The Modular Approach (Modern); With the advent of more complex models like FLUX.1 and the flexibility of interfaces like ComfyUI, it has become common to load the components separately. In this scenario, you don't load a single \"checkpoint\", but its constituent parts: - A file for the UNet (often called \"base model\" or \"diffusion model\"). - One or more files for the CLIP Text Encoder (FLUX.1 even uses two). - A file for the VAE. This approach offers enormous flexibility: you can, for example, use the UNet of one model with the VAE of another to correct color problems, or experiment with different Text Encoders. So, does it still make sense to talk about Checkpoints? Yes. The term \"checkpoint\" is still commonly used in the community to refer to the main model file, especially the UNet. When you download a \"finetuned\" model from Civitai, you are primarily downloading a modified UNet, which you can use both monolithically (if it contains everything) and modularly, pairing it with CLIP and VAE of your choice. The Hierarchy of Models; We can classify models into a sort of hierarchy: 1. Base Models: They are the foundations. Released by research labs (e.g., Stability AI, Black Forest Labs), they are trained on huge and generic datasets. They are very powerful but often do not have a defined artistic style. Examples: `Stable Diffusion 1.5`, `SDXL Base`. 2. Finetuned Models: These are base models that the community has further trained on smaller and more specific datasets to achieve a particular style (e.g., photorealism, anime, fantasy). The vast majority of models on sites like Civitai fall into this category. [1, 3] 3. Custom Models (Merge): These are not trained, but created by mixing the weights of two or more finetuned models. It is a very popular technique for combining the styles of different models and creating a new and unique one. It's more of an art than a science, and the results can vary. 4. Distilled Models: They are a special category. A \"distilled\" model is a smaller and faster version of a base model, created with a training process that \"distills\" the knowledge of the larger model. The most famous example is SDXL Turbo, which can generate high-quality images in very few steps (1-4), at the cost of less flexibility. Or even versions like FLUX.1 Dev distilled from the Pro. Formats: `.ckpt` vs. `.safetensors`; Regardless of the approach, the files are distributed in two formats: - `.ckpt` (Checkpoint): The original format based on Python's \"pickle\". Potentially insecure, as it can contain executable code. - `.safetensors` (Safe Tensors): The new standard, safer and faster to load, which contains only the model's data. It is always recommended to prefer the `.safetensors` format when available."},uses:{params:["lang","category","slug"],parent:1}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":24,"seo":110,"textContent":111},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"en","system-anatomy","System Anatomy","teal","checkpoint","Checkpoint: The Brain of the Model","\u003Cp>The term \u003Cstrong>Checkpoint\u003C/strong> (or \u003Cem>Model\u003C/em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the \u003Cstrong>trained brain\u003C/strong> of the artif...\u003C/p>\n","The term Checkpoint (or Model) refers to the files that contain the \"weights\" of the neural network, that is, the trained brain of the artif...","\u003Cp>The term \u003Cstrong>Checkpoint\u003C/strong> (or \u003Cem>Model\u003C/em>) refers to the files that contain the &quot;weights&quot; of the neural network, that is, the \u003Cstrong>trained brain\u003C/strong> of the artificial intelligence. [1] Loading a checkpoint is the first step of any workflow, but the way it is done reflects two main approaches: monolithic and modular.\u003C/p>\n\u003Ch3>1. The Monolithic Approach (Traditional)\u003C/h3>\n\u003Cp>In this approach, a single checkpoint file (with extension \u003Ccode>.ckpt\u003C/code> or \u003Ccode>.safetensors\u003C/code>) contains all three key components necessary for generation: [3]\u003C/p>\n\u003Cul>\n\u003Cli>The \u003Cstrong>UNet\u003C/strong>, the heart of the diffusion model.\u003C/li>\n\u003Cli>The \u003Cstrong>Text Encoder\u003C/strong> (CLIP), to interpret the prompt.\u003C/li>\n\u003Cli>The \u003Cstrong>VAE\u003C/strong>, to create the final image.\u003C/li>\n\u003C/ul>\n\u003Cp>This method is simple and direct: you load one file and you have everything you need. It is very common for models based on Stable Diffusion 1.5.\u003C/p>\n\u003Ch3>2. The Modular Approach (Modern)\u003C/h3>\n\u003Cp>With the advent of more complex models like FLUX.1 and the flexibility of interfaces like ComfyUI, it has become common to load the components separately. In this scenario, you don&#39;t load a single &quot;checkpoint&quot;, but its constituent parts:\u003C/p>\n\u003Cul>\n\u003Cli>A file for the \u003Cstrong>UNet\u003C/strong> (often called &quot;base model&quot; or &quot;diffusion model&quot;).\u003C/li>\n\u003Cli>One or more files for the \u003Cstrong>CLIP Text Encoder\u003C/strong> (FLUX.1 even uses two).\u003C/li>\n\u003Cli>A file for the \u003Cstrong>VAE\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003Cp>This approach offers enormous flexibility: you can, for example, use the UNet of one model with the VAE of another to correct color problems, or experiment with different Text Encoders. [1]\u003C/p>\n\u003Cp>\u003Cstrong>So, does it still make sense to talk about Checkpoints?\u003C/strong>\nYes. The term &quot;checkpoint&quot; is still commonly used in the community to refer to the main model file, especially the \u003Cstrong>UNet\u003C/strong>. When you download a &quot;finetuned&quot; model from Civitai, you are primarily downloading a modified UNet, which you can use both monolithically (if it contains everything) and modularly, pairing it with CLIP and VAE of your choice.\u003C/p>\n\u003Ch3>The Hierarchy of Models\u003C/h3>\n\u003Cp>We can classify models into a sort of hierarchy:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Base Models:\u003C/strong>\nThey are the foundations. Released by research labs (e.g., Stability AI, Black Forest Labs), they are trained on huge and generic datasets. They are very powerful but often do not have a defined artistic style. Examples: \u003Ccode>Stable Diffusion 1.5\u003C/code>, \u003Ccode>SDXL Base\u003C/code>. [3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Finetuned Models:\u003C/strong>\nThese are base models that the community has further trained on smaller and more specific datasets to achieve a particular style (e.g., photorealism, anime, fantasy). The vast majority of models on sites like Civitai fall into this category. [1, 3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Custom Models (Merge):\u003C/strong>\nThese are not trained, but created by \u003Cstrong>mixing the weights\u003C/strong> of two or more finetuned models. It is a very popular technique for combining the styles of different models and creating a new and unique one. It&#39;s more of an art than a science, and the results can vary. [3]\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Distilled Models:\u003C/strong>\nThey are a special category. A &quot;distilled&quot; model is a smaller and faster version of a base model, created with a training process that &quot;distills&quot; the knowledge of the larger model. The most famous example is \u003Cstrong>SDXL Turbo\u003C/strong>, which can generate high-quality images in very few steps (1-4), at the cost of less flexibility. [4] Or even versions like FLUX.1 Dev distilled from the Pro.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch3>Formats: \u003Ccode>.ckpt\u003C/code> vs. \u003Ccode>.safetensors\u003C/code>\u003C/h3>\n\u003Cp>Regardless of the approach, the files are distributed in two formats:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>.ckpt\u003C/code> (Checkpoint):\u003C/strong> The original format based on Python&#39;s &quot;pickle&quot;. Potentially insecure, as it can contain executable code. [2]\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>.safetensors\u003C/code> (Safe Tensors):\u003C/strong> The new standard, safer and faster to load, which contains only the model&#39;s data. [2] \u003Cstrong>It is always recommended to prefer the \u003Ccode>.safetensors\u003C/code> format when available.\u003C/strong>\u003C/li>\n\u003C/ul>\n",[12,15,18,21],{"text":13,"url":14},"What are Stable Diffusion Models? - Stable Diffusion Art","https://stablediffusionart.com/models/",{"text":16,"url":17},"Explanation of .ckpt and .safetensors formats - Hugging Face","https://huggingface.co/docs/safetensors/index",{"text":19,"url":20},"Guide to different types of AI models","https://civitai.com/articles/8/a-guide-to-the-different-ai-model-types",{"text":22,"url":23},"Introduction to Distilled Models (SDXL Turbo)","https://stability.ai/news/sdxl-turbo",{"it":25,"en":40,"fr":55,"es":69,"de":84,"pt":99},{"category":26,"connections":27,"backToHub":28,"noPostsFound":29,"pageTitleCategory":26,"initializing":30,"backToArticles":31,"sources":32,"searchPlaceholder":33,"showMap":34,"hideMap":35,"listenToArticle":36,"playing":37,"paused":38,"voice":39},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":41,"connections":42,"backToHub":43,"noPostsFound":44,"pageTitleCategory":41,"initializing":45,"backToArticles":46,"sources":47,"searchPlaceholder":48,"showMap":49,"hideMap":50,"listenToArticle":51,"playing":52,"paused":53,"voice":54},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":56,"connections":57,"backToHub":58,"noPostsFound":59,"pageTitleCategory":56,"initializing":60,"backToArticles":61,"sources":47,"searchPlaceholder":62,"showMap":63,"hideMap":64,"listenToArticle":65,"playing":66,"paused":67,"voice":68},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":70,"connections":71,"backToHub":72,"noPostsFound":73,"pageTitleCategory":70,"initializing":74,"backToArticles":75,"sources":76,"searchPlaceholder":77,"showMap":78,"hideMap":79,"listenToArticle":80,"playing":81,"paused":82,"voice":83},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":85,"connections":86,"backToHub":87,"noPostsFound":88,"pageTitleCategory":85,"initializing":89,"backToArticles":90,"sources":91,"searchPlaceholder":92,"showMap":93,"hideMap":94,"listenToArticle":95,"playing":96,"paused":97,"voice":98},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":26,"connections":100,"backToHub":101,"noPostsFound":102,"pageTitleCategory":26,"initializing":74,"backToArticles":103,"sources":104,"searchPlaceholder":105,"showMap":106,"hideMap":79,"listenToArticle":107,"playing":108,"paused":109,"voice":83},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Checkpoint: The Brain of the Model. The term Checkpoint (or Model) refers to the files that contain the \"weights\" of the neural network, that is, the trained brain of the artificial intelligence. Loading a checkpoint is the first step of any workflow, but the way it is done reflects two main approaches: monolithic and modular. 1. The Monolithic Approach (Traditional); In this approach, a single checkpoint file (with extension `.ckpt` or `.safetensors`) contains all three key components necessary for generation: - The UNet, the heart of the diffusion model. - The Text Encoder (CLIP), to interpret the prompt. - The VAE, to create the final image. This method is simple and direct: you load one file and you have everything you need. It is very common for models based on Stable Diffusion 1.5. 2. The Modular Approach (Modern); With the advent of more complex models like FLUX.1 and the flexibility of interfaces like ComfyUI, it has become common to load the components separately. In this scenario, you don't load a single \"checkpoint\", but its constituent parts: - A file for the UNet (often called \"base model\" or \"diffusion model\"). - One or more files for the CLIP Text Encoder (FLUX.1 even uses two). - A file for the VAE. This approach offers enormous flexibility: you can, for example, use the UNet of one model with the VAE of another to correct color problems, or experiment with different Text Encoders. So, does it still make sense to talk about Checkpoints? Yes. The term \"checkpoint\" is still commonly used in the community to refer to the main model file, especially the UNet. When you download a \"finetuned\" model from Civitai, you are primarily downloading a modified UNet, which you can use both monolithically (if it contains everything) and modularly, pairing it with CLIP and VAE of your choice. The Hierarchy of Models; We can classify models into a sort of hierarchy: 1. Base Models: They are the foundations. Released by research labs (e.g., Stability AI, Black Forest Labs), they are trained on huge and generic datasets. They are very powerful but often do not have a defined artistic style. Examples: `Stable Diffusion 1.5`, `SDXL Base`. 2. Finetuned Models: These are base models that the community has further trained on smaller and more specific datasets to achieve a particular style (e.g., photorealism, anime, fantasy). The vast majority of models on sites like Civitai fall into this category. [1, 3] 3. Custom Models (Merge): These are not trained, but created by mixing the weights of two or more finetuned models. It is a very popular technique for combining the styles of different models and creating a new and unique one. It's more of an art than a science, and the results can vary. 4. Distilled Models: They are a special category. A \"distilled\" model is a smaller and faster version of a base model, created with a training process that \"distills\" the knowledge of the larger model. The most famous example is SDXL Turbo, which can generate high-quality images in very few steps (1-4), at the cost of less flexibility. Or even versions like FLUX.1 Dev distilled from the Pro. Formats: `.ckpt` vs. `.safetensors`; Regardless of the approach, the files are distributed in two formats: - `.ckpt` (Checkpoint): The original format based on Python's \"pickle\". Potentially insecure, as it can contain executable code. - `.safetensors` (Safe Tensors): The new standard, safer and faster to load, which contains only the model's data. It is always recommended to prefer the `.safetensors` format when available."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

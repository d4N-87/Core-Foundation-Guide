{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"it","advanced-topics","Advanced Topics","cyan","gguf","GGUF: Quantizzazione per CPU e GPU","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> è un formato di file progettato per contenere modelli neurali \u003Cstrong>quantizzati\u003C/strong>, ovvero convertiti in format...\u003C/p>\n","GGUF (Georgi Gerganov Universal Format) è un formato di file progettato per contenere modelli neurali quantizzati, ovvero convertiti in format...","\u003Cp>\u003Cstrong>GGUF (Georgi Gerganov Universal Format)\u003C/strong> è un formato di file progettato per contenere modelli neurali \u003Cstrong>quantizzati\u003C/strong>, ovvero convertiti in formati a bassissima precisione (come 4 o 8 bit) per ridurne drasticamente le dimensioni e il consumo di memoria. [1]\u003C/p>\n\u003Cp>Nato dal progetto \u003Cstrong>\u003Ccode>llama.cpp\u003C/code>\u003C/strong> per eseguire Large Language Models (LLM) su CPU, il suo utilizzo si è recentemente espanso anche all&#39;ecosistema dei modelli di diffusione per immagini. [2]\u003C/p>\n\u003Ch3>Lo Scopo Principale: Ridurre il Consumo di Memoria\u003C/h3>\n\u003Cp>Il vantaggio chiave di GGUF è la \u003Cstrong>quantizzazione\u003C/strong>. Un modello che in formato FP16 (\u003Ccode>.safetensors\u003C/code>) occuperebbe 14 GB di VRAM, in formato GGUF quantizzato a 4-bit (\u003Ccode>q4_K_M\u003C/code>) può occuparne meno di 5 GB. Questo permette di:\u003C/p>\n\u003Cul>\n\u003Cli>Eseguire modelli enormi su GPU con meno VRAM.\u003C/li>\n\u003Cli>Caricare più componenti in memoria contemporaneamente.\u003C/li>\n\u003Cli>Eseguire modelli su CPU in modo efficiente.\u003C/li>\n\u003C/ul>\n\u003Ch3>GGUF nel Mondo degli LLM (Uso Classico)\u003C/h3>\n\u003Cp>L&#39;uso primario di GGUF è per i modelli linguistici. Interfacce come LM Studio o Ollama usano file GGUF per far girare chatbot potenti (come Llama, Mistral) su hardware consumer, sfruttando principalmente la CPU. [3]\u003C/p>\n\u003Ch3>GGUF nel Mondo della Diffusione (Uso Moderno in ComfyUI)\u003C/h3>\n\u003Cp>Recentemente, la community ha iniziato ad applicare i vantaggi della quantizzazione GGUF anche ai componenti di elaborazione. In ComfyUI, tramite nodi specializzati (\u003Ccode>Load GGUF Model\u003C/code>), è possibile caricare versioni GGUF di:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Text Encoder (CLIP):\u003C/strong> Caricare un CLIP quantizzato riduce significativamente il suo impatto sulla VRAM, liberando risorse preziose per il modello UNet. Questo è l&#39;uso più comune ed efficace.\u003C/li>\n\u003Cli>\u003Cstrong>UNet:\u003C/strong> Esistono anche esperimenti per quantizzare l&#39;intera UNet in formato GGUF. Sebbene questo offra il massimo risparmio di memoria, può portare a una perdita di qualità più evidente nell&#39;immagine finale rispetto all&#39;uso di una UNet in formato FP16.\u003C/li>\n\u003C/ul>\n\u003Cp>È uno strumento versatile per la \u003Cstrong>gestione avanzata della memoria\u003C/strong>, permettendo agli utenti di eseguire workflow sempre più complessi su hardware consumer, bilanciando sapientemente il compromesso tra consumo di VRAM e qualità dell&#39;output.\u003C/p>\n\u003Ch3>Decifrare le Nomenclature di Quantizzazione (es. \u003Ccode>Q4_K_M\u003C/code>)\u003C/h3>\n\u003Cp>Quando si scarica un modello GGUF, il nome del file contiene spesso una sigla che descrive il metodo di quantizzazione usato. Capirla aiuta a scegliere il giusto equilibrio tra dimensione e qualità. Ecco come leggerla:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q\u003C/code> seguito da un numero (es. \u003Ccode>Q4\u003C/code>, \u003Ccode>Q5\u003C/code>, \u003Ccode>Q8\u003C/code>):\u003C/strong> Indica il numero di \u003Cstrong>bit\u003C/strong> usati per ogni peso. \u003Ccode>Q8\u003C/code> usa 8 bit (qualità più alta, file più grande), \u003Ccode>Q4\u003C/code> usa 4 bit (qualità più bassa, file più piccolo).\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_K\u003C/code>:\u003C/strong> Indica una variante &quot;K-Quant&quot;. È una tecnica di quantizzazione migliorata che cerca di preservare meglio la qualità dell&#39;informazione, specialmente per i pesi più importanti. I modelli \u003Ccode>_K\u003C/code> sono spesso la scelta raccomandata.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_0\u003C/code> o \u003Ccode>_1\u003C/code> (es. \u003Ccode>Q4_0\u003C/code>, \u003Ccode>Q5_1\u003C/code>):\u003C/strong> Indicano versioni diverse dello stesso metodo. \u003Ccode>_0\u003C/code> è la versione &quot;pura&quot; a 4-bit, mentre \u003Ccode>_1\u003C/code> è una versione mista che usa una precisione leggermente più alta (5-bit) per alcuni pesi, offrendo un piccolo miglioramento di qualità con un file leggermente più grande.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>_S\u003C/code>, \u003Ccode>_M\u003C/code>, \u003Ccode>_L\u003C/code> (es. \u003Ccode>Q4_K_S\u003C/code>):\u003C/strong> Indicano le dimensioni del modello (&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;). Non si riferiscono alla quantizzazione stessa, ma a diverse &quot;taglie&quot; del modello originale.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Esempi Pratici:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>Q8_0\u003C/code>:\u003C/strong> Quantizzazione a 8-bit. Massima qualità tra le versioni GGUF, ma anche la più pesante.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q5_K_M\u003C/code>:\u003C/strong> Quantizzazione &quot;K-Quant&quot; a 5-bit, versione &quot;Medium&quot;. Un ottimo compromesso tra qualità e dimensione.\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>Q4_0\u003C/code>:\u003C/strong> Quantizzazione &quot;pura&quot; a 4-bit. La versione più piccola e leggera, ma con la maggiore perdita di qualità. Spesso usata per far girare modelli enormi su hardware molto limitato.\u003C/li>\n\u003C/ul>\n",[12,15,18],{"text":13,"url":14},"Annuncio ufficiale di GGUF sul blog di Hugging Face","https://huggingface.co/blog/gguf",{"text":16,"url":17},"Repository GitHub di llama.cpp","https://github.com/ggerganov/llama.cpp",{"text":19,"url":20},"Esempio di workflow in ComfyUI con GGUF Loader","https://comfyanonymous.github.io/ComfyUI_examples/llm/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"GGUF: Quantizzazione per CPU e GPU. GGUF (Georgi Gerganov Universal Format) è un formato di file progettato per contenere modelli neurali quantizzati, ovvero convertiti in formati a bassissima precisione (come 4 o 8 bit) per ridurne drasticamente le dimensioni e il consumo di memoria. Nato dal progetto `llama.cpp` per eseguire Large Language Models (LLM) su CPU, il suo utilizzo si è recentemente espanso anche all'ecosistema dei modelli di diffusione per immagini. Lo Scopo Principale: Ridurre il Consumo di Memoria; Il vantaggio chiave di GGUF è la quantizzazione. Un modello che in formato FP16 (`.safetensors`) occuperebbe 14 GB di VRAM, in formato GGUF quantizzato a 4-bit (`q4KM`) può occuparne meno di 5 GB. Questo permette di: - Eseguire modelli enormi su GPU con meno VRAM. - Caricare più componenti in memoria contemporaneamente. - Eseguire modelli su CPU in modo efficiente. GGUF nel Mondo degli LLM (Uso Classico); L'uso primario di GGUF è per i modelli linguistici. Interfacce come LM Studio o Ollama usano file GGUF per far girare chatbot potenti (come Llama, Mistral) su hardware consumer, sfruttando principalmente la CPU. GGUF nel Mondo della Diffusione (Uso Moderno in ComfyUI); Recentemente, la community ha iniziato ad applicare i vantaggi della quantizzazione GGUF anche ai componenti di elaborazione. In ComfyUI, tramite nodi specializzati (`Load GGUF Model`), è possibile caricare versioni GGUF di: - Text Encoder (CLIP): Caricare un CLIP quantizzato riduce significativamente il suo impatto sulla VRAM, liberando risorse preziose per il modello UNet. Questo è l'uso più comune ed efficace. - UNet: Esistono anche esperimenti per quantizzare l'intera UNet in formato GGUF. Sebbene questo offra il massimo risparmio di memoria, può portare a una perdita di qualità più evidente nell'immagine finale rispetto all'uso di una UNet in formato FP16. È uno strumento versatile per la gestione avanzata della memoria, permettendo agli utenti di eseguire workflow sempre più complessi su hardware consumer, bilanciando sapientemente il compromesso tra consumo di VRAM e qualità dell'output. Decifrare le Nomenclature di Quantizzazione (es. `Q4KM`); Quando si scarica un modello GGUF, il nome del file contiene spesso una sigla che descrive il metodo di quantizzazione usato. Capirla aiuta a scegliere il giusto equilibrio tra dimensione e qualità. Ecco come leggerla: - `Q` seguito da un numero (es. `Q4`, `Q5`, `Q8`): Indica il numero di bit usati per ogni peso. `Q8` usa 8 bit (qualità più alta, file più grande), `Q4` usa 4 bit (qualità più bassa, file più piccolo). - `K`: Indica una variante \"K-Quant\". È una tecnica di quantizzazione migliorata che cerca di preservare meglio la qualità dell'informazione, specialmente per i pesi più importanti. I modelli `K` sono spesso la scelta raccomandata. - `0` o `1` (es. `Q40`, `Q51`): Indicano versioni diverse dello stesso metodo. `0` è la versione \"pura\" a 4-bit, mentre `1` è una versione mista che usa una precisione leggermente più alta (5-bit) per alcuni pesi, offrendo un piccolo miglioramento di qualità con un file leggermente più grande. - `S`, `M`, `L` (es. `Q4KS`): Indicano le dimensioni del modello (\"Small\", \"Medium\", \"Large\"). Non si riferiscono alla quantizzazione stessa, ma a diverse \"taglie\" del modello originale. Esempi Pratici: - `Q80`: Quantizzazione a 8-bit. Massima qualità tra le versioni GGUF, ma anche la più pesante. - `Q5KM`: Quantizzazione \"K-Quant\" a 5-bit, versione \"Medium\". Un ottimo compromesso tra qualità e dimensione. - `Q40`: Quantizzazione \"pura\" a 4-bit. La versione più piccola e leggera, ma con la maggiore perdita di qualità. Spesso usata per far girare modelli enormi su hardware molto limitato."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

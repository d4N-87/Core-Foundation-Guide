{"type":"data","nodes":[{"type":"data","data":[{"translations":1},{"it":2,"en":17,"fr":32,"es":46,"de":61,"pt":76},{"category":3,"connections":4,"backToHub":5,"noPostsFound":6,"pageTitleCategory":3,"initializing":7,"backToArticles":8,"sources":9,"searchPlaceholder":10,"showMap":11,"hideMap":12,"listenToArticle":13,"playing":14,"paused":15,"voice":16},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":18,"connections":19,"backToHub":20,"noPostsFound":21,"pageTitleCategory":18,"initializing":22,"backToArticles":23,"sources":24,"searchPlaceholder":25,"showMap":26,"hideMap":27,"listenToArticle":28,"playing":29,"paused":30,"voice":31},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":33,"connections":34,"backToHub":35,"noPostsFound":36,"pageTitleCategory":33,"initializing":37,"backToArticles":38,"sources":24,"searchPlaceholder":39,"showMap":40,"hideMap":41,"listenToArticle":42,"playing":43,"paused":44,"voice":45},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":47,"connections":48,"backToHub":49,"noPostsFound":50,"pageTitleCategory":47,"initializing":51,"backToArticles":52,"sources":53,"searchPlaceholder":54,"showMap":55,"hideMap":56,"listenToArticle":57,"playing":58,"paused":59,"voice":60},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":62,"connections":63,"backToHub":64,"noPostsFound":65,"pageTitleCategory":62,"initializing":66,"backToArticles":67,"sources":68,"searchPlaceholder":69,"showMap":70,"hideMap":71,"listenToArticle":72,"playing":73,"paused":74,"voice":75},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":3,"connections":77,"backToHub":78,"noPostsFound":79,"pageTitleCategory":3,"initializing":51,"backToArticles":80,"sources":81,"searchPlaceholder":82,"showMap":83,"hideMap":56,"listenToArticle":84,"playing":85,"paused":86,"voice":60},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa"],"uses":{}},null,{"type":"data","data":[{"post":1,"translations":21,"seo":107,"textContent":108},{"lang":2,"categorySlug":3,"categoryName":4,"categoryColor":5,"slug":6,"title":7,"excerpt":8,"plainExcerpt":9,"content":10,"sources":11},"it","advanced-topics","Advanced Topics","cyan","tokens","Tokens: I Mattoni del Linguaggio","\u003Cp>I \u003Cstrong>Tokens\u003C/strong> sono le unità fondamentali in cui un testo viene scomposto prima di essere processato da un modello linguistico come CLIP. [1] Sono i &quot;ma...\u003C/p>\n","I Tokens sono le unità fondamentali in cui un testo viene scomposto prima di essere processato da un modello linguistico come CLIP. [1] Sono i \"ma...","\u003Cp>I \u003Cstrong>Tokens\u003C/strong> sono le unità fondamentali in cui un testo viene scomposto prima di essere processato da un modello linguistico come CLIP. [1] Sono i &quot;mattoni&quot; con cui il modello legge e comprende il nostro prompt.\u003C/p>\n\u003Cp>Un token \u003Cstrong>non è necessariamente una parola intera\u003C/strong>. Il processo di \u003Cstrong>Tokenizing\u003C/strong> (tokenizzazione) utilizza un vocabolario predefinito per suddividere il testo in pezzi che il modello conosce. [3]\u003C/p>\n\u003Ch3>Esempi di Tokenizzazione\u003C/h3>\n\u003Cp>Consideriamo la parola \u003Ccode>indescrivibilmente\u003C/code>. Un tokenizer potrebbe scomporla in più token che conosce:\n\u003Ccode>inde\u003C/code> + \u003Ccode>scrivi\u003C/code> + \u003Ccode>bil\u003C/code> + \u003Ccode>mente\u003C/code>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Parole comuni\u003C/strong> (es. \u003Ccode>gatto\u003C/code>, \u003Ccode>il\u003C/code>, \u003Ccode>un\u003C/code>) sono spesso un singolo token.\u003C/li>\n\u003Cli>\u003Cstrong>Parole complesse o rare\u003C/strong> vengono scomposte in sotto-parole.\u003C/li>\n\u003Cli>\u003Cstrong>Spazi e punteggiatura\u003C/strong> vengono gestiti come token separati.\u003C/li>\n\u003C/ul>\n\u003Cp>Questo approccio permette al modello di gestire un vocabolario virtualmente infinito partendo da un numero finito di token (solitamente tra 30.000 e 50.000). [1]\u003C/p>\n\u003Ch3>Il Limite di Token e l&#39;Evoluzione dei Modelli\u003C/h3>\n\u003Cp>Ogni Text Encoder ha un \u003Cstrong>limite massimo di token\u003C/strong> che può elaborare in un singolo &quot;chunk&quot; (blocco). Questo limite è stato a lungo una delle principali restrizioni nel prompt engineering.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>\u003Cstrong>Vecchie Architetture (es. Stable Diffusion 1.5, SDXL):\u003C/strong>\nQuesti modelli usano Text Encoder (CLIP) con un limite di \u003Cstrong>75 token\u003C/strong> per chunk. [3] Se un prompt è più lungo, viene diviso in più chunk, ma la comprensione del contesto tra un blocco e l&#39;altro è molto più debole. Questo ha costretto gli utenti a concentrare i concetti più importanti all&#39;inizio del prompt.\u003C/p>\n\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Nuove Architetture (es. FLUX.1):\u003C/strong>\nI modelli di nuova generazione, come \u003Cstrong>FLUX.1\u003C/strong>, sono progettati per superare questa limitazione. FLUX.1 utilizza un Text Encoder molto più potente (basato su T5-XXL) che è stato specificamente addestrato per comprendere prompt lunghi e complessi in modo nativo. [2] Questo permette un&#39;espressione molto più naturale e dettagliata, senza doversi preoccupare del limite artificiale dei 75 token.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>Capire il concetto di token e i limiti dei diversi modelli è fondamentale per scrivere prompt efficaci e sfruttare al massimo le capacità di ogni architettura.\u003C/p>\n",[12,15,18],{"text":13,"url":14},"Introduzione al Tokenizing - Hugging Face","https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt",{"text":16,"url":17},"Annuncio del modello FLUX.1 di Black Forest Labs","https://blackforestlabs.ai/announcing-flux/",{"text":19,"url":20},"Spiegazione dei Token nel contesto di Stable Diffusion","https://stable-diffusion-art.com/token/",{"it":22,"en":37,"fr":52,"es":66,"de":81,"pt":96},{"category":23,"connections":24,"backToHub":25,"noPostsFound":26,"pageTitleCategory":23,"initializing":27,"backToArticles":28,"sources":29,"searchPlaceholder":30,"showMap":31,"hideMap":32,"listenToArticle":33,"playing":34,"paused":35,"voice":36},"Categoria","Collegamenti","Torna all'Hub","Nessun articolo trovato.","Inizializzazione...","Torna agli articoli","Fonti","Cerca articoli...","Mostra Mappa Contenuti","Nascondi Mappa","Ascolta questo articolo","In riproduzione...","In pausa","Voce",{"category":38,"connections":39,"backToHub":40,"noPostsFound":41,"pageTitleCategory":38,"initializing":42,"backToArticles":43,"sources":44,"searchPlaceholder":45,"showMap":46,"hideMap":47,"listenToArticle":48,"playing":49,"paused":50,"voice":51},"Category","Connections","Back to Hub","No articles found.","Initializing...","Back to articles","Sources","Search articles...","Show Content Map","Hide Map","Listen to this article","Playing...","Paused","Voice",{"category":53,"connections":54,"backToHub":55,"noPostsFound":56,"pageTitleCategory":53,"initializing":57,"backToArticles":58,"sources":44,"searchPlaceholder":59,"showMap":60,"hideMap":61,"listenToArticle":62,"playing":63,"paused":64,"voice":65},"Catégorie","Connexions","Retour à l'accueil","Aucun article trouvé.","Initialisation...","Retour aux articles","Rechercher des articles...","Afficher la carte du contenu","Masquer la carte","Écouter cet article","Lecture en cours...","En pause","Voix",{"category":67,"connections":68,"backToHub":69,"noPostsFound":70,"pageTitleCategory":67,"initializing":71,"backToArticles":72,"sources":73,"searchPlaceholder":74,"showMap":75,"hideMap":76,"listenToArticle":77,"playing":78,"paused":79,"voice":80},"Categoría","Conexiones","Volver al inicio","No se encontraron artículos.","Inicializando...","Volver a los artículos","Fuentes","Buscar artículos...","Mostrar mapa de contenido","Ocultar mapa","Escuchar este artículo","Reproduciendo...","En pausa","Voz",{"category":82,"connections":83,"backToHub":84,"noPostsFound":85,"pageTitleCategory":82,"initializing":86,"backToArticles":87,"sources":88,"searchPlaceholder":89,"showMap":90,"hideMap":91,"listenToArticle":92,"playing":93,"paused":94,"voice":95},"Kategorie","Verbindungen","Zurück zum Hub","Keine Artikel gefunden.","Initialisiere...","Zurück zu den Artikeln","Quellen","Artikel suchen...","Inhaltsverzeichnis anzeigen","Verzeichnis ausblenden","Diesen Artikel anhören","Wiedergabe...","Pausiert","Stimme",{"category":23,"connections":97,"backToHub":98,"noPostsFound":99,"pageTitleCategory":23,"initializing":71,"backToArticles":100,"sources":101,"searchPlaceholder":102,"showMap":103,"hideMap":76,"listenToArticle":104,"playing":105,"paused":106,"voice":80},"Conexões","Voltar ao início","Nenhum artigo encontrado.","Voltar aos artigos","Fontes","Pesquisar artigos...","Mostrar mapa de conteúdo","Ouvir este artigo","Reproduzindo...","Em pausa",{"title":7,"description":8},"Tokens: I Mattoni del Linguaggio. I Tokens sono le unità fondamentali in cui un testo viene scomposto prima di essere processato da un modello linguistico come CLIP. Sono i \"mattoni\" con cui il modello legge e comprende il nostro prompt. Un token non è necessariamente una parola intera. Il processo di Tokenizing (tokenizzazione) utilizza un vocabolario predefinito per suddividere il testo in pezzi che il modello conosce. Esempi di Tokenizzazione; Consideriamo la parola `indescrivibilmente`. Un tokenizer potrebbe scomporla in più token che conosce: `inde` + `scrivi` + `bil` + `mente` - Parole comuni (es. `gatto`, `il`, `un`) sono spesso un singolo token. - Parole complesse o rare vengono scomposte in sotto-parole. - Spazi e punteggiatura vengono gestiti come token separati. Questo approccio permette al modello di gestire un vocabolario virtualmente infinito partendo da un numero finito di token (solitamente tra 30.000 e 50.000). Il Limite di Token e l'Evoluzione dei Modelli; Ogni Text Encoder ha un limite massimo di token che può elaborare in un singolo \"chunk\" (blocco). Questo limite è stato a lungo una delle principali restrizioni nel prompt engineering. - Vecchie Architetture (es. Stable Diffusion 1.5, SDXL): Questi modelli usano Text Encoder (CLIP) con un limite di 75 token per chunk. Se un prompt è più lungo, viene diviso in più chunk, ma la comprensione del contesto tra un blocco e l'altro è molto più debole. Questo ha costretto gli utenti a concentrare i concetti più importanti all'inizio del prompt. - Nuove Architetture (es. FLUX.1): I modelli di nuova generazione, come FLUX.1, sono progettati per superare questa limitazione. FLUX.1 utilizza un Text Encoder molto più potente (basato su T5-XXL) che è stato specificamente addestrato per comprendere prompt lunghi e complessi in modo nativo. Questo permette un'espressione molto più naturale e dettagliata, senza doversi preoccupare del limite artificiale dei 75 token. Capire il concetto di token e i limiti dei diversi modelli è fondamentale per scrivere prompt efficaci e sfruttare al massimo le capacità di ogni architettura."],"uses":{"params":["lang","category","slug"],"parent":1}}]}

---
title: 'VAE: The Visual Decoder'
category: System Anatomy
sources:
  - text: 'Original Paper: Auto-Encoding Variational Bayes'
    url: 'https://arxiv.org/abs/1312.6114'
  - text: Explanation on Hugging Face
    url: 'https://huggingface.co/docs/diffusers/main/en/api/models/autoencoderkl'
related:
  - latent_space
  - checkpoint
---

The **VAE (Variational Autoencoder)** is the final decoder of your system. [1, 2]

Imagine that the AI model does not "think" with images, but in an abstract mathematical language, a **latent space**. It's like a composer writing a score: the score is not music, it's symbols on a sheet.

The VAE is the orchestra that reads that score and transforms it into the visual symphony you see on the screen. Without it, you would only be left with the score (incomprehensible noise) and not the music (the final image).

### What is it for in practice?

- **From Latent to Pixels:** Its primary function is to convert the abstract representation (latent tensor) generated by the model into a real image, with pixels and colors. [2]
- **Compression:** It can also do the opposite, compressing an existing image into its latent representation (encoding process).
